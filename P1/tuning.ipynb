{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelyue/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR, AdaBoostRegressor as ABR, ExtraTreesRegressor as ETR, BaggingRegressor as BR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "def kfold(k, predictor, X):\n",
    "    kf = KFold(n_splits = k, random_state = 0)\n",
    "    count = 0\n",
    "    for trains, tests in kf.split(X):\n",
    "        x_train = X.iloc[trains, 1:-1]\n",
    "        y_train = X.iloc[trains, -1]\n",
    "        x_test = X.iloc[tests, 1:-1]\n",
    "        y_test = X.iloc[tests, -1]\n",
    "        predictor.fit(x_train, y_train)\n",
    "        predictions = predictor.predict(x_test)\n",
    "        count += np.sqrt(mean_squared_error(predictions, y_test))\n",
    "    return count/k\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test sets (assumes you have these in current working directory)\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Feat 1</th>\n",
       "      <th>Feat 2</th>\n",
       "      <th>Feat 3</th>\n",
       "      <th>Feat 4</th>\n",
       "      <th>Feat 5</th>\n",
       "      <th>Feat 6</th>\n",
       "      <th>Feat 7</th>\n",
       "      <th>Feat 8</th>\n",
       "      <th>Feat 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feat 243</th>\n",
       "      <th>Feat 244</th>\n",
       "      <th>Feat 245</th>\n",
       "      <th>Feat 246</th>\n",
       "      <th>Feat 247</th>\n",
       "      <th>Feat 248</th>\n",
       "      <th>Feat 249</th>\n",
       "      <th>Feat 250</th>\n",
       "      <th>Feat 251</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612863</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.217791</td>\n",
       "      <td>0.233629</td>\n",
       "      <td>0.540962</td>\n",
       "      <td>0.901355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.997079</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688941</td>\n",
       "      <td>0.075030</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.246119</td>\n",
       "      <td>0.143860</td>\n",
       "      <td>0.525384</td>\n",
       "      <td>0.913550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.998759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.996325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.436279</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119091</td>\n",
       "      <td>0.162869</td>\n",
       "      <td>0.361124</td>\n",
       "      <td>0.884824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.997969</td>\n",
       "      <td>0.997321</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709647</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.392743</td>\n",
       "      <td>0.377302</td>\n",
       "      <td>0.613776</td>\n",
       "      <td>0.977236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.998278</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.998427</td>\n",
       "      <td>0.996269</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.364235</td>\n",
       "      <td>0.041818</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.096297</td>\n",
       "      <td>0.166459</td>\n",
       "      <td>0.408322</td>\n",
       "      <td>0.921138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    Feat 1    Feat 2    Feat 3    Feat 4    Feat 5    Feat 6  Feat 7  \\\n",
       "0   1  0.998952  0.174118  0.999211  0.996460  0.133333  0.057143   0.000   \n",
       "1   2  0.999445  0.174118  0.999329  0.997079  0.133333  0.000000   0.000   \n",
       "2   3  0.998759  0.000000  0.997260  0.996325  0.000000  0.085714   0.125   \n",
       "3   4  0.999619  0.174118  0.997969  0.997321  0.266667  0.057143   0.125   \n",
       "4   5  0.998278  0.174118  0.998427  0.996269  0.200000  0.000000   0.000   \n",
       "\n",
       "   Feat 8  Feat 9    ...     Feat 243  Feat 244  Feat 245  Feat 246  Feat 247  \\\n",
       "0     0.0     0.0    ...          0.0       0.0         0  0.612863  0.026812   \n",
       "1     0.0     0.0    ...          0.0       0.0         0  0.688941  0.075030   \n",
       "2     0.0     0.0    ...          0.0       0.0         0  0.156863  0.436279   \n",
       "3     0.0     0.0    ...          0.0       0.0         0  0.709647  0.075472   \n",
       "4     0.0     0.0    ...          0.0       0.0         0  0.364235  0.041818   \n",
       "\n",
       "   Feat 248  Feat 249  Feat 250  Feat 251    Target  \n",
       "0     0.522  0.217791  0.233629  0.540962  0.901355  \n",
       "1     0.704  0.246119  0.143860  0.525384  0.913550  \n",
       "2     0.000  0.119091  0.162869  0.361124  0.884824  \n",
       "3     0.513  0.392743  0.377302  0.613776  0.977236  \n",
       "4     0.200  0.096297  0.166459  0.408322  0.921138  \n",
       "\n",
       "[5 rows x 253 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect training set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Feat 1</th>\n",
       "      <th>Feat 2</th>\n",
       "      <th>Feat 3</th>\n",
       "      <th>Feat 4</th>\n",
       "      <th>Feat 5</th>\n",
       "      <th>Feat 6</th>\n",
       "      <th>Feat 7</th>\n",
       "      <th>Feat 8</th>\n",
       "      <th>Feat 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feat 242</th>\n",
       "      <th>Feat 243</th>\n",
       "      <th>Feat 244</th>\n",
       "      <th>Feat 245</th>\n",
       "      <th>Feat 246</th>\n",
       "      <th>Feat 247</th>\n",
       "      <th>Feat 248</th>\n",
       "      <th>Feat 249</th>\n",
       "      <th>Feat 250</th>\n",
       "      <th>Feat 251</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.997841</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.728471</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.416164</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>0.667391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996741</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497255</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.165514</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>0.506650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688941</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.192069</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.498784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654118</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.451252</td>\n",
       "      <td>0.164180</td>\n",
       "      <td>0.774466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.997234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.160433</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.147407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    Feat 1    Feat 2    Feat 3    Feat 4    Feat 5  Feat 6  Feat 7  \\\n",
       "0   1  0.999849  0.174118  0.999819  0.997841  0.133333     0.2     0.0   \n",
       "1   2  0.999958  0.164706  1.000000  0.996741  0.066667     0.0     0.0   \n",
       "2   3  0.999666  0.174118  0.999479  0.997376  0.000000     0.0     0.0   \n",
       "3   4  0.999735  0.174118  0.999655  0.997173  0.133333     0.0     0.0   \n",
       "4   5  0.999806  0.164706  0.999551  0.997234  0.000000     0.0     0.0   \n",
       "\n",
       "   Feat 8    Feat 9    ...     Feat 242  Feat 243  Feat 244  Feat 245  \\\n",
       "0     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "1     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "2     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "3     0.0  0.363636    ...          0.0       0.0       0.0         0   \n",
       "4     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "\n",
       "   Feat 246  Feat 247  Feat 248  Feat 249  Feat 250  Feat 251  \n",
       "0  0.728471  0.054397     0.649  0.416164  0.053998  0.667391  \n",
       "1  0.497255  0.037736     0.375  0.165514  0.101973  0.506650  \n",
       "2  0.688941  0.019309     1.000  0.192069  0.120700  0.498784  \n",
       "3  0.654118  0.019089     0.333  0.451252  0.164180  0.774466  \n",
       "4  0.627451  0.160433     0.882  0.147407  0.000000  0.481240  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect test set\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set into X and y (removing first column containing IDs)\n",
    "X_train = train.iloc[:, 1:-1]\n",
    "y_train = train.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute RMSE\n",
    "def scoreRMSE(predictor, X, true_y):\n",
    "    predictions = predictor.predict(X)\n",
    "    return np.sqrt(mean_squared_error(predictions, true_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR n = 50 : 0.027152880683807758\n",
      "ABR n = 50 : 0.029322638314763914\n",
      "BR n = 50 : 0.027034923144826695\n",
      "ETR n = 50 : 0.02718804400477707\n",
      "RFR n = 75 : 0.026947865558143304\n",
      "ABR n = 75 : 0.02944534852472768\n",
      "BR n = 75 : 0.027098553115467532\n",
      "ETR n = 75 : 0.027135860388625505\n",
      "RFR n = 100 : 0.026913787983221815\n",
      "ABR n = 100 : 0.029054524755758704\n",
      "BR n = 100 : 0.026940555757043738\n",
      "ETR n = 100 : 0.027120922569592427\n",
      "RFR n = 125 : 0.026905848566217323\n",
      "ABR n = 125 : 0.02933265211059597\n",
      "BR n = 125 : 0.02689425214395493\n",
      "ETR n = 125 : 0.027102500282994647\n",
      "RFR n = 150 : 0.02686969225376329\n",
      "ABR n = 150 : 0.028953887313950648\n",
      "BR n = 150 : 0.02687091485696306\n",
      "ETR n = 150 : 0.027009915629994073\n",
      "RFR n = 175 : 0.026902252522980803\n",
      "ABR n = 175 : 0.029551181493814267\n",
      "BR n = 175 : 0.026846609024094546\n",
      "ETR n = 175 : 0.02702222898198551\n",
      "RFR n = 200 : 0.026848159874397166\n",
      "ABR n = 200 : 0.029416299994313038\n",
      "BR n = 200 : 0.026886047812234914\n",
      "ETR n = 200 : 0.026992788363546842\n",
      "RFR n = 225 : 0.0268363225102952\n",
      "ABR n = 225 : 0.029197374205947674\n",
      "BR n = 225 : 0.026833860261556774\n",
      "ETR n = 225 : 0.027009261802550234\n",
      "RFR n = 250 : 0.02688851080045328\n",
      "ABR n = 250 : 0.029042927389533856\n",
      "BR n = 250 : 0.026849199324191435\n",
      "ETR n = 250 : 0.027014599637350218\n",
      "RFR n = 275 : 0.026803317372697318\n",
      "ABR n = 275 : 0.02915441266198197\n",
      "BR n = 275 : 0.02685606828069407\n",
      "ETR n = 275 : 0.02699061416230718\n",
      "RFR n = 300 : 0.02682025960054311\n",
      "ABR n = 300 : 0.029116905658976334\n",
      "BR n = 300 : 0.026833802610899814\n",
      "ETR n = 300 : 0.026992413617462234\n",
      "RFR n = 325 : 0.02681830739634833\n",
      "ABR n = 325 : 0.029126333860639612\n",
      "BR n = 325 : 0.026847389841495883\n",
      "ETR n = 325 : 0.02695457914449446\n"
     ]
    }
   ],
   "source": [
    "# Testing 4 ensemble methods vs. n_est\n",
    "for n_est in range(50, 350, 25):\n",
    "    print(\"RFR n =\", n_est, \":\", kfold(10, RFR(n_estimators=n_est), train))\n",
    "    print(\"ABR n =\", n_est, \":\", kfold(10, ABR(n_estimators=n_est), train))\n",
    "    print(\"BR n =\", n_est, \":\", kfold(10, BR(n_estimators=n_est), train))\n",
    "    print(\"ETR n =\", n_est, \":\", kfold(10, ETR(n_estimators=n_est), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR n = 50 : 0.010459998106587457\n",
      "ABR n = 50 : 0.028151063821895743\n",
      "BR n = 50 : 0.01034430202642949\n",
      "ETR n = 50 : 0.0021228399088563978\n",
      "RFR n = 75 : 0.01032399737093872\n",
      "ABR n = 75 : 0.029926560081168434\n",
      "BR n = 75 : 0.010313168581899531\n",
      "ETR n = 75 : 0.002122831449061378\n",
      "RFR n = 100 : 0.010229984597762165\n",
      "ABR n = 100 : 0.028404736816260247\n",
      "BR n = 100 : 0.010205892617720832\n",
      "ETR n = 100 : 0.002122830802281152\n",
      "RFR n = 125 : 0.0102090449043317\n",
      "ABR n = 125 : 0.029077167839701157\n",
      "BR n = 125 : 0.010267924406875814\n",
      "ETR n = 125 : 0.0021228265017234987\n",
      "RFR n = 150 : 0.010116688958378568\n",
      "ABR n = 150 : 0.02890635250886352\n",
      "BR n = 150 : 0.010167699278428623\n",
      "ETR n = 150 : 0.002122825485695412\n",
      "RFR n = 175 : 0.010127141080715897\n",
      "ABR n = 175 : 0.028213203332107056\n",
      "BR n = 175 : 0.010124540857739922\n",
      "ETR n = 175 : 0.002122825932854721\n",
      "RFR n = 200 : 0.01014598470168394\n",
      "ABR n = 200 : 0.02865194032684834\n",
      "BR n = 200 : 0.010157547813377125\n",
      "ETR n = 200 : 0.0021228240376605246\n",
      "RFR n = 225 : 0.0100724017670848\n",
      "ABR n = 225 : 0.028565416370154997\n",
      "BR n = 225 : 0.010100962611801448\n",
      "ETR n = 225 : 0.0021228251653658803\n",
      "RFR n = 250 : 0.010064293665362215\n",
      "ABR n = 250 : 0.02822242694979026\n",
      "BR n = 250 : 0.010058770319688715\n",
      "ETR n = 250 : 0.002122822662280688\n",
      "RFR n = 275 : 0.010043559695031243\n",
      "ABR n = 275 : 0.029046368285488337\n",
      "BR n = 275 : 0.010031787733345046\n",
      "ETR n = 275 : 0.0021228227379995186\n",
      "RFR n = 300 : 0.010109280948045904\n",
      "ABR n = 300 : 0.028597298258960766\n",
      "BR n = 300 : 0.010069282686974905\n",
      "ETR n = 300 : 0.0021228232301809934\n",
      "RFR n = 325 : 0.010060983775837693\n",
      "ABR n = 325 : 0.027963581130807887\n",
      "BR n = 325 : 0.010059593348616194\n",
      "ETR n = 325 : 0.0021228237079335337\n"
     ]
    }
   ],
   "source": [
    "for n_est in range(50, 350, 25):\n",
    "    rfr = RFR(n_estimators=n_est)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    print(\"RFR n =\", n_est, \":\", scoreRMSE(rfr, X_train, y_train))\n",
    "    \n",
    "    abr = ABR(n_estimators=n_est)\n",
    "    abr.fit(X_train, y_train)\n",
    "    print(\"ABR n =\", n_est, \":\", scoreRMSE(abr, X_train, y_train))\n",
    "    \n",
    "    br = BR(n_estimators=n_est)\n",
    "    br.fit(X_train, y_train)\n",
    "    print(\"BR n =\", n_est, \":\", scoreRMSE(br, X_train, y_train))\n",
    "    \n",
    "    etr = ETR(n_estimators=n_est)\n",
    "    etr.fit(X_train, y_train)\n",
    "    print(\"ETR n =\", n_est, \":\", scoreRMSE(etr, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE:  0.032418983721719974\n"
     ]
    }
   ],
   "source": [
    "# Naive implementations of several candidates just to compare\n",
    "# preliminary relative 10-folds performance to random forests/gradient boosting\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "\n",
    "mlp = MLP()\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training RMSE: \", scoreRMSE(mlp, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02796802133524843"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, MLP(hidden_layer_sizes=(1000,500,100,100,100),activation='logistic', alpha=0.00005, solver='sgd', learning_rate_init=0.0005), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029742836179759823"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KR\n",
    "\n",
    "kfold(10, KR(alpha=1), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028141823013705915"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "kfold(10, SVR(), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037553002433404196"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "\n",
    "kfold(10, DTR(), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027492723581277672"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "\n",
    "kfold(10, KNR(n_neighbors=10, weights='distance'), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027032063335554347"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, KNR(n_neighbors=30, weights='uniform', algorithm='kd_tree'), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02679644096620772"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, RFR(n_estimators=1000), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0268210537288536"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, BR(n_estimators=1000), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02699775126973728"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, RFR(n_estimators=100), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-5d60065bebc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRFR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c2b53341356e>\u001b[0m in \u001b[0;36mkfold\u001b[0;34m(k, predictor, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold(10, RFR(n_estimators=200, min_samples_leaf=100), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50, score=0.062127110194932667, total= 3.9min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50, score=0.0749879081925846, total= 3.9min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50, score=0.09544312974075686, total= 3.9min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=3000, random_state=50, score=0.07369768699350754, total= 3.9min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50, score=0.06930719318538836, total= 3.3min\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50, score=0.062258462253159896, total= 3.3min\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50, score=0.08294179130044343, total= 3.3min\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=5, n_estimators=3000, random_state=50, score=0.09844355366873636, total= 3.3min\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50, score=0.06581401889359928, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50, score=0.06001190947272994, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50, score=0.09735620246971244, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=10, n_estimators=3000, random_state=50, score=0.08212675126134605, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50, score=0.06283742060544595, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50, score=0.056918084418789205, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50, score=0.08020522700043364, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=15, n_estimators=3000, random_state=50, score=0.09501764419846104, total= 3.1min\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50, score=0.06111033623170004, total= 3.2min\n",
      "[CV] max_depth=15, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50, score=0.07878788277600468, total= 3.2min\n",
      "[CV] max_depth=15, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50, score=0.055219544557572386, total= 3.2min\n",
      "[CV] max_depth=15, min_samples_leaf=1, n_estimators=3000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=20, n_estimators=3000, random_state=50, score=0.09288166705867107, total= 3.2min\n",
      "[CV] max_depth=15, min_samples_leaf=1, n_estimators=3000, random_state=50 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-be7dacdbbc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrfR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrfR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tuning random forests at n_est=3000 with 4-fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [10, 15, 20, 30, None],\n",
    "    'min_samples_leaf': [1, 5, 10, 15, 20],\n",
    "    'n_estimators': [3000],\n",
    "    'random_state': [50]\n",
    "}\n",
    "\n",
    "rfR = RFR()\n",
    "gs = GridSearchCV(estimator=rfR, param_grid=params, cv=4, n_jobs=4, verbose=3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(kfold(10, gs.best_estimator_, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026882546415785885"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(3, RFR(n_estimators=500, n_jobs=-1), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15,\n",
       " 'min_samples_leaf': 5,\n",
       " 'n_estimators': 3000,\n",
       " 'random_state': 50}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gs.best_estimator_.predict(test.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.931733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.912044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.916009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.925792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.929910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.931733\n",
       "1  2   0.912044\n",
       "2  3   0.916009\n",
       "3  4   0.925792\n",
       "4  5   0.929910"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.DataFrame(data=predictions, columns=['Predicted'])\n",
    "sample_submission.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "sample_submission['Id'] = sample_submission['Id'].astype(str)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to .csv file for upload to Kaggle\n",
    "sample_submission.to_csv(\"tunedrfr_3000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50, score=0.061358335889442506, total= 2.4min\n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50, score=0.07474844022140092, total= 2.4min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=2000, random_state=50 \n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=2000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50, score=0.09566727341871328, total= 2.5min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=2000, random_state=50 \n",
      "[CV]  max_depth=10, min_samples_leaf=1, n_estimators=2000, random_state=50, score=0.07356021017925951, total= 2.5min\n",
      "[CV] max_depth=10, min_samples_leaf=5, n_estimators=2000, random_state=50 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-992a8150c453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrfR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': [10, 15, 20, 30, None],\n",
    "    'min_samples_leaf': [1, 5, 10, 15, 20],\n",
    "    'n_estimators': [2000],\n",
    "    'random_state': [50]\n",
    "}\n",
    "\n",
    "gs2 = GridSearchCV(estimator=rfR, param_grid=params, cv=4, n_jobs=4, verbose=3)\n",
    "gs2.fit(X_train, y_train)\n",
    "\n",
    "print(gs2.best_params_)\n",
    "print(kfold(10, gs2.best_estimator_, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 3000\n",
      "building tree 2 of 3000\n",
      "building tree 3 of 3000\n",
      "building tree 4 of 3000\n",
      "building tree 5 of 3000\n",
      "building tree 6 of 3000\n",
      "building tree 7 of 3000\n",
      "building tree 8 of 3000\n",
      "building tree 9 of 3000\n",
      "building tree 10 of 3000\n",
      "building tree 11 of 3000\n",
      "building tree 12 of 3000\n",
      "building tree 13 of 3000\n",
      "building tree 14 of 3000\n",
      "building tree 15 of 3000\n",
      "building tree 16 of 3000\n",
      "building tree 17 of 3000\n",
      "building tree 18 of 3000\n",
      "building tree 19 of 3000\n",
      "building tree 20 of 3000\n",
      "building tree 21 of 3000\n",
      "building tree 22 of 3000\n",
      "building tree 23 of 3000\n",
      "building tree 24 of 3000\n",
      "building tree 25 of 3000\n",
      "building tree 26 of 3000\n",
      "building tree 27 of 3000\n",
      "building tree 28 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 29 of 3000\n",
      "building tree 30 of 3000\n",
      "building tree 31 of 3000\n",
      "building tree 32 of 3000\n",
      "building tree 33 of 3000\n",
      "building tree 34 of 3000\n",
      "building tree 35 of 3000\n",
      "building tree 36 of 3000\n",
      "building tree 37 of 3000\n",
      "building tree 38 of 3000\n",
      "building tree 39 of 3000\n",
      "building tree 40 of 3000\n",
      "building tree 41 of 3000\n",
      "building tree 42 of 3000\n",
      "building tree 43 of 3000\n",
      "building tree 44 of 3000\n",
      "building tree 45 of 3000\n",
      "building tree 46 of 3000\n",
      "building tree 47 of 3000\n",
      "building tree 48 of 3000\n",
      "building tree 49 of 3000\n",
      "building tree 50 of 3000\n",
      "building tree 51 of 3000\n",
      "building tree 52 of 3000\n",
      "building tree 53 of 3000\n",
      "building tree 54 of 3000\n",
      "building tree 55 of 3000\n",
      "building tree 56 of 3000\n",
      "building tree 57 of 3000\n",
      "building tree 58 of 3000\n",
      "building tree 59 of 3000\n",
      "building tree 60 of 3000\n",
      "building tree 61 of 3000\n",
      "building tree 62 of 3000\n",
      "building tree 63 of 3000\n",
      "building tree 64 of 3000\n",
      "building tree 65 of 3000\n",
      "building tree 66 of 3000\n",
      "building tree 67 of 3000\n",
      "building tree 68 of 3000\n",
      "building tree 69 of 3000\n",
      "building tree 70 of 3000\n",
      "building tree 71 of 3000\n",
      "building tree 72 of 3000\n",
      "building tree 73 of 3000building tree 74 of 3000\n",
      "\n",
      "building tree 75 of 3000\n",
      "building tree 76 of 3000\n",
      "building tree 77 of 3000\n",
      "building tree 78 of 3000\n",
      "building tree 79 of 3000\n",
      "building tree 80 of 3000\n",
      "building tree 81 of 3000\n",
      "building tree 82 of 3000\n",
      "building tree 83 of 3000\n",
      "building tree 84 of 3000\n",
      "building tree 85 of 3000\n",
      "building tree 86 of 3000\n",
      "building tree 87 of 3000\n",
      "building tree 88 of 3000\n",
      "building tree 89 of 3000\n",
      "building tree 90 of 3000\n",
      "building tree 91 of 3000\n",
      "building tree 92 of 3000\n",
      "building tree 93 of 3000\n",
      "building tree 94 of 3000\n",
      "building tree 95 of 3000\n",
      "building tree 96 of 3000\n",
      "building tree 97 of 3000\n",
      "building tree 98 of 3000\n",
      "building tree 99 of 3000\n",
      "building tree 100 of 3000\n",
      "building tree 101 of 3000\n",
      "building tree 102 of 3000\n",
      "building tree 103 of 3000\n",
      "building tree 104 of 3000\n",
      "building tree 105 of 3000\n",
      "building tree 106 of 3000\n",
      "building tree 107 of 3000\n",
      "building tree 108 of 3000\n",
      "building tree 109 of 3000\n",
      "building tree 110 of 3000\n",
      "building tree 111 of 3000\n",
      "building tree 112 of 3000\n",
      "building tree 113 of 3000\n",
      "building tree 114 of 3000\n",
      "building tree 115 of 3000\n",
      "building tree 116 of 3000\n",
      "building tree 117 of 3000\n",
      "building tree 118 of 3000\n",
      "building tree 119 of 3000\n",
      "building tree 120 of 3000\n",
      "building tree 121 of 3000\n",
      "building tree 122 of 3000\n",
      "building tree 123 of 3000\n",
      "building tree 124 of 3000\n",
      "building tree 125 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    8.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 126 of 3000\n",
      "building tree 127 of 3000\n",
      "building tree 128 of 3000\n",
      "building tree 129 of 3000\n",
      "building tree 130 of 3000\n",
      "building tree 131 of 3000\n",
      "building tree 132 of 3000\n",
      "building tree 133 of 3000\n",
      "building tree 134 of 3000\n",
      "building tree 135 of 3000\n",
      "building tree 136 of 3000\n",
      "building tree 137 of 3000\n",
      "building tree 138 of 3000\n",
      "building tree 139 of 3000\n",
      "building tree 140 of 3000\n",
      "building tree 141 of 3000\n",
      "building tree 142 of 3000\n",
      "building tree 143 of 3000\n",
      "building tree 144 of 3000\n",
      "building tree 145 of 3000\n",
      "building tree 146 of 3000\n",
      "building tree 147 of 3000\n",
      "building tree 148 of 3000\n",
      "building tree 149 of 3000\n",
      "building tree 150 of 3000\n",
      "building tree 151 of 3000\n",
      "building tree 152 of 3000\n",
      "building tree 153 of 3000\n",
      "building tree 154 of 3000\n",
      "building tree 155 of 3000\n",
      "building tree 156 of 3000\n",
      "building tree 157 of 3000\n",
      "building tree 158 of 3000\n",
      "building tree 159 of 3000\n",
      "building tree 160 of 3000\n",
      "building tree 161 of 3000\n",
      "building tree 162 of 3000\n",
      "building tree 163 of 3000\n",
      "building tree 164 of 3000\n",
      "building tree 165 of 3000\n",
      "building tree 166 of 3000\n",
      "building tree 167 of 3000\n",
      "building tree 168 of 3000\n",
      "building tree 169 of 3000\n",
      "building tree 170 of 3000\n",
      "building tree 171 of 3000\n",
      "building tree 172 of 3000\n",
      "building tree 173 of 3000\n",
      "building tree 174 of 3000\n",
      "building tree 175 of 3000\n",
      "building tree 176 of 3000\n",
      "building tree 177 of 3000\n",
      "building tree 178 of 3000\n",
      "building tree 179 of 3000\n",
      "building tree 180 of 3000\n",
      "building tree 181 of 3000\n",
      "building tree 182 of 3000\n",
      "building tree 183 of 3000\n",
      "building tree 184 of 3000\n",
      "building tree 185 of 3000\n",
      "building tree 186 of 3000\n",
      "building tree 187 of 3000\n",
      "building tree 188 of 3000\n",
      "building tree 189 of 3000\n",
      "building tree 190 of 3000\n",
      "building tree 191 of 3000\n",
      "building tree 192 of 3000\n",
      "building tree 193 of 3000\n",
      "building tree 194 of 3000\n",
      "building tree 195 of 3000\n",
      "building tree 196 of 3000\n",
      "building tree 197 of 3000\n",
      "building tree 198 of 3000\n",
      "building tree 199 of 3000\n",
      "building tree 200 of 3000\n",
      "building tree 201 of 3000\n",
      "building tree 202 of 3000\n",
      "building tree 203 of 3000\n",
      "building tree 204 of 3000\n",
      "building tree 205 of 3000\n",
      "building tree 206 of 3000\n",
      "building tree 207 of 3000\n",
      "building tree 208 of 3000\n",
      "building tree 209 of 3000\n",
      "building tree 210 of 3000\n",
      "building tree 211 of 3000\n",
      "building tree 212 of 3000\n",
      "building tree 213 of 3000\n",
      "building tree 214 of 3000\n",
      "building tree 215 of 3000\n",
      "building tree 216 of 3000\n",
      "building tree 217 of 3000\n",
      "building tree 218 of 3000\n",
      "building tree 219 of 3000\n",
      "building tree 220 of 3000\n",
      "building tree 221 of 3000\n",
      "building tree 222 of 3000\n",
      "building tree 223 of 3000\n",
      "building tree 224 of 3000\n",
      "building tree 225 of 3000\n",
      "building tree 226 of 3000\n",
      "building tree 227 of 3000\n",
      "building tree 228 of 3000\n",
      "building tree 229 of 3000\n",
      "building tree 230 of 3000\n",
      "building tree 231 of 3000\n",
      "building tree 232 of 3000\n",
      "building tree 233 of 3000\n",
      "building tree 234 of 3000\n",
      "building tree 235 of 3000\n",
      "building tree 236 of 3000\n",
      "building tree 237 of 3000\n",
      "building tree 238 of 3000\n",
      "building tree 239 of 3000\n",
      "building tree 240 of 3000\n",
      "building tree 241 of 3000\n",
      "building tree 242 of 3000\n",
      "building tree 243 of 3000\n",
      "building tree 244 of 3000\n",
      "building tree 245 of 3000\n",
      "building tree 246 of 3000\n",
      "building tree 247 of 3000\n",
      "building tree 248 of 3000\n",
      "building tree 249 of 3000\n",
      "building tree 250 of 3000\n",
      "building tree 251 of 3000\n",
      "building tree 252 of 3000\n",
      "building tree 253 of 3000\n",
      "building tree 254 of 3000\n",
      "building tree 255 of 3000\n",
      "building tree 256 of 3000\n",
      "building tree 257 of 3000\n",
      "building tree 258 of 3000\n",
      "building tree 259 of 3000\n",
      "building tree 260 of 3000\n",
      "building tree 261 of 3000\n",
      "building tree 262 of 3000\n",
      "building tree 263 of 3000\n",
      "building tree 264 of 3000\n",
      "building tree 265 of 3000\n",
      "building tree 266 of 3000\n",
      "building tree 267 of 3000\n",
      "building tree 268 of 3000\n",
      "building tree 269 of 3000\n",
      "building tree 270 of 3000\n",
      "building tree 271 of 3000\n",
      "building tree 272 of 3000\n",
      "building tree 273 of 3000\n",
      "building tree 274 of 3000\n",
      "building tree 275 of 3000\n",
      "building tree 276 of 3000\n",
      "building tree 277 of 3000\n",
      "building tree 278 of 3000\n",
      "building tree 279 of 3000\n",
      "building tree 280 of 3000\n",
      "building tree 281 of 3000\n",
      "building tree 282 of 3000\n",
      "building tree 283 of 3000\n",
      "building tree 284 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   19.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 285 of 3000\n",
      "building tree 286 of 3000\n",
      "building tree 287 of 3000\n",
      "building tree 288 of 3000\n",
      "building tree 289 of 3000\n",
      "building tree 290 of 3000\n",
      "building tree 291 of 3000\n",
      "building tree 292 of 3000\n",
      "building tree 293 of 3000\n",
      "building tree 294 of 3000\n",
      "building tree 295 of 3000\n",
      "building tree 296 of 3000\n",
      "building tree 297 of 3000\n",
      "building tree 298 of 3000\n",
      "building tree 299 of 3000\n",
      "building tree 300 of 3000\n",
      "building tree 301 of 3000\n",
      "building tree 302 of 3000\n",
      "building tree 303 of 3000\n",
      "building tree 304 of 3000\n",
      "building tree 305 of 3000\n",
      "building tree 306 of 3000\n",
      "building tree 307 of 3000\n",
      "building tree 308 of 3000\n",
      "building tree 309 of 3000\n",
      "building tree 310 of 3000\n",
      "building tree 311 of 3000\n",
      "building tree 312 of 3000\n",
      "building tree 313 of 3000\n",
      "building tree 314 of 3000\n",
      "building tree 315 of 3000\n",
      "building tree 316 of 3000\n",
      "building tree 317 of 3000\n",
      "building tree 318 of 3000\n",
      "building tree 319 of 3000\n",
      "building tree 320 of 3000\n",
      "building tree 321 of 3000\n",
      "building tree 322 of 3000\n",
      "building tree 323 of 3000\n",
      "building tree 324 of 3000building tree 325 of 3000\n",
      "\n",
      "building tree 326 of 3000\n",
      "building tree 327 of 3000\n",
      "building tree 328 of 3000\n",
      "building tree 329 of 3000\n",
      "building tree 330 of 3000\n",
      "building tree 331 of 3000\n",
      "building tree 332 of 3000\n",
      "building tree 333 of 3000\n",
      "building tree 334 of 3000\n",
      "building tree 335 of 3000\n",
      "building tree 336 of 3000\n",
      "building tree 337 of 3000\n",
      "building tree 338 of 3000\n",
      "building tree 339 of 3000\n",
      "building tree 340 of 3000\n",
      "building tree 341 of 3000\n",
      "building tree 342 of 3000\n",
      "building tree 343 of 3000\n",
      "building tree 344 of 3000\n",
      "building tree 345 of 3000\n",
      "building tree 346 of 3000\n",
      "building tree 347 of 3000\n",
      "building tree 348 of 3000\n",
      "building tree 349 of 3000\n",
      "building tree 350 of 3000\n",
      "building tree 351 of 3000\n",
      "building tree 352 of 3000\n",
      "building tree 353 of 3000\n",
      "building tree 354 of 3000\n",
      "building tree 355 of 3000\n",
      "building tree 356 of 3000\n",
      "building tree 357 of 3000\n",
      "building tree 358 of 3000\n",
      "building tree 359 of 3000\n",
      "building tree 360 of 3000\n",
      "building tree 361 of 3000\n",
      "building tree 362 of 3000\n",
      "building tree 363 of 3000\n",
      "building tree 364 of 3000\n",
      "building tree 365 of 3000\n",
      "building tree 366 of 3000\n",
      "building tree 367 of 3000\n",
      "building tree 368 of 3000\n",
      "building tree 369 of 3000\n",
      "building tree 370 of 3000\n",
      "building tree 371 of 3000\n",
      "building tree 372 of 3000\n",
      "building tree 373 of 3000\n",
      "building tree 374 of 3000\n",
      "building tree 375 of 3000\n",
      "building tree 376 of 3000\n",
      "building tree 377 of 3000\n",
      "building tree 378 of 3000\n",
      "building tree 379 of 3000\n",
      "building tree 380 of 3000\n",
      "building tree 381 of 3000\n",
      "building tree 382 of 3000\n",
      "building tree 383 of 3000\n",
      "building tree 384 of 3000\n",
      "building tree 385 of 3000\n",
      "building tree 386 of 3000\n",
      "building tree 387 of 3000\n",
      "building tree 388 of 3000\n",
      "building tree 389 of 3000\n",
      "building tree 390 of 3000\n",
      "building tree 391 of 3000\n",
      "building tree 392 of 3000\n",
      "building tree 393 of 3000\n",
      "building tree 394 of 3000\n",
      "building tree 395 of 3000\n",
      "building tree 396 of 3000\n",
      "building tree 397 of 3000\n",
      "building tree 398 of 3000\n",
      "building tree 399 of 3000\n",
      "building tree 400 of 3000\n",
      "building tree 401 of 3000\n",
      "building tree 402 of 3000\n",
      "building tree 403 of 3000\n",
      "building tree 404 of 3000\n",
      "building tree 405 of 3000\n",
      "building tree 406 of 3000\n",
      "building tree 407 of 3000\n",
      "building tree 408 of 3000\n",
      "building tree 409 of 3000\n",
      "building tree 410 of 3000\n",
      "building tree 411 of 3000\n",
      "building tree 412 of 3000\n",
      "building tree 413 of 3000\n",
      "building tree 414 of 3000\n",
      "building tree 415 of 3000\n",
      "building tree 416 of 3000\n",
      "building tree 417 of 3000\n",
      "building tree 418 of 3000\n",
      "building tree 419 of 3000\n",
      "building tree 420 of 3000\n",
      "building tree 421 of 3000\n",
      "building tree 422 of 3000\n",
      "building tree 423 of 3000\n",
      "building tree 424 of 3000\n",
      "building tree 425 of 3000\n",
      "building tree 426 of 3000\n",
      "building tree 427 of 3000\n",
      "building tree 428 of 3000\n",
      "building tree 429 of 3000\n",
      "building tree 430 of 3000\n",
      "building tree 431 of 3000\n",
      "building tree 432 of 3000\n",
      "building tree 433 of 3000\n",
      "building tree 434 of 3000\n",
      "building tree 435 of 3000\n",
      "building tree 436 of 3000\n",
      "building tree 437 of 3000\n",
      "building tree 438 of 3000\n",
      "building tree 439 of 3000\n",
      "building tree 440 of 3000\n",
      "building tree 441 of 3000\n",
      "building tree 442 of 3000\n",
      "building tree 443 of 3000\n",
      "building tree 444 of 3000\n",
      "building tree 445 of 3000\n",
      "building tree 446 of 3000\n",
      "building tree 447 of 3000\n",
      "building tree 448 of 3000\n",
      "building tree 449 of 3000\n",
      "building tree 450 of 3000\n",
      "building tree 451 of 3000\n",
      "building tree 452 of 3000\n",
      "building tree 453 of 3000\n",
      "building tree 454 of 3000\n",
      "building tree 455 of 3000\n",
      "building tree 456 of 3000\n",
      "building tree 457 of 3000\n",
      "building tree 458 of 3000\n",
      "building tree 459 of 3000\n",
      "building tree 460 of 3000\n",
      "building tree 461 of 3000\n",
      "building tree 462 of 3000\n",
      "building tree 463 of 3000\n",
      "building tree 464 of 3000\n",
      "building tree 465 of 3000\n",
      "building tree 466 of 3000\n",
      "building tree 467 of 3000\n",
      "building tree 468 of 3000\n",
      "building tree 469 of 3000\n",
      "building tree 470 of 3000\n",
      "building tree 471 of 3000\n",
      "building tree 472 of 3000\n",
      "building tree 473 of 3000\n",
      "building tree 474 of 3000\n",
      "building tree 475 of 3000\n",
      "building tree 476 of 3000\n",
      "building tree 477 of 3000\n",
      "building tree 478 of 3000\n",
      "building tree 479 of 3000\n",
      "building tree 480 of 3000\n",
      "building tree 481 of 3000\n",
      "building tree 482 of 3000\n",
      "building tree 483 of 3000\n",
      "building tree 484 of 3000\n",
      "building tree 485 of 3000\n",
      "building tree 486 of 3000\n",
      "building tree 487 of 3000\n",
      "building tree 488 of 3000\n",
      "building tree 489 of 3000\n",
      "building tree 490 of 3000\n",
      "building tree 491 of 3000\n",
      "building tree 492 of 3000\n",
      "building tree 493 of 3000\n",
      "building tree 494 of 3000\n",
      "building tree 495 of 3000\n",
      "building tree 496 of 3000\n",
      "building tree 497 of 3000\n",
      "building tree 498 of 3000\n",
      "building tree 499 of 3000\n",
      "building tree 500 of 3000\n",
      "building tree 501 of 3000\n",
      "building tree 502 of 3000\n",
      "building tree 503 of 3000\n",
      "building tree 504 of 3000\n",
      "building tree 505 of 3000\n",
      "building tree 506 of 3000\n",
      "building tree 507 of 3000\n",
      "building tree 508 of 3000\n",
      "building tree 509 of 3000\n",
      "building tree 510 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:   34.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 511 of 3000\n",
      "building tree 512 of 3000\n",
      "building tree 513 of 3000\n",
      "building tree 514 of 3000\n",
      "building tree 515 of 3000\n",
      "building tree 516 of 3000\n",
      "building tree 517 of 3000\n",
      "building tree 518 of 3000\n",
      "building tree 519 of 3000\n",
      "building tree 520 of 3000\n",
      "building tree 521 of 3000\n",
      "building tree 522 of 3000\n",
      "building tree 523 of 3000\n",
      "building tree 524 of 3000\n",
      "building tree 525 of 3000\n",
      "building tree 526 of 3000\n",
      "building tree 527 of 3000\n",
      "building tree 528 of 3000\n",
      "building tree 529 of 3000\n",
      "building tree 530 of 3000\n",
      "building tree 531 of 3000\n",
      "building tree 532 of 3000\n",
      "building tree 533 of 3000\n",
      "building tree 534 of 3000\n",
      "building tree 535 of 3000\n",
      "building tree 536 of 3000\n",
      "building tree 537 of 3000\n",
      "building tree 538 of 3000\n",
      "building tree 539 of 3000\n",
      "building tree 540 of 3000\n",
      "building tree 541 of 3000\n",
      "building tree 542 of 3000\n",
      "building tree 543 of 3000\n",
      "building tree 544 of 3000\n",
      "building tree 545 of 3000\n",
      "building tree 546 of 3000\n",
      "building tree 547 of 3000\n",
      "building tree 548 of 3000\n",
      "building tree 549 of 3000\n",
      "building tree 550 of 3000\n",
      "building tree 551 of 3000\n",
      "building tree 552 of 3000\n",
      "building tree 553 of 3000\n",
      "building tree 554 of 3000\n",
      "building tree 555 of 3000\n",
      "building tree 556 of 3000\n",
      "building tree 557 of 3000\n",
      "building tree 558 of 3000\n",
      "building tree 559 of 3000\n",
      "building tree 560 of 3000\n",
      "building tree 561 of 3000\n",
      "building tree 562 of 3000\n",
      "building tree 563 of 3000\n",
      "building tree 564 of 3000\n",
      "building tree 565 of 3000\n",
      "building tree 566 of 3000\n",
      "building tree 567 of 3000\n",
      "building tree 568 of 3000\n",
      "building tree 569 of 3000\n",
      "building tree 570 of 3000\n",
      "building tree 571 of 3000\n",
      "building tree 572 of 3000\n",
      "building tree 573 of 3000\n",
      "building tree 574 of 3000\n",
      "building tree 575 of 3000\n",
      "building tree 576 of 3000\n",
      "building tree 577 of 3000\n",
      "building tree 578 of 3000\n",
      "building tree 579 of 3000\n",
      "building tree 580 of 3000\n",
      "building tree 581 of 3000\n",
      "building tree 582 of 3000\n",
      "building tree 583 of 3000\n",
      "building tree 584 of 3000\n",
      "building tree 585 of 3000\n",
      "building tree 586 of 3000\n",
      "building tree 587 of 3000\n",
      "building tree 588 of 3000\n",
      "building tree 589 of 3000\n",
      "building tree 590 of 3000\n",
      "building tree 591 of 3000\n",
      "building tree 592 of 3000\n",
      "building tree 593 of 3000\n",
      "building tree 594 of 3000\n",
      "building tree 595 of 3000\n",
      "building tree 596 of 3000\n",
      "building tree 597 of 3000\n",
      "building tree 598 of 3000\n",
      "building tree 599 of 3000\n",
      "building tree 600 of 3000\n",
      "building tree 601 of 3000\n",
      "building tree 602 of 3000\n",
      "building tree 603 of 3000\n",
      "building tree 604 of 3000\n",
      "building tree 605 of 3000\n",
      "building tree 606 of 3000\n",
      "building tree 607 of 3000\n",
      "building tree 608 of 3000\n",
      "building tree 609 of 3000\n",
      "building tree 610 of 3000\n",
      "building tree 611 of 3000\n",
      "building tree 612 of 3000\n",
      "building tree 613 of 3000\n",
      "building tree 614 of 3000\n",
      "building tree 615 of 3000\n",
      "building tree 616 of 3000\n",
      "building tree 617 of 3000\n",
      "building tree 618 of 3000\n",
      "building tree 619 of 3000\n",
      "building tree 620 of 3000\n",
      "building tree 621 of 3000building tree 622 of 3000\n",
      "\n",
      "building tree 623 of 3000building tree 624 of 3000\n",
      "\n",
      "building tree 625 of 3000\n",
      "building tree 626 of 3000\n",
      "building tree 627 of 3000\n",
      "building tree 628 of 3000\n",
      "building tree 629 of 3000\n",
      "building tree 630 of 3000\n",
      "building tree 631 of 3000\n",
      "building tree 632 of 3000\n",
      "building tree 633 of 3000\n",
      "building tree 634 of 3000\n",
      "building tree 635 of 3000\n",
      "building tree 636 of 3000\n",
      "building tree 637 of 3000\n",
      "building tree 638 of 3000\n",
      "building tree 639 of 3000\n",
      "building tree 640 of 3000\n",
      "building tree 641 of 3000\n",
      "building tree 642 of 3000\n",
      "building tree 643 of 3000\n",
      "building tree 644 of 3000\n",
      "building tree 645 of 3000\n",
      "building tree 646 of 3000\n",
      "building tree 647 of 3000\n",
      "building tree 648 of 3000\n",
      "building tree 649 of 3000\n",
      "building tree 650 of 3000\n",
      "building tree 651 of 3000\n",
      "building tree 652 of 3000\n",
      "building tree 653 of 3000\n",
      "building tree 654 of 3000\n",
      "building tree 655 of 3000\n",
      "building tree 656 of 3000\n",
      "building tree 657 of 3000\n",
      "building tree 658 of 3000\n",
      "building tree 659 of 3000\n",
      "building tree 660 of 3000\n",
      "building tree 661 of 3000\n",
      "building tree 662 of 3000\n",
      "building tree 663 of 3000\n",
      "building tree 664 of 3000\n",
      "building tree 665 of 3000\n",
      "building tree 666 of 3000\n",
      "building tree 667 of 3000\n",
      "building tree 668 of 3000\n",
      "building tree 669 of 3000\n",
      "building tree 670 of 3000\n",
      "building tree 671 of 3000\n",
      "building tree 672 of 3000\n",
      "building tree 673 of 3000\n",
      "building tree 674 of 3000building tree 675 of 3000\n",
      "\n",
      "building tree 676 of 3000\n",
      "building tree 677 of 3000\n",
      "building tree 678 of 3000\n",
      "building tree 679 of 3000\n",
      "building tree 680 of 3000\n",
      "building tree 681 of 3000\n",
      "building tree 682 of 3000\n",
      "building tree 683 of 3000\n",
      "building tree 684 of 3000\n",
      "building tree 685 of 3000\n",
      "building tree 686 of 3000\n",
      "building tree 687 of 3000\n",
      "building tree 688 of 3000\n",
      "building tree 689 of 3000\n",
      "building tree 690 of 3000\n",
      "building tree 691 of 3000\n",
      "building tree 692 of 3000\n",
      "building tree 693 of 3000\n",
      "building tree 694 of 3000\n",
      "building tree 695 of 3000\n",
      "building tree 696 of 3000\n",
      "building tree 697 of 3000\n",
      "building tree 698 of 3000\n",
      "building tree 699 of 3000\n",
      "building tree 700 of 3000\n",
      "building tree 701 of 3000\n",
      "building tree 702 of 3000\n",
      "building tree 703 of 3000\n",
      "building tree 704 of 3000\n",
      "building tree 705 of 3000\n",
      "building tree 706 of 3000\n",
      "building tree 707 of 3000\n",
      "building tree 708 of 3000\n",
      "building tree 709 of 3000\n",
      "building tree 710 of 3000\n",
      "building tree 711 of 3000\n",
      "building tree 712 of 3000\n",
      "building tree 713 of 3000\n",
      "building tree 714 of 3000\n",
      "building tree 715 of 3000\n",
      "building tree 716 of 3000\n",
      "building tree 717 of 3000\n",
      "building tree 718 of 3000\n",
      "building tree 719 of 3000\n",
      "building tree 720 of 3000\n",
      "building tree 721 of 3000\n",
      "building tree 722 of 3000\n",
      "building tree 723 of 3000\n",
      "building tree 724 of 3000\n",
      "building tree 725 of 3000\n",
      "building tree 726 of 3000\n",
      "building tree 727 of 3000\n",
      "building tree 728 of 3000\n",
      "building tree 729 of 3000\n",
      "building tree 730 of 3000\n",
      "building tree 731 of 3000\n",
      "building tree 732 of 3000\n",
      "building tree 733 of 3000\n",
      "building tree 734 of 3000\n",
      "building tree 735 of 3000\n",
      "building tree 736 of 3000\n",
      "building tree 737 of 3000\n",
      "building tree 738 of 3000\n",
      "building tree 739 of 3000\n",
      "building tree 740 of 3000\n",
      "building tree 741 of 3000\n",
      "building tree 742 of 3000\n",
      "building tree 743 of 3000\n",
      "building tree 744 of 3000\n",
      "building tree 745 of 3000\n",
      "building tree 746 of 3000\n",
      "building tree 747 of 3000\n",
      "building tree 748 of 3000\n",
      "building tree 749 of 3000\n",
      "building tree 750 of 3000\n",
      "building tree 751 of 3000\n",
      "building tree 752 of 3000\n",
      "building tree 753 of 3000\n",
      "building tree 754 of 3000\n",
      "building tree 755 of 3000\n",
      "building tree 756 of 3000\n",
      "building tree 757 of 3000\n",
      "building tree 758 of 3000\n",
      "building tree 759 of 3000\n",
      "building tree 760 of 3000\n",
      "building tree 761 of 3000\n",
      "building tree 762 of 3000\n",
      "building tree 763 of 3000\n",
      "building tree 764 of 3000\n",
      "building tree 765 of 3000\n",
      "building tree 766 of 3000\n",
      "building tree 767 of 3000\n",
      "building tree 768 of 3000\n",
      "building tree 769 of 3000\n",
      "building tree 770 of 3000\n",
      "building tree 771 of 3000\n",
      "building tree 772 of 3000\n",
      "building tree 773 of 3000\n",
      "building tree 774 of 3000\n",
      "building tree 775 of 3000\n",
      "building tree 776 of 3000\n",
      "building tree 777 of 3000\n",
      "building tree 778 of 3000\n",
      "building tree 779 of 3000\n",
      "building tree 780 of 3000\n",
      "building tree 781 of 3000\n",
      "building tree 782 of 3000\n",
      "building tree 783 of 3000\n",
      "building tree 784 of 3000\n",
      "building tree 785 of 3000\n",
      "building tree 786 of 3000\n",
      "building tree 787 of 3000\n",
      "building tree 788 of 3000\n",
      "building tree 789 of 3000\n",
      "building tree 790 of 3000\n",
      "building tree 791 of 3000\n",
      "building tree 792 of 3000\n",
      "building tree 793 of 3000\n",
      "building tree 794 of 3000\n",
      "building tree 795 of 3000\n",
      "building tree 796 of 3000\n",
      "building tree 797 of 3000\n",
      "building tree 798 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   56.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 799 of 3000\n",
      "building tree 800 of 3000\n",
      "building tree 801 of 3000\n",
      "building tree 802 of 3000\n",
      "building tree 803 of 3000\n",
      "building tree 804 of 3000\n",
      "building tree 805 of 3000\n",
      "building tree 806 of 3000\n",
      "building tree 807 of 3000\n",
      "building tree 808 of 3000\n",
      "building tree 809 of 3000\n",
      "building tree 810 of 3000\n",
      "building tree 811 of 3000\n",
      "building tree 812 of 3000\n",
      "building tree 813 of 3000\n",
      "building tree 814 of 3000\n",
      "building tree 815 of 3000\n",
      "building tree 816 of 3000\n",
      "building tree 817 of 3000\n",
      "building tree 818 of 3000\n",
      "building tree 819 of 3000\n",
      "building tree 820 of 3000\n",
      "building tree 821 of 3000\n",
      "building tree 822 of 3000\n",
      "building tree 823 of 3000\n",
      "building tree 824 of 3000\n",
      "building tree 825 of 3000\n",
      "building tree 826 of 3000\n",
      "building tree 827 of 3000\n",
      "building tree 828 of 3000\n",
      "building tree 829 of 3000\n",
      "building tree 830 of 3000\n",
      "building tree 831 of 3000\n",
      "building tree 832 of 3000\n",
      "building tree 833 of 3000\n",
      "building tree 834 of 3000\n",
      "building tree 835 of 3000\n",
      "building tree 836 of 3000\n",
      "building tree 837 of 3000\n",
      "building tree 838 of 3000\n",
      "building tree 839 of 3000\n",
      "building tree 840 of 3000\n",
      "building tree 841 of 3000\n",
      "building tree 842 of 3000\n",
      "building tree 843 of 3000\n",
      "building tree 844 of 3000\n",
      "building tree 845 of 3000\n",
      "building tree 846 of 3000\n",
      "building tree 847 of 3000\n",
      "building tree 848 of 3000\n",
      "building tree 849 of 3000\n",
      "building tree 850 of 3000\n",
      "building tree 851 of 3000\n",
      "building tree 852 of 3000\n",
      "building tree 853 of 3000\n",
      "building tree 854 of 3000\n",
      "building tree 855 of 3000\n",
      "building tree 856 of 3000\n",
      "building tree 857 of 3000\n",
      "building tree 858 of 3000\n",
      "building tree 859 of 3000\n",
      "building tree 860 of 3000\n",
      "building tree 861 of 3000\n",
      "building tree 862 of 3000\n",
      "building tree 863 of 3000\n",
      "building tree 864 of 3000\n",
      "building tree 865 of 3000\n",
      "building tree 866 of 3000\n",
      "building tree 867 of 3000\n",
      "building tree 868 of 3000\n",
      "building tree 869 of 3000\n",
      "building tree 870 of 3000\n",
      "building tree 871 of 3000\n",
      "building tree 872 of 3000\n",
      "building tree 873 of 3000\n",
      "building tree 874 of 3000\n",
      "building tree 875 of 3000\n",
      "building tree 876 of 3000\n",
      "building tree 877 of 3000\n",
      "building tree 878 of 3000\n",
      "building tree 879 of 3000\n",
      "building tree 880 of 3000\n",
      "building tree 881 of 3000\n",
      "building tree 882 of 3000\n",
      "building tree 883 of 3000\n",
      "building tree 884 of 3000\n",
      "building tree 885 of 3000\n",
      "building tree 886 of 3000\n",
      "building tree 887 of 3000\n",
      "building tree 888 of 3000\n",
      "building tree 889 of 3000\n",
      "building tree 890 of 3000\n",
      "building tree 891 of 3000\n",
      "building tree 892 of 3000\n",
      "building tree 893 of 3000\n",
      "building tree 894 of 3000\n",
      "building tree 895 of 3000\n",
      "building tree 896 of 3000\n",
      "building tree 897 of 3000\n",
      "building tree 898 of 3000\n",
      "building tree 899 of 3000\n",
      "building tree 900 of 3000\n",
      "building tree 901 of 3000\n",
      "building tree 902 of 3000\n",
      "building tree 903 of 3000\n",
      "building tree 904 of 3000\n",
      "building tree 905 of 3000\n",
      "building tree 906 of 3000\n",
      "building tree 907 of 3000\n",
      "building tree 908 of 3000\n",
      "building tree 909 of 3000\n",
      "building tree 910 of 3000\n",
      "building tree 911 of 3000\n",
      "building tree 912 of 3000\n",
      "building tree 913 of 3000\n",
      "building tree 914 of 3000\n",
      "building tree 915 of 3000\n",
      "building tree 916 of 3000\n",
      "building tree 917 of 3000\n",
      "building tree 918 of 3000\n",
      "building tree 919 of 3000\n",
      "building tree 920 of 3000\n",
      "building tree 921 of 3000\n",
      "building tree 922 of 3000\n",
      "building tree 923 of 3000\n",
      "building tree 924 of 3000\n",
      "building tree 925 of 3000\n",
      "building tree 926 of 3000\n",
      "building tree 927 of 3000\n",
      "building tree 928 of 3000\n",
      "building tree 929 of 3000\n",
      "building tree 930 of 3000\n",
      "building tree 931 of 3000\n",
      "building tree 932 of 3000\n",
      "building tree 933 of 3000\n",
      "building tree 934 of 3000\n",
      "building tree 935 of 3000\n",
      "building tree 936 of 3000\n",
      "building tree 937 of 3000\n",
      "building tree 938 of 3000\n",
      "building tree 939 of 3000\n",
      "building tree 940 of 3000\n",
      "building tree 941 of 3000\n",
      "building tree 942 of 3000\n",
      "building tree 943 of 3000\n",
      "building tree 944 of 3000\n",
      "building tree 945 of 3000\n",
      "building tree 946 of 3000\n",
      "building tree 947 of 3000\n",
      "building tree 948 of 3000\n",
      "building tree 949 of 3000\n",
      "building tree 950 of 3000\n",
      "building tree 951 of 3000\n",
      "building tree 952 of 3000\n",
      "building tree 953 of 3000\n",
      "building tree 954 of 3000\n",
      "building tree 955 of 3000\n",
      "building tree 956 of 3000\n",
      "building tree 957 of 3000\n",
      "building tree 958 of 3000\n",
      "building tree 959 of 3000\n",
      "building tree 960 of 3000\n",
      "building tree 961 of 3000\n",
      "building tree 962 of 3000\n",
      "building tree 963 of 3000\n",
      "building tree 964 of 3000\n",
      "building tree 965 of 3000\n",
      "building tree 966 of 3000\n",
      "building tree 967 of 3000\n",
      "building tree 968 of 3000\n",
      "building tree 969 of 3000\n",
      "building tree 970 of 3000\n",
      "building tree 971 of 3000\n",
      "building tree 972 of 3000\n",
      "building tree 973 of 3000\n",
      "building tree 974 of 3000\n",
      "building tree 975 of 3000\n",
      "building tree 976 of 3000\n",
      "building tree 977 of 3000\n",
      "building tree 978 of 3000\n",
      "building tree 979 of 3000\n",
      "building tree 980 of 3000\n",
      "building tree 981 of 3000\n",
      "building tree 982 of 3000\n",
      "building tree 983 of 3000\n",
      "building tree 984 of 3000\n",
      "building tree 985 of 3000\n",
      "building tree 986 of 3000\n",
      "building tree 987 of 3000\n",
      "building tree 988 of 3000\n",
      "building tree 989 of 3000\n",
      "building tree 990 of 3000\n",
      "building tree 991 of 3000\n",
      "building tree 992 of 3000\n",
      "building tree 993 of 3000\n",
      "building tree 994 of 3000\n",
      "building tree 995 of 3000\n",
      "building tree 996 of 3000\n",
      "building tree 997 of 3000\n",
      "building tree 998 of 3000\n",
      "building tree 999 of 3000\n",
      "building tree 1000 of 3000\n",
      "building tree 1001 of 3000\n",
      "building tree 1002 of 3000\n",
      "building tree 1003 of 3000\n",
      "building tree 1004 of 3000\n",
      "building tree 1005 of 3000\n",
      "building tree 1006 of 3000\n",
      "building tree 1007 of 3000\n",
      "building tree 1008 of 3000\n",
      "building tree 1009 of 3000\n",
      "building tree 1010 of 3000\n",
      "building tree 1011 of 3000\n",
      "building tree 1012 of 3000\n",
      "building tree 1013 of 3000\n",
      "building tree 1014 of 3000\n",
      "building tree 1015 of 3000\n",
      "building tree 1016 of 3000\n",
      "building tree 1017 of 3000\n",
      "building tree 1018 of 3000\n",
      "building tree 1019 of 3000\n",
      "building tree 1020 of 3000\n",
      "building tree 1021 of 3000\n",
      "building tree 1022 of 3000\n",
      "building tree 1023 of 3000\n",
      "building tree 1024 of 3000\n",
      "building tree 1025 of 3000\n",
      "building tree 1026 of 3000\n",
      "building tree 1027 of 3000\n",
      "building tree 1028 of 3000\n",
      "building tree 1029 of 3000\n",
      "building tree 1030 of 3000\n",
      "building tree 1031 of 3000\n",
      "building tree 1032 of 3000\n",
      "building tree 1033 of 3000\n",
      "building tree 1034 of 3000\n",
      "building tree 1035 of 3000\n",
      "building tree 1036 of 3000\n",
      "building tree 1037 of 3000\n",
      "building tree 1038 of 3000\n",
      "building tree 1039 of 3000\n",
      "building tree 1040 of 3000\n",
      "building tree 1041 of 3000\n",
      "building tree 1042 of 3000\n",
      "building tree 1043 of 3000\n",
      "building tree 1044 of 3000\n",
      "building tree 1045 of 3000\n",
      "building tree 1046 of 3000\n",
      "building tree 1047 of 3000\n",
      "building tree 1048 of 3000\n",
      "building tree 1049 of 3000\n",
      "building tree 1050 of 3000\n",
      "building tree 1051 of 3000\n",
      "building tree 1052 of 3000\n",
      "building tree 1053 of 3000\n",
      "building tree 1054 of 3000\n",
      "building tree 1055 of 3000\n",
      "building tree 1056 of 3000\n",
      "building tree 1057 of 3000\n",
      "building tree 1058 of 3000\n",
      "building tree 1059 of 3000\n",
      "building tree 1060 of 3000\n",
      "building tree 1061 of 3000\n",
      "building tree 1062 of 3000\n",
      "building tree 1063 of 3000\n",
      "building tree 1064 of 3000\n",
      "building tree 1065 of 3000\n",
      "building tree 1066 of 3000\n",
      "building tree 1067 of 3000\n",
      "building tree 1068 of 3000\n",
      "building tree 1069 of 3000\n",
      "building tree 1070 of 3000\n",
      "building tree 1071 of 3000\n",
      "building tree 1072 of 3000\n",
      "building tree 1073 of 3000\n",
      "building tree 1074 of 3000\n",
      "building tree 1075 of 3000\n",
      "building tree 1076 of 3000\n",
      "building tree 1077 of 3000\n",
      "building tree 1078 of 3000\n",
      "building tree 1079 of 3000\n",
      "building tree 1080 of 3000\n",
      "building tree 1081 of 3000\n",
      "building tree 1082 of 3000\n",
      "building tree 1083 of 3000\n",
      "building tree 1084 of 3000\n",
      "building tree 1085 of 3000\n",
      "building tree 1086 of 3000\n",
      "building tree 1087 of 3000\n",
      "building tree 1088 of 3000\n",
      "building tree 1089 of 3000\n",
      "building tree 1090 of 3000\n",
      "building tree 1091 of 3000\n",
      "building tree 1092 of 3000\n",
      "building tree 1093 of 3000\n",
      "building tree 1094 of 3000\n",
      "building tree 1095 of 3000\n",
      "building tree 1096 of 3000\n",
      "building tree 1097 of 3000\n",
      "building tree 1098 of 3000\n",
      "building tree 1099 of 3000\n",
      "building tree 1100 of 3000\n",
      "building tree 1101 of 3000\n",
      "building tree 1102 of 3000\n",
      "building tree 1103 of 3000\n",
      "building tree 1104 of 3000\n",
      "building tree 1105 of 3000\n",
      "building tree 1106 of 3000\n",
      "building tree 1107 of 3000\n",
      "building tree 1108 of 3000\n",
      "building tree 1109 of 3000\n",
      "building tree 1110 of 3000\n",
      "building tree 1111 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1112 of 3000\n",
      "building tree 1113 of 3000\n",
      "building tree 1114 of 3000\n",
      "building tree 1115 of 3000\n",
      "building tree 1116 of 3000\n",
      "building tree 1117 of 3000\n",
      "building tree 1118 of 3000\n",
      "building tree 1119 of 3000\n",
      "building tree 1120 of 3000\n",
      "building tree 1121 of 3000\n",
      "building tree 1122 of 3000\n",
      "building tree 1123 of 3000\n",
      "building tree 1124 of 3000\n",
      "building tree 1125 of 3000\n",
      "building tree 1126 of 3000\n",
      "building tree 1127 of 3000\n",
      "building tree 1128 of 3000\n",
      "building tree 1129 of 3000\n",
      "building tree 1130 of 3000\n",
      "building tree 1131 of 3000\n",
      "building tree 1132 of 3000\n",
      "building tree 1133 of 3000\n",
      "building tree 1134 of 3000\n",
      "building tree 1135 of 3000\n",
      "building tree 1136 of 3000\n",
      "building tree 1137 of 3000\n",
      "building tree 1138 of 3000\n",
      "building tree 1139 of 3000\n",
      "building tree 1140 of 3000\n",
      "building tree 1141 of 3000\n",
      "building tree 1142 of 3000\n",
      "building tree 1143 of 3000\n",
      "building tree 1144 of 3000\n",
      "building tree 1145 of 3000\n",
      "building tree 1146 of 3000\n",
      "building tree 1147 of 3000\n",
      "building tree 1148 of 3000\n",
      "building tree 1149 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1150 of 3000\n",
      "building tree 1151 of 3000\n",
      "building tree 1152 of 3000\n",
      "building tree 1153 of 3000\n",
      "building tree 1154 of 3000\n",
      "building tree 1155 of 3000\n",
      "building tree 1156 of 3000\n",
      "building tree 1157 of 3000\n",
      "building tree 1158 of 3000\n",
      "building tree 1159 of 3000\n",
      "building tree 1160 of 3000\n",
      "building tree 1161 of 3000\n",
      "building tree 1162 of 3000\n",
      "building tree 1163 of 3000\n",
      "building tree 1164 of 3000\n",
      "building tree 1165 of 3000\n",
      "building tree 1166 of 3000\n",
      "building tree 1167 of 3000\n",
      "building tree 1168 of 3000\n",
      "building tree 1169 of 3000\n",
      "building tree 1170 of 3000\n",
      "building tree 1171 of 3000\n",
      "building tree 1172 of 3000\n",
      "building tree 1173 of 3000\n",
      "building tree 1174 of 3000\n",
      "building tree 1175 of 3000\n",
      "building tree 1176 of 3000\n",
      "building tree 1177 of 3000\n",
      "building tree 1178 of 3000\n",
      "building tree 1179 of 3000\n",
      "building tree 1180 of 3000\n",
      "building tree 1181 of 3000\n",
      "building tree 1182 of 3000\n",
      "building tree 1183 of 3000\n",
      "building tree 1184 of 3000\n",
      "building tree 1185 of 3000\n",
      "building tree 1186 of 3000\n",
      "building tree 1187 of 3000\n",
      "building tree 1188 of 3000\n",
      "building tree 1189 of 3000building tree 1190 of 3000\n",
      "\n",
      "building tree 1191 of 3000\n",
      "building tree 1192 of 3000\n",
      "building tree 1193 of 3000\n",
      "building tree 1194 of 3000\n",
      "building tree 1195 of 3000\n",
      "building tree 1196 of 3000\n",
      "building tree 1197 of 3000\n",
      "building tree 1198 of 3000\n",
      "building tree 1199 of 3000\n",
      "building tree 1200 of 3000\n",
      "building tree 1201 of 3000\n",
      "building tree 1202 of 3000\n",
      "building tree 1203 of 3000\n",
      "building tree 1204 of 3000\n",
      "building tree 1205 of 3000\n",
      "building tree 1206 of 3000\n",
      "building tree 1207 of 3000\n",
      "building tree 1208 of 3000\n",
      "building tree 1209 of 3000\n",
      "building tree 1210 of 3000\n",
      "building tree 1211 of 3000\n",
      "building tree 1212 of 3000\n",
      "building tree 1213 of 3000\n",
      "building tree 1214 of 3000\n",
      "building tree 1215 of 3000\n",
      "building tree 1216 of 3000\n",
      "building tree 1217 of 3000\n",
      "building tree 1218 of 3000\n",
      "building tree 1219 of 3000\n",
      "building tree 1220 of 3000\n",
      "building tree 1221 of 3000\n",
      "building tree 1222 of 3000\n",
      "building tree 1223 of 3000\n",
      "building tree 1224 of 3000\n",
      "building tree 1225 of 3000\n",
      "building tree 1226 of 3000\n",
      "building tree 1227 of 3000\n",
      "building tree 1228 of 3000\n",
      "building tree 1229 of 3000\n",
      "building tree 1230 of 3000\n",
      "building tree 1231 of 3000\n",
      "building tree 1232 of 3000\n",
      "building tree 1233 of 3000\n",
      "building tree 1234 of 3000\n",
      "building tree 1235 of 3000\n",
      "building tree 1236 of 3000\n",
      "building tree 1237 of 3000\n",
      "building tree 1238 of 3000\n",
      "building tree 1239 of 3000\n",
      "building tree 1240 of 3000\n",
      "building tree 1241 of 3000\n",
      "building tree 1242 of 3000\n",
      "building tree 1243 of 3000\n",
      "building tree 1244 of 3000\n",
      "building tree 1245 of 3000\n",
      "building tree 1246 of 3000\n",
      "building tree 1247 of 3000\n",
      "building tree 1248 of 3000\n",
      "building tree 1249 of 3000\n",
      "building tree 1250 of 3000\n",
      "building tree 1251 of 3000\n",
      "building tree 1252 of 3000\n",
      "building tree 1253 of 3000\n",
      "building tree 1254 of 3000\n",
      "building tree 1255 of 3000\n",
      "building tree 1256 of 3000\n",
      "building tree 1257 of 3000\n",
      "building tree 1258 of 3000\n",
      "building tree 1259 of 3000\n",
      "building tree 1260 of 3000\n",
      "building tree 1261 of 3000\n",
      "building tree 1262 of 3000\n",
      "building tree 1263 of 3000\n",
      "building tree 1264 of 3000\n",
      "building tree 1265 of 3000\n",
      "building tree 1266 of 3000\n",
      "building tree 1267 of 3000\n",
      "building tree 1268 of 3000\n",
      "building tree 1269 of 3000\n",
      "building tree 1270 of 3000\n",
      "building tree 1271 of 3000\n",
      "building tree 1272 of 3000\n",
      "building tree 1273 of 3000\n",
      "building tree 1274 of 3000\n",
      "building tree 1275 of 3000\n",
      "building tree 1276 of 3000\n",
      "building tree 1277 of 3000\n",
      "building tree 1278 of 3000\n",
      "building tree 1279 of 3000\n",
      "building tree 1280 of 3000\n",
      "building tree 1281 of 3000\n",
      "building tree 1282 of 3000\n",
      "building tree 1283 of 3000\n",
      "building tree 1284 of 3000\n",
      "building tree 1285 of 3000\n",
      "building tree 1286 of 3000\n",
      "building tree 1287 of 3000\n",
      "building tree 1288 of 3000\n",
      "building tree 1289 of 3000\n",
      "building tree 1290 of 3000\n",
      "building tree 1291 of 3000\n",
      "building tree 1292 of 3000\n",
      "building tree 1293 of 3000\n",
      "building tree 1294 of 3000\n",
      "building tree 1295 of 3000\n",
      "building tree 1296 of 3000\n",
      "building tree 1297 of 3000\n",
      "building tree 1298 of 3000\n",
      "building tree 1299 of 3000\n",
      "building tree 1300 of 3000\n",
      "building tree 1301 of 3000\n",
      "building tree 1302 of 3000\n",
      "building tree 1303 of 3000\n",
      "building tree 1304 of 3000\n",
      "building tree 1305 of 3000\n",
      "building tree 1306 of 3000\n",
      "building tree 1307 of 3000\n",
      "building tree 1308 of 3000\n",
      "building tree 1309 of 3000\n",
      "building tree 1310 of 3000\n",
      "building tree 1311 of 3000\n",
      "building tree 1312 of 3000\n",
      "building tree 1313 of 3000\n",
      "building tree 1314 of 3000\n",
      "building tree 1315 of 3000\n",
      "building tree 1316 of 3000\n",
      "building tree 1317 of 3000\n",
      "building tree 1318 of 3000\n",
      "building tree 1319 of 3000\n",
      "building tree 1320 of 3000\n",
      "building tree 1321 of 3000\n",
      "building tree 1322 of 3000\n",
      "building tree 1323 of 3000\n",
      "building tree 1324 of 3000\n",
      "building tree 1325 of 3000\n",
      "building tree 1326 of 3000\n",
      "building tree 1327 of 3000\n",
      "building tree 1328 of 3000\n",
      "building tree 1329 of 3000\n",
      "building tree 1330 of 3000\n",
      "building tree 1331 of 3000\n",
      "building tree 1332 of 3000\n",
      "building tree 1333 of 3000\n",
      "building tree 1334 of 3000\n",
      "building tree 1335 of 3000\n",
      "building tree 1336 of 3000\n",
      "building tree 1337 of 3000\n",
      "building tree 1338 of 3000\n",
      "building tree 1339 of 3000\n",
      "building tree 1340 of 3000\n",
      "building tree 1341 of 3000\n",
      "building tree 1342 of 3000\n",
      "building tree 1343 of 3000\n",
      "building tree 1344 of 3000\n",
      "building tree 1345 of 3000\n",
      "building tree 1346 of 3000\n",
      "building tree 1347 of 3000\n",
      "building tree 1348 of 3000\n",
      "building tree 1349 of 3000\n",
      "building tree 1350 of 3000\n",
      "building tree 1351 of 3000\n",
      "building tree 1352 of 3000\n",
      "building tree 1353 of 3000\n",
      "building tree 1354 of 3000\n",
      "building tree 1355 of 3000\n",
      "building tree 1356 of 3000\n",
      "building tree 1357 of 3000\n",
      "building tree 1358 of 3000\n",
      "building tree 1359 of 3000\n",
      "building tree 1360 of 3000\n",
      "building tree 1361 of 3000\n",
      "building tree 1362 of 3000\n",
      "building tree 1363 of 3000\n",
      "building tree 1364 of 3000\n",
      "building tree 1365 of 3000\n",
      "building tree 1366 of 3000\n",
      "building tree 1367 of 3000\n",
      "building tree 1368 of 3000\n",
      "building tree 1369 of 3000\n",
      "building tree 1370 of 3000\n",
      "building tree 1371 of 3000\n",
      "building tree 1372 of 3000\n",
      "building tree 1373 of 3000\n",
      "building tree 1374 of 3000\n",
      "building tree 1375 of 3000\n",
      "building tree 1376 of 3000\n",
      "building tree 1377 of 3000\n",
      "building tree 1378 of 3000\n",
      "building tree 1379 of 3000\n",
      "building tree 1380 of 3000\n",
      "building tree 1381 of 3000\n",
      "building tree 1382 of 3000\n",
      "building tree 1383 of 3000\n",
      "building tree 1384 of 3000\n",
      "building tree 1385 of 3000\n",
      "building tree 1386 of 3000\n",
      "building tree 1387 of 3000\n",
      "building tree 1388 of 3000\n",
      "building tree 1389 of 3000\n",
      "building tree 1390 of 3000\n",
      "building tree 1391 of 3000\n",
      "building tree 1392 of 3000\n",
      "building tree 1393 of 3000\n",
      "building tree 1394 of 3000\n",
      "building tree 1395 of 3000\n",
      "building tree 1396 of 3000\n",
      "building tree 1397 of 3000\n",
      "building tree 1398 of 3000\n",
      "building tree 1399 of 3000\n",
      "building tree 1400 of 3000\n",
      "building tree 1401 of 3000\n",
      "building tree 1402 of 3000\n",
      "building tree 1403 of 3000\n",
      "building tree 1404 of 3000\n",
      "building tree 1405 of 3000\n",
      "building tree 1406 of 3000\n",
      "building tree 1407 of 3000\n",
      "building tree 1408 of 3000\n",
      "building tree 1409 of 3000\n",
      "building tree 1410 of 3000\n",
      "building tree 1411 of 3000\n",
      "building tree 1412 of 3000\n",
      "building tree 1413 of 3000\n",
      "building tree 1414 of 3000\n",
      "building tree 1415 of 3000\n",
      "building tree 1416 of 3000\n",
      "building tree 1417 of 3000\n",
      "building tree 1418 of 3000\n",
      "building tree 1419 of 3000\n",
      "building tree 1420 of 3000\n",
      "building tree 1421 of 3000building tree 1422 of 3000\n",
      "\n",
      "building tree 1423 of 3000\n",
      "building tree 1424 of 3000\n",
      "building tree 1425 of 3000\n",
      "building tree 1426 of 3000\n",
      "building tree 1427 of 3000\n",
      "building tree 1428 of 3000\n",
      "building tree 1429 of 3000\n",
      "building tree 1430 of 3000\n",
      "building tree 1431 of 3000\n",
      "building tree 1432 of 3000\n",
      "building tree 1433 of 3000\n",
      "building tree 1434 of 3000\n",
      "building tree 1435 of 3000\n",
      "building tree 1436 of 3000\n",
      "building tree 1437 of 3000\n",
      "building tree 1438 of 3000\n",
      "building tree 1439 of 3000\n",
      "building tree 1440 of 3000\n",
      "building tree 1441 of 3000\n",
      "building tree 1442 of 3000\n",
      "building tree 1443 of 3000\n",
      "building tree 1444 of 3000\n",
      "building tree 1445 of 3000building tree 1446 of 3000\n",
      "building tree 1447 of 3000\n",
      "\n",
      "building tree 1448 of 3000\n",
      "building tree 1449 of 3000\n",
      "building tree 1450 of 3000\n",
      "building tree 1451 of 3000\n",
      "building tree 1452 of 3000\n",
      "building tree 1453 of 3000\n",
      "building tree 1454 of 3000\n",
      "building tree 1455 of 3000\n",
      "building tree 1456 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1457 of 3000\n",
      "building tree 1458 of 3000\n",
      "building tree 1459 of 3000\n",
      "building tree 1460 of 3000\n",
      "building tree 1461 of 3000\n",
      "building tree 1462 of 3000\n",
      "building tree 1463 of 3000\n",
      "building tree 1464 of 3000\n",
      "building tree 1465 of 3000\n",
      "building tree 1466 of 3000\n",
      "building tree 1467 of 3000\n",
      "building tree 1468 of 3000\n",
      "building tree 1469 of 3000\n",
      "building tree 1470 of 3000\n",
      "building tree 1471 of 3000\n",
      "building tree 1472 of 3000\n",
      "building tree 1473 of 3000building tree 1474 of 3000\n",
      "\n",
      "building tree 1475 of 3000\n",
      "building tree 1476 of 3000\n",
      "building tree 1477 of 3000\n",
      "building tree 1478 of 3000\n",
      "building tree 1479 of 3000\n",
      "building tree 1480 of 3000\n",
      "building tree 1481 of 3000\n",
      "building tree 1482 of 3000\n",
      "building tree 1483 of 3000\n",
      "building tree 1484 of 3000\n",
      "building tree 1485 of 3000\n",
      "building tree 1486 of 3000\n",
      "building tree 1487 of 3000\n",
      "building tree 1488 of 3000\n",
      "building tree 1489 of 3000\n",
      "building tree 1490 of 3000\n",
      "building tree 1491 of 3000\n",
      "building tree 1492 of 3000\n",
      "building tree 1493 of 3000\n",
      "building tree 1494 of 3000\n",
      "building tree 1495 of 3000\n",
      "building tree 1496 of 3000\n",
      "building tree 1497 of 3000\n",
      "building tree 1498 of 3000\n",
      "building tree 1499 of 3000\n",
      "building tree 1500 of 3000\n",
      "building tree 1501 of 3000\n",
      "building tree 1502 of 3000\n",
      "building tree 1503 of 3000\n",
      "building tree 1504 of 3000\n",
      "building tree 1505 of 3000\n",
      "building tree 1506 of 3000\n",
      "building tree 1507 of 3000\n",
      "building tree 1508 of 3000\n",
      "building tree 1509 of 3000building tree 1510 of 3000\n",
      "\n",
      "building tree 1511 of 3000\n",
      "building tree 1512 of 3000\n",
      "building tree 1513 of 3000\n",
      "building tree 1514 of 3000\n",
      "building tree 1515 of 3000\n",
      "building tree 1516 of 3000\n",
      "building tree 1517 of 3000\n",
      "building tree 1518 of 3000\n",
      "building tree 1519 of 3000\n",
      "building tree 1520 of 3000\n",
      "building tree 1521 of 3000\n",
      "building tree 1522 of 3000\n",
      "building tree 1523 of 3000\n",
      "building tree 1524 of 3000\n",
      "building tree 1525 of 3000\n",
      "building tree 1526 of 3000\n",
      "building tree 1527 of 3000\n",
      "building tree 1528 of 3000\n",
      "building tree 1529 of 3000\n",
      "building tree 1530 of 3000\n",
      "building tree 1531 of 3000\n",
      "building tree 1532 of 3000\n",
      "building tree 1533 of 3000\n",
      "building tree 1534 of 3000\n",
      "building tree 1535 of 3000\n",
      "building tree 1536 of 3000\n",
      "building tree 1537 of 3000\n",
      "building tree 1538 of 3000\n",
      "building tree 1539 of 3000\n",
      "building tree 1540 of 3000\n",
      "building tree 1541 of 3000\n",
      "building tree 1542 of 3000\n",
      "building tree 1543 of 3000\n",
      "building tree 1544 of 3000\n",
      "building tree 1545 of 3000\n",
      "building tree 1546 of 3000\n",
      "building tree 1547 of 3000\n",
      "building tree 1548 of 3000\n",
      "building tree 1549 of 3000\n",
      "building tree 1550 of 3000\n",
      "building tree 1551 of 3000\n",
      "building tree 1552 of 3000\n",
      "building tree 1553 of 3000\n",
      "building tree 1554 of 3000\n",
      "building tree 1555 of 3000\n",
      "building tree 1556 of 3000\n",
      "building tree 1557 of 3000\n",
      "building tree 1558 of 3000\n",
      "building tree 1559 of 3000\n",
      "building tree 1560 of 3000\n",
      "building tree 1561 of 3000\n",
      "building tree 1562 of 3000\n",
      "building tree 1563 of 3000\n",
      "building tree 1564 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed:  1.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1565 of 3000\n",
      "building tree 1566 of 3000\n",
      "building tree 1567 of 3000\n",
      "building tree 1568 of 3000\n",
      "building tree 1569 of 3000\n",
      "building tree 1570 of 3000\n",
      "building tree 1571 of 3000\n",
      "building tree 1572 of 3000\n",
      "building tree 1573 of 3000\n",
      "building tree 1574 of 3000\n",
      "building tree 1575 of 3000\n",
      "building tree 1576 of 3000\n",
      "building tree 1577 of 3000\n",
      "building tree 1578 of 3000\n",
      "building tree 1579 of 3000\n",
      "building tree 1580 of 3000\n",
      "building tree 1581 of 3000\n",
      "building tree 1582 of 3000\n",
      "building tree 1583 of 3000\n",
      "building tree 1584 of 3000\n",
      "building tree 1585 of 3000\n",
      "building tree 1586 of 3000\n",
      "building tree 1587 of 3000\n",
      "building tree 1588 of 3000\n",
      "building tree 1589 of 3000\n",
      "building tree 1590 of 3000\n",
      "building tree 1591 of 3000\n",
      "building tree 1592 of 3000\n",
      "building tree 1593 of 3000\n",
      "building tree 1594 of 3000\n",
      "building tree 1595 of 3000\n",
      "building tree 1596 of 3000\n",
      "building tree 1597 of 3000\n",
      "building tree 1598 of 3000\n",
      "building tree 1599 of 3000\n",
      "building tree 1600 of 3000\n",
      "building tree 1601 of 3000\n",
      "building tree 1602 of 3000\n",
      "building tree 1603 of 3000\n",
      "building tree 1604 of 3000\n",
      "building tree 1605 of 3000\n",
      "building tree 1606 of 3000\n",
      "building tree 1607 of 3000\n",
      "building tree 1608 of 3000\n",
      "building tree 1609 of 3000\n",
      "building tree 1610 of 3000\n",
      "building tree 1611 of 3000\n",
      "building tree 1612 of 3000\n",
      "building tree 1613 of 3000\n",
      "building tree 1614 of 3000\n",
      "building tree 1615 of 3000\n",
      "building tree 1616 of 3000\n",
      "building tree 1617 of 3000\n",
      "building tree 1618 of 3000\n",
      "building tree 1619 of 3000\n",
      "building tree 1620 of 3000\n",
      "building tree 1621 of 3000\n",
      "building tree 1622 of 3000\n",
      "building tree 1623 of 3000\n",
      "building tree 1624 of 3000\n",
      "building tree 1625 of 3000\n",
      "building tree 1626 of 3000\n",
      "building tree 1627 of 3000\n",
      "building tree 1628 of 3000\n",
      "building tree 1629 of 3000\n",
      "building tree 1630 of 3000\n",
      "building tree 1631 of 3000\n",
      "building tree 1632 of 3000\n",
      "building tree 1633 of 3000\n",
      "building tree 1634 of 3000\n",
      "building tree 1635 of 3000\n",
      "building tree 1636 of 3000\n",
      "building tree 1637 of 3000\n",
      "building tree 1638 of 3000\n",
      "building tree 1639 of 3000\n",
      "building tree 1640 of 3000\n",
      "building tree 1641 of 3000\n",
      "building tree 1642 of 3000\n",
      "building tree 1643 of 3000\n",
      "building tree 1644 of 3000\n",
      "building tree 1645 of 3000\n",
      "building tree 1646 of 3000\n",
      "building tree 1647 of 3000\n",
      "building tree 1648 of 3000\n",
      "building tree 1649 of 3000\n",
      "building tree 1650 of 3000\n",
      "building tree 1651 of 3000\n",
      "building tree 1652 of 3000\n",
      "building tree 1653 of 3000\n",
      "building tree 1654 of 3000\n",
      "building tree 1655 of 3000\n",
      "building tree 1656 of 3000\n",
      "building tree 1657 of 3000\n",
      "building tree 1658 of 3000\n",
      "building tree 1659 of 3000\n",
      "building tree 1660 of 3000\n",
      "building tree 1661 of 3000\n",
      "building tree 1662 of 3000\n",
      "building tree 1663 of 3000\n",
      "building tree 1664 of 3000\n",
      "building tree 1665 of 3000\n",
      "building tree 1666 of 3000\n",
      "building tree 1667 of 3000\n",
      "building tree 1668 of 3000\n",
      "building tree 1669 of 3000\n",
      "building tree 1670 of 3000\n",
      "building tree 1671 of 3000\n",
      "building tree 1672 of 3000\n",
      "building tree 1673 of 3000\n",
      "building tree 1674 of 3000\n",
      "building tree 1675 of 3000\n",
      "building tree 1676 of 3000\n",
      "building tree 1677 of 3000\n",
      "building tree 1678 of 3000\n",
      "building tree 1679 of 3000\n",
      "building tree 1680 of 3000\n",
      "building tree 1681 of 3000\n",
      "building tree 1682 of 3000\n",
      "building tree 1683 of 3000\n",
      "building tree 1684 of 3000\n",
      "building tree 1685 of 3000\n",
      "building tree 1686 of 3000\n",
      "building tree 1687 of 3000\n",
      "building tree 1688 of 3000\n",
      "building tree 1689 of 3000\n",
      "building tree 1690 of 3000\n",
      "building tree 1691 of 3000\n",
      "building tree 1692 of 3000\n",
      "building tree 1693 of 3000\n",
      "building tree 1694 of 3000\n",
      "building tree 1695 of 3000\n",
      "building tree 1696 of 3000\n",
      "building tree 1697 of 3000\n",
      "building tree 1698 of 3000\n",
      "building tree 1699 of 3000\n",
      "building tree 1700 of 3000\n",
      "building tree 1701 of 3000\n",
      "building tree 1702 of 3000\n",
      "building tree 1703 of 3000\n",
      "building tree 1704 of 3000\n",
      "building tree 1705 of 3000\n",
      "building tree 1706 of 3000\n",
      "building tree 1707 of 3000\n",
      "building tree 1708 of 3000\n",
      "building tree 1709 of 3000\n",
      "building tree 1710 of 3000\n",
      "building tree 1711 of 3000\n",
      "building tree 1712 of 3000\n",
      "building tree 1713 of 3000\n",
      "building tree 1714 of 3000\n",
      "building tree 1715 of 3000\n",
      "building tree 1716 of 3000\n",
      "building tree 1717 of 3000\n",
      "building tree 1718 of 3000\n",
      "building tree 1719 of 3000\n",
      "building tree 1720 of 3000\n",
      "building tree 1721 of 3000\n",
      "building tree 1722 of 3000\n",
      "building tree 1723 of 3000\n",
      "building tree 1724 of 3000\n",
      "building tree 1725 of 3000\n",
      "building tree 1726 of 3000\n",
      "building tree 1727 of 3000\n",
      "building tree 1728 of 3000\n",
      "building tree 1729 of 3000\n",
      "building tree 1730 of 3000\n",
      "building tree 1731 of 3000\n",
      "building tree 1732 of 3000\n",
      "building tree 1733 of 3000\n",
      "building tree 1734 of 3000\n",
      "building tree 1735 of 3000\n",
      "building tree 1736 of 3000\n",
      "building tree 1737 of 3000\n",
      "building tree 1738 of 3000\n",
      "building tree 1739 of 3000\n",
      "building tree 1740 of 3000\n",
      "building tree 1741 of 3000\n",
      "building tree 1742 of 3000\n",
      "building tree 1743 of 3000\n",
      "building tree 1744 of 3000\n",
      "building tree 1745 of 3000\n",
      "building tree 1746 of 3000\n",
      "building tree 1747 of 3000\n",
      "building tree 1748 of 3000\n",
      "building tree 1749 of 3000\n",
      "building tree 1750 of 3000\n",
      "building tree 1751 of 3000\n",
      "building tree 1752 of 3000\n",
      "building tree 1753 of 3000\n",
      "building tree 1754 of 3000\n",
      "building tree 1755 of 3000\n",
      "building tree 1756 of 3000\n",
      "building tree 1757 of 3000\n",
      "building tree 1758 of 3000\n",
      "building tree 1759 of 3000\n",
      "building tree 1760 of 3000\n",
      "building tree 1761 of 3000\n",
      "building tree 1762 of 3000\n",
      "building tree 1763 of 3000\n",
      "building tree 1764 of 3000\n",
      "building tree 1765 of 3000\n",
      "building tree 1766 of 3000\n",
      "building tree 1767 of 3000\n",
      "building tree 1768 of 3000\n",
      "building tree 1769 of 3000\n",
      "building tree 1770 of 3000\n",
      "building tree 1771 of 3000\n",
      "building tree 1772 of 3000\n",
      "building tree 1773 of 3000\n",
      "building tree 1774 of 3000\n",
      "building tree 1775 of 3000\n",
      "building tree 1776 of 3000\n",
      "building tree 1777 of 3000\n",
      "building tree 1778 of 3000\n",
      "building tree 1779 of 3000\n",
      "building tree 1780 of 3000\n",
      "building tree 1781 of 3000\n",
      "building tree 1782 of 3000\n",
      "building tree 1783 of 3000\n",
      "building tree 1784 of 3000\n",
      "building tree 1785 of 3000\n",
      "building tree 1786 of 3000\n",
      "building tree 1787 of 3000\n",
      "building tree 1788 of 3000\n",
      "building tree 1789 of 3000\n",
      "building tree 1790 of 3000\n",
      "building tree 1791 of 3000\n",
      "building tree 1792 of 3000\n",
      "building tree 1793 of 3000\n",
      "building tree 1794 of 3000\n",
      "building tree 1795 of 3000\n",
      "building tree 1796 of 3000\n",
      "building tree 1797 of 3000\n",
      "building tree 1798 of 3000\n",
      "building tree 1799 of 3000\n",
      "building tree 1800 of 3000\n",
      "building tree 1801 of 3000\n",
      "building tree 1802 of 3000\n",
      "building tree 1803 of 3000\n",
      "building tree 1804 of 3000\n",
      "building tree 1805 of 3000\n",
      "building tree 1806 of 3000\n",
      "building tree 1807 of 3000\n",
      "building tree 1808 of 3000\n",
      "building tree 1809 of 3000\n",
      "building tree 1810 of 3000\n",
      "building tree 1811 of 3000\n",
      "building tree 1812 of 3000\n",
      "building tree 1813 of 3000\n",
      "building tree 1814 of 3000\n",
      "building tree 1815 of 3000\n",
      "building tree 1816 of 3000\n",
      "building tree 1817 of 3000\n",
      "building tree 1818 of 3000\n",
      "building tree 1819 of 3000\n",
      "building tree 1820 of 3000\n",
      "building tree 1821 of 3000\n",
      "building tree 1822 of 3000\n",
      "building tree 1823 of 3000\n",
      "building tree 1824 of 3000\n",
      "building tree 1825 of 3000\n",
      "building tree 1826 of 3000\n",
      "building tree 1827 of 3000\n",
      "building tree 1828 of 3000\n",
      "building tree 1829 of 3000\n",
      "building tree 1830 of 3000\n",
      "building tree 1831 of 3000\n",
      "building tree 1832 of 3000\n",
      "building tree 1833 of 3000\n",
      "building tree 1834 of 3000\n",
      "building tree 1835 of 3000\n",
      "building tree 1836 of 3000\n",
      "building tree 1837 of 3000\n",
      "building tree 1838 of 3000building tree 1839 of 3000\n",
      "\n",
      "building tree 1840 of 3000\n",
      "building tree 1841 of 3000\n",
      "building tree 1842 of 3000\n",
      "building tree 1843 of 3000\n",
      "building tree 1844 of 3000\n",
      "building tree 1845 of 3000\n",
      "building tree 1846 of 3000\n",
      "building tree 1847 of 3000\n",
      "building tree 1848 of 3000\n",
      "building tree 1849 of 3000\n",
      "building tree 1850 of 3000\n",
      "building tree 1851 of 3000\n",
      "building tree 1852 of 3000\n",
      "building tree 1853 of 3000\n",
      "building tree 1854 of 3000\n",
      "building tree 1855 of 3000\n",
      "building tree 1856 of 3000\n",
      "building tree 1857 of 3000\n",
      "building tree 1858 of 3000\n",
      "building tree 1859 of 3000\n",
      "building tree 1860 of 3000\n",
      "building tree 1861 of 3000\n",
      "building tree 1862 of 3000\n",
      "building tree 1863 of 3000\n",
      "building tree 1864 of 3000\n",
      "building tree 1865 of 3000\n",
      "building tree 1866 of 3000\n",
      "building tree 1867 of 3000\n",
      "building tree 1868 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1869 of 3000\n",
      "building tree 1870 of 3000\n",
      "building tree 1871 of 3000\n",
      "building tree 1872 of 3000\n",
      "building tree 1873 of 3000\n",
      "building tree 1874 of 3000\n",
      "building tree 1875 of 3000\n",
      "building tree 1876 of 3000\n",
      "building tree 1877 of 3000\n",
      "building tree 1878 of 3000\n",
      "building tree 1879 of 3000\n",
      "building tree 1880 of 3000\n",
      "building tree 1881 of 3000\n",
      "building tree 1882 of 3000\n",
      "building tree 1883 of 3000\n",
      "building tree 1884 of 3000\n",
      "building tree 1885 of 3000\n",
      "building tree 1886 of 3000\n",
      "building tree 1887 of 3000\n",
      "building tree 1888 of 3000\n",
      "building tree 1889 of 3000\n",
      "building tree 1890 of 3000\n",
      "building tree 1891 of 3000\n",
      "building tree 1892 of 3000\n",
      "building tree 1893 of 3000\n",
      "building tree 1894 of 3000\n",
      "building tree 1895 of 3000\n",
      "building tree 1896 of 3000\n",
      "building tree 1897 of 3000\n",
      "building tree 1898 of 3000\n",
      "building tree 1899 of 3000\n",
      "building tree 1900 of 3000\n",
      "building tree 1901 of 3000\n",
      "building tree 1902 of 3000\n",
      "building tree 1903 of 3000\n",
      "building tree 1904 of 3000\n",
      "building tree 1905 of 3000\n",
      "building tree 1906 of 3000\n",
      "building tree 1907 of 3000\n",
      "building tree 1908 of 3000\n",
      "building tree 1909 of 3000\n",
      "building tree 1910 of 3000\n",
      "building tree 1911 of 3000\n",
      "building tree 1912 of 3000\n",
      "building tree 1913 of 3000\n",
      "building tree 1914 of 3000\n",
      "building tree 1915 of 3000\n",
      "building tree 1916 of 3000\n",
      "building tree 1917 of 3000\n",
      "building tree 1918 of 3000\n",
      "building tree 1919 of 3000\n",
      "building tree 1920 of 3000\n",
      "building tree 1921 of 3000\n",
      "building tree 1922 of 3000\n",
      "building tree 1923 of 3000\n",
      "building tree 1924 of 3000\n",
      "building tree 1925 of 3000\n",
      "building tree 1926 of 3000\n",
      "building tree 1927 of 3000\n",
      "building tree 1928 of 3000\n",
      "building tree 1929 of 3000\n",
      "building tree 1930 of 3000\n",
      "building tree 1931 of 3000\n",
      "building tree 1932 of 3000\n",
      "building tree 1933 of 3000\n",
      "building tree 1934 of 3000\n",
      "building tree 1935 of 3000\n",
      "building tree 1936 of 3000\n",
      "building tree 1937 of 3000\n",
      "building tree 1938 of 3000\n",
      "building tree 1939 of 3000\n",
      "building tree 1940 of 3000\n",
      "building tree 1941 of 3000\n",
      "building tree 1942 of 3000\n",
      "building tree 1943 of 3000\n",
      "building tree 1944 of 3000\n",
      "building tree 1945 of 3000\n",
      "building tree 1946 of 3000\n",
      "building tree 1947 of 3000\n",
      "building tree 1948 of 3000\n",
      "building tree 1949 of 3000\n",
      "building tree 1950 of 3000\n",
      "building tree 1951 of 3000\n",
      "building tree 1952 of 3000\n",
      "building tree 1953 of 3000\n",
      "building tree 1954 of 3000\n",
      "building tree 1955 of 3000\n",
      "building tree 1956 of 3000\n",
      "building tree 1957 of 3000\n",
      "building tree 1958 of 3000\n",
      "building tree 1959 of 3000\n",
      "building tree 1960 of 3000\n",
      "building tree 1961 of 3000\n",
      "building tree 1962 of 3000\n",
      "building tree 1963 of 3000\n",
      "building tree 1964 of 3000\n",
      "building tree 1965 of 3000\n",
      "building tree 1966 of 3000\n",
      "building tree 1967 of 3000\n",
      "building tree 1968 of 3000\n",
      "building tree 1969 of 3000\n",
      "building tree 1970 of 3000\n",
      "building tree 1971 of 3000\n",
      "building tree 1972 of 3000\n",
      "building tree 1973 of 3000\n",
      "building tree 1974 of 3000\n",
      "building tree 1975 of 3000\n",
      "building tree 1976 of 3000\n",
      "building tree 1977 of 3000\n",
      "building tree 1978 of 3000\n",
      "building tree 1979 of 3000\n",
      "building tree 1980 of 3000\n",
      "building tree 1981 of 3000\n",
      "building tree 1982 of 3000\n",
      "building tree 1983 of 3000\n",
      "building tree 1984 of 3000\n",
      "building tree 1985 of 3000\n",
      "building tree 1986 of 3000\n",
      "building tree 1987 of 3000\n",
      "building tree 1988 of 3000\n",
      "building tree 1989 of 3000\n",
      "building tree 1990 of 3000\n",
      "building tree 1991 of 3000\n",
      "building tree 1992 of 3000\n",
      "building tree 1993 of 3000\n",
      "building tree 1994 of 3000\n",
      "building tree 1995 of 3000\n",
      "building tree 1996 of 3000\n",
      "building tree 1997 of 3000\n",
      "building tree 1998 of 3000\n",
      "building tree 1999 of 3000\n",
      "building tree 2000 of 3000\n",
      "building tree 2001 of 3000\n",
      "building tree 2002 of 3000\n",
      "building tree 2003 of 3000\n",
      "building tree 2004 of 3000\n",
      "building tree 2005 of 3000\n",
      "building tree 2006 of 3000\n",
      "building tree 2007 of 3000\n",
      "building tree 2008 of 3000\n",
      "building tree 2009 of 3000\n",
      "building tree 2010 of 3000\n",
      "building tree 2011 of 3000\n",
      "building tree 2012 of 3000\n",
      "building tree 2013 of 3000\n",
      "building tree 2014 of 3000\n",
      "building tree 2015 of 3000\n",
      "building tree 2016 of 3000\n",
      "building tree 2017 of 3000\n",
      "building tree 2018 of 3000\n",
      "building tree 2019 of 3000\n",
      "building tree 2020 of 3000\n",
      "building tree 2021 of 3000\n",
      "building tree 2022 of 3000\n",
      "building tree 2023 of 3000\n",
      "building tree 2024 of 3000\n",
      "building tree 2025 of 3000\n",
      "building tree 2026 of 3000\n",
      "building tree 2027 of 3000\n",
      "building tree 2028 of 3000\n",
      "building tree 2029 of 3000\n",
      "building tree 2030 of 3000\n",
      "building tree 2031 of 3000\n",
      "building tree 2032 of 3000\n",
      "building tree 2033 of 3000\n",
      "building tree 2034 of 3000\n",
      "building tree 2035 of 3000\n",
      "building tree 2036 of 3000\n",
      "building tree 2037 of 3000\n",
      "building tree 2038 of 3000\n",
      "building tree 2039 of 3000\n",
      "building tree 2040 of 3000\n",
      "building tree 2041 of 3000\n",
      "building tree 2042 of 3000\n",
      "building tree 2043 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2044 of 3000\n",
      "building tree 2045 of 3000\n",
      "building tree 2046 of 3000\n",
      "building tree 2047 of 3000\n",
      "building tree 2048 of 3000\n",
      "building tree 2049 of 3000\n",
      "building tree 2050 of 3000\n",
      "building tree 2051 of 3000\n",
      "building tree 2052 of 3000\n",
      "building tree 2053 of 3000\n",
      "building tree 2054 of 3000\n",
      "building tree 2055 of 3000\n",
      "building tree 2056 of 3000\n",
      "building tree 2057 of 3000\n",
      "building tree 2058 of 3000\n",
      "building tree 2059 of 3000\n",
      "building tree 2060 of 3000\n",
      "building tree 2061 of 3000\n",
      "building tree 2062 of 3000\n",
      "building tree 2063 of 3000\n",
      "building tree 2064 of 3000\n",
      "building tree 2065 of 3000\n",
      "building tree 2066 of 3000\n",
      "building tree 2067 of 3000\n",
      "building tree 2068 of 3000\n",
      "building tree 2069 of 3000\n",
      "building tree 2070 of 3000\n",
      "building tree 2071 of 3000\n",
      "building tree 2072 of 3000\n",
      "building tree 2073 of 3000\n",
      "building tree 2074 of 3000\n",
      "building tree 2075 of 3000\n",
      "building tree 2076 of 3000\n",
      "building tree 2077 of 3000\n",
      "building tree 2078 of 3000\n",
      "building tree 2079 of 3000\n",
      "building tree 2080 of 3000\n",
      "building tree 2081 of 3000\n",
      "building tree 2082 of 3000\n",
      "building tree 2083 of 3000\n",
      "building tree 2084 of 3000\n",
      "building tree 2085 of 3000\n",
      "building tree 2086 of 3000\n",
      "building tree 2087 of 3000\n",
      "building tree 2088 of 3000\n",
      "building tree 2089 of 3000\n",
      "building tree 2090 of 3000\n",
      "building tree 2091 of 3000\n",
      "building tree 2092 of 3000\n",
      "building tree 2093 of 3000\n",
      "building tree 2094 of 3000\n",
      "building tree 2095 of 3000\n",
      "building tree 2096 of 3000\n",
      "building tree 2097 of 3000\n",
      "building tree 2098 of 3000\n",
      "building tree 2099 of 3000\n",
      "building tree 2100 of 3000\n",
      "building tree 2101 of 3000\n",
      "building tree 2102 of 3000\n",
      "building tree 2103 of 3000\n",
      "building tree 2104 of 3000\n",
      "building tree 2105 of 3000\n",
      "building tree 2106 of 3000\n",
      "building tree 2107 of 3000\n",
      "building tree 2108 of 3000\n",
      "building tree 2109 of 3000\n",
      "building tree 2110 of 3000\n",
      "building tree 2111 of 3000\n",
      "building tree 2112 of 3000\n",
      "building tree 2113 of 3000\n",
      "building tree 2114 of 3000\n",
      "building tree 2115 of 3000\n",
      "building tree 2116 of 3000\n",
      "building tree 2117 of 3000\n",
      "building tree 2118 of 3000\n",
      "building tree 2119 of 3000\n",
      "building tree 2120 of 3000\n",
      "building tree 2121 of 3000\n",
      "building tree 2122 of 3000\n",
      "building tree 2123 of 3000\n",
      "building tree 2124 of 3000\n",
      "building tree 2125 of 3000\n",
      "building tree 2126 of 3000\n",
      "building tree 2127 of 3000\n",
      "building tree 2128 of 3000\n",
      "building tree 2129 of 3000\n",
      "building tree 2130 of 3000\n",
      "building tree 2131 of 3000\n",
      "building tree 2132 of 3000\n",
      "building tree 2133 of 3000\n",
      "building tree 2134 of 3000\n",
      "building tree 2135 of 3000\n",
      "building tree 2136 of 3000\n",
      "building tree 2137 of 3000\n",
      "building tree 2138 of 3000\n",
      "building tree 2139 of 3000\n",
      "building tree 2140 of 3000\n",
      "building tree 2141 of 3000\n",
      "building tree 2142 of 3000\n",
      "building tree 2143 of 3000\n",
      "building tree 2144 of 3000\n",
      "building tree 2145 of 3000\n",
      "building tree 2146 of 3000\n",
      "building tree 2147 of 3000\n",
      "building tree 2148 of 3000\n",
      "building tree 2149 of 3000\n",
      "building tree 2150 of 3000\n",
      "building tree 2151 of 3000\n",
      "building tree 2152 of 3000\n",
      "building tree 2153 of 3000\n",
      "building tree 2154 of 3000\n",
      "building tree 2155 of 3000\n",
      "building tree 2156 of 3000\n",
      "building tree 2157 of 3000\n",
      "building tree 2158 of 3000\n",
      "building tree 2159 of 3000\n",
      "building tree 2160 of 3000\n",
      "building tree 2161 of 3000\n",
      "building tree 2162 of 3000\n",
      "building tree 2163 of 3000\n",
      "building tree 2164 of 3000\n",
      "building tree 2165 of 3000\n",
      "building tree 2166 of 3000\n",
      "building tree 2167 of 3000\n",
      "building tree 2168 of 3000\n",
      "building tree 2169 of 3000\n",
      "building tree 2170 of 3000\n",
      "building tree 2171 of 3000\n",
      "building tree 2172 of 3000\n",
      "building tree 2173 of 3000\n",
      "building tree 2174 of 3000\n",
      "building tree 2175 of 3000\n",
      "building tree 2176 of 3000\n",
      "building tree 2177 of 3000\n",
      "building tree 2178 of 3000\n",
      "building tree 2179 of 3000\n",
      "building tree 2180 of 3000\n",
      "building tree 2181 of 3000\n",
      "building tree 2182 of 3000\n",
      "building tree 2183 of 3000\n",
      "building tree 2184 of 3000\n",
      "building tree 2185 of 3000\n",
      "building tree 2186 of 3000\n",
      "building tree 2187 of 3000\n",
      "building tree 2188 of 3000\n",
      "building tree 2189 of 3000\n",
      "building tree 2190 of 3000\n",
      "building tree 2191 of 3000\n",
      "building tree 2192 of 3000\n",
      "building tree 2193 of 3000\n",
      "building tree 2194 of 3000\n",
      "building tree 2195 of 3000\n",
      "building tree 2196 of 3000\n",
      "building tree 2197 of 3000\n",
      "building tree 2198 of 3000\n",
      "building tree 2199 of 3000\n",
      "building tree 2200 of 3000\n",
      "building tree 2201 of 3000\n",
      "building tree 2202 of 3000\n",
      "building tree 2203 of 3000\n",
      "building tree 2204 of 3000\n",
      "building tree 2205 of 3000\n",
      "building tree 2206 of 3000\n",
      "building tree 2207 of 3000\n",
      "building tree 2208 of 3000\n",
      "building tree 2209 of 3000\n",
      "building tree 2210 of 3000\n",
      "building tree 2211 of 3000\n",
      "building tree 2212 of 3000\n",
      "building tree 2213 of 3000\n",
      "building tree 2214 of 3000\n",
      "building tree 2215 of 3000\n",
      "building tree 2216 of 3000\n",
      "building tree 2217 of 3000\n",
      "building tree 2218 of 3000\n",
      "building tree 2219 of 3000\n",
      "building tree 2220 of 3000\n",
      "building tree 2221 of 3000\n",
      "building tree 2222 of 3000\n",
      "building tree 2223 of 3000\n",
      "building tree 2224 of 3000\n",
      "building tree 2225 of 3000\n",
      "building tree 2226 of 3000\n",
      "building tree 2227 of 3000\n",
      "building tree 2228 of 3000\n",
      "building tree 2229 of 3000\n",
      "building tree 2230 of 3000\n",
      "building tree 2231 of 3000\n",
      "building tree 2232 of 3000\n",
      "building tree 2233 of 3000\n",
      "building tree 2234 of 3000\n",
      "building tree 2235 of 3000\n",
      "building tree 2236 of 3000\n",
      "building tree 2237 of 3000\n",
      "building tree 2238 of 3000\n",
      "building tree 2239 of 3000\n",
      "building tree 2240 of 3000\n",
      "building tree 2241 of 3000\n",
      "building tree 2242 of 3000\n",
      "building tree 2243 of 3000\n",
      "building tree 2244 of 3000\n",
      "building tree 2245 of 3000\n",
      "building tree 2246 of 3000\n",
      "building tree 2247 of 3000\n",
      "building tree 2248 of 3000\n",
      "building tree 2249 of 3000\n",
      "building tree 2250 of 3000\n",
      "building tree 2251 of 3000\n",
      "building tree 2252 of 3000\n",
      "building tree 2253 of 3000\n",
      "building tree 2254 of 3000\n",
      "building tree 2255 of 3000\n",
      "building tree 2256 of 3000\n",
      "building tree 2257 of 3000\n",
      "building tree 2258 of 3000\n",
      "building tree 2259 of 3000\n",
      "building tree 2260 of 3000\n",
      "building tree 2261 of 3000\n",
      "building tree 2262 of 3000\n",
      "building tree 2263 of 3000\n",
      "building tree 2264 of 3000\n",
      "building tree 2265 of 3000\n",
      "building tree 2266 of 3000\n",
      "building tree 2267 of 3000\n",
      "building tree 2268 of 3000\n",
      "building tree 2269 of 3000\n",
      "building tree 2270 of 3000\n",
      "building tree 2271 of 3000\n",
      "building tree 2272 of 3000\n",
      "building tree 2273 of 3000\n",
      "building tree 2274 of 3000\n",
      "building tree 2275 of 3000\n",
      "building tree 2276 of 3000\n",
      "building tree 2277 of 3000\n",
      "building tree 2278 of 3000\n",
      "building tree 2279 of 3000\n",
      "building tree 2280 of 3000\n",
      "building tree 2281 of 3000\n",
      "building tree 2282 of 3000\n",
      "building tree 2283 of 3000\n",
      "building tree 2284 of 3000\n",
      "building tree 2285 of 3000\n",
      "building tree 2286 of 3000\n",
      "building tree 2287 of 3000\n",
      "building tree 2288 of 3000\n",
      "building tree 2289 of 3000\n",
      "building tree 2290 of 3000\n",
      "building tree 2291 of 3000\n",
      "building tree 2292 of 3000\n",
      "building tree 2293 of 3000\n",
      "building tree 2294 of 3000\n",
      "building tree 2295 of 3000\n",
      "building tree 2296 of 3000\n",
      "building tree 2297 of 3000\n",
      "building tree 2298 of 3000\n",
      "building tree 2299 of 3000\n",
      "building tree 2300 of 3000\n",
      "building tree 2301 of 3000\n",
      "building tree 2302 of 3000\n",
      "building tree 2303 of 3000\n",
      "building tree 2304 of 3000\n",
      "building tree 2305 of 3000\n",
      "building tree 2306 of 3000\n",
      "building tree 2307 of 3000\n",
      "building tree 2308 of 3000\n",
      "building tree 2309 of 3000\n",
      "building tree 2310 of 3000\n",
      "building tree 2311 of 3000\n",
      "building tree 2312 of 3000\n",
      "building tree 2313 of 3000\n",
      "building tree 2314 of 3000\n",
      "building tree 2315 of 3000\n",
      "building tree 2316 of 3000\n",
      "building tree 2317 of 3000\n",
      "building tree 2318 of 3000\n",
      "building tree 2319 of 3000building tree 2320 of 3000\n",
      "\n",
      "building tree 2321 of 3000\n",
      "building tree 2322 of 3000\n",
      "building tree 2323 of 3000\n",
      "building tree 2324 of 3000\n",
      "building tree 2325 of 3000\n",
      "building tree 2326 of 3000\n",
      "building tree 2327 of 3000\n",
      "building tree 2328 of 3000\n",
      "building tree 2329 of 3000\n",
      "building tree 2330 of 3000\n",
      "building tree 2331 of 3000\n",
      "building tree 2332 of 3000\n",
      "building tree 2333 of 3000\n",
      "building tree 2334 of 3000\n",
      "building tree 2335 of 3000\n",
      "building tree 2336 of 3000\n",
      "building tree 2337 of 3000\n",
      "building tree 2338 of 3000\n",
      "building tree 2339 of 3000\n",
      "building tree 2340 of 3000\n",
      "building tree 2341 of 3000\n",
      "building tree 2342 of 3000\n",
      "building tree 2343 of 3000\n",
      "building tree 2344 of 3000\n",
      "building tree 2345 of 3000\n",
      "building tree 2346 of 3000\n",
      "building tree 2347 of 3000\n",
      "building tree 2348 of 3000\n",
      "building tree 2349 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2350 of 3000\n",
      "building tree 2351 of 3000\n",
      "building tree 2352 of 3000\n",
      "building tree 2353 of 3000\n",
      "building tree 2354 of 3000\n",
      "building tree 2355 of 3000\n",
      "building tree 2356 of 3000\n",
      "building tree 2357 of 3000\n",
      "building tree 2358 of 3000\n",
      "building tree 2359 of 3000\n",
      "building tree 2360 of 3000\n",
      "building tree 2361 of 3000\n",
      "building tree 2362 of 3000\n",
      "building tree 2363 of 3000\n",
      "building tree 2364 of 3000\n",
      "building tree 2365 of 3000\n",
      "building tree 2366 of 3000\n",
      "building tree 2367 of 3000\n",
      "building tree 2368 of 3000\n",
      "building tree 2369 of 3000\n",
      "building tree 2370 of 3000\n",
      "building tree 2371 of 3000\n",
      "building tree 2372 of 3000\n",
      "building tree 2373 of 3000\n",
      "building tree 2374 of 3000\n",
      "building tree 2375 of 3000\n",
      "building tree 2376 of 3000\n",
      "building tree 2377 of 3000\n",
      "building tree 2378 of 3000\n",
      "building tree 2379 of 3000\n",
      "building tree 2380 of 3000\n",
      "building tree 2381 of 3000\n",
      "building tree 2382 of 3000\n",
      "building tree 2383 of 3000\n",
      "building tree 2384 of 3000\n",
      "building tree 2385 of 3000\n",
      "building tree 2386 of 3000\n",
      "building tree 2387 of 3000\n",
      "building tree 2388 of 3000\n",
      "building tree 2389 of 3000\n",
      "building tree 2390 of 3000\n",
      "building tree 2391 of 3000\n",
      "building tree 2392 of 3000\n",
      "building tree 2393 of 3000\n",
      "building tree 2394 of 3000\n",
      "building tree 2395 of 3000\n",
      "building tree 2396 of 3000\n",
      "building tree 2397 of 3000\n",
      "building tree 2398 of 3000\n",
      "building tree 2399 of 3000\n",
      "building tree 2400 of 3000\n",
      "building tree 2401 of 3000\n",
      "building tree 2402 of 3000\n",
      "building tree 2403 of 3000\n",
      "building tree 2404 of 3000\n",
      "building tree 2405 of 3000\n",
      "building tree 2406 of 3000\n",
      "building tree 2407 of 3000\n",
      "building tree 2408 of 3000\n",
      "building tree 2409 of 3000\n",
      "building tree 2410 of 3000\n",
      "building tree 2411 of 3000\n",
      "building tree 2412 of 3000\n",
      "building tree 2413 of 3000\n",
      "building tree 2414 of 3000\n",
      "building tree 2415 of 3000\n",
      "building tree 2416 of 3000\n",
      "building tree 2417 of 3000\n",
      "building tree 2418 of 3000\n",
      "building tree 2419 of 3000\n",
      "building tree 2420 of 3000\n",
      "building tree 2421 of 3000\n",
      "building tree 2422 of 3000\n",
      "building tree 2423 of 3000\n",
      "building tree 2424 of 3000\n",
      "building tree 2425 of 3000\n",
      "building tree 2426 of 3000\n",
      "building tree 2427 of 3000\n",
      "building tree 2428 of 3000\n",
      "building tree 2429 of 3000\n",
      "building tree 2430 of 3000\n",
      "building tree 2431 of 3000\n",
      "building tree 2432 of 3000\n",
      "building tree 2433 of 3000\n",
      "building tree 2434 of 3000\n",
      "building tree 2435 of 3000\n",
      "building tree 2436 of 3000\n",
      "building tree 2437 of 3000\n",
      "building tree 2438 of 3000\n",
      "building tree 2439 of 3000\n",
      "building tree 2440 of 3000\n",
      "building tree 2441 of 3000\n",
      "building tree 2442 of 3000\n",
      "building tree 2443 of 3000\n",
      "building tree 2444 of 3000\n",
      "building tree 2445 of 3000\n",
      "building tree 2446 of 3000\n",
      "building tree 2447 of 3000\n",
      "building tree 2448 of 3000\n",
      "building tree 2449 of 3000\n",
      "building tree 2450 of 3000\n",
      "building tree 2451 of 3000\n",
      "building tree 2452 of 3000\n",
      "building tree 2453 of 3000\n",
      "building tree 2454 of 3000\n",
      "building tree 2455 of 3000\n",
      "building tree 2456 of 3000\n",
      "building tree 2457 of 3000\n",
      "building tree 2458 of 3000\n",
      "building tree 2459 of 3000\n",
      "building tree 2460 of 3000\n",
      "building tree 2461 of 3000\n",
      "building tree 2462 of 3000\n",
      "building tree 2463 of 3000\n",
      "building tree 2464 of 3000\n",
      "building tree 2465 of 3000\n",
      "building tree 2466 of 3000\n",
      "building tree 2467 of 3000\n",
      "building tree 2468 of 3000\n",
      "building tree 2469 of 3000\n",
      "building tree 2470 of 3000\n",
      "building tree 2471 of 3000\n",
      "building tree 2472 of 3000\n",
      "building tree 2473 of 3000\n",
      "building tree 2474 of 3000\n",
      "building tree 2475 of 3000\n",
      "building tree 2476 of 3000\n",
      "building tree 2477 of 3000\n",
      "building tree 2478 of 3000\n",
      "building tree 2479 of 3000\n",
      "building tree 2480 of 3000\n",
      "building tree 2481 of 3000\n",
      "building tree 2482 of 3000\n",
      "building tree 2483 of 3000\n",
      "building tree 2484 of 3000\n",
      "building tree 2485 of 3000\n",
      "building tree 2486 of 3000\n",
      "building tree 2487 of 3000\n",
      "building tree 2488 of 3000\n",
      "building tree 2489 of 3000\n",
      "building tree 2490 of 3000\n",
      "building tree 2491 of 3000\n",
      "building tree 2492 of 3000\n",
      "building tree 2493 of 3000\n",
      "building tree 2494 of 3000\n",
      "building tree 2495 of 3000\n",
      "building tree 2496 of 3000\n",
      "building tree 2497 of 3000\n",
      "building tree 2498 of 3000\n",
      "building tree 2499 of 3000\n",
      "building tree 2500 of 3000\n",
      "building tree 2501 of 3000\n",
      "building tree 2502 of 3000\n",
      "building tree 2503 of 3000\n",
      "building tree 2504 of 3000\n",
      "building tree 2505 of 3000\n",
      "building tree 2506 of 3000\n",
      "building tree 2507 of 3000\n",
      "building tree 2508 of 3000\n",
      "building tree 2509 of 3000\n",
      "building tree 2510 of 3000\n",
      "building tree 2511 of 3000\n",
      "building tree 2512 of 3000\n",
      "building tree 2513 of 3000\n",
      "building tree 2514 of 3000\n",
      "building tree 2515 of 3000\n",
      "building tree 2516 of 3000\n",
      "building tree 2517 of 3000\n",
      "building tree 2518 of 3000\n",
      "building tree 2519 of 3000\n",
      "building tree 2520 of 3000\n",
      "building tree 2521 of 3000\n",
      "building tree 2522 of 3000\n",
      "building tree 2523 of 3000\n",
      "building tree 2524 of 3000\n",
      "building tree 2525 of 3000\n",
      "building tree 2526 of 3000\n",
      "building tree 2527 of 3000\n",
      "building tree 2528 of 3000\n",
      "building tree 2529 of 3000\n",
      "building tree 2530 of 3000\n",
      "building tree 2531 of 3000\n",
      "building tree 2532 of 3000\n",
      "building tree 2533 of 3000\n",
      "building tree 2534 of 3000\n",
      "building tree 2535 of 3000\n",
      "building tree 2536 of 3000\n",
      "building tree 2537 of 3000\n",
      "building tree 2538 of 3000\n",
      "building tree 2539 of 3000\n",
      "building tree 2540 of 3000\n",
      "building tree 2541 of 3000\n",
      "building tree 2542 of 3000\n",
      "building tree 2543 of 3000\n",
      "building tree 2544 of 3000\n",
      "building tree 2545 of 3000\n",
      "building tree 2546 of 3000\n",
      "building tree 2547 of 3000\n",
      "building tree 2548 of 3000\n",
      "building tree 2549 of 3000\n",
      "building tree 2550 of 3000\n",
      "building tree 2551 of 3000\n",
      "building tree 2552 of 3000\n",
      "building tree 2553 of 3000\n",
      "building tree 2554 of 3000\n",
      "building tree 2555 of 3000\n",
      "building tree 2556 of 3000\n",
      "building tree 2557 of 3000\n",
      "building tree 2558 of 3000\n",
      "building tree 2559 of 3000\n",
      "building tree 2560 of 3000\n",
      "building tree 2561 of 3000\n",
      "building tree 2562 of 3000\n",
      "building tree 2563 of 3000\n",
      "building tree 2564 of 3000\n",
      "building tree 2565 of 3000\n",
      "building tree 2566 of 3000\n",
      "building tree 2567 of 3000\n",
      "building tree 2568 of 3000\n",
      "building tree 2569 of 3000\n",
      "building tree 2570 of 3000\n",
      "building tree 2571 of 3000\n",
      "building tree 2572 of 3000\n",
      "building tree 2573 of 3000\n",
      "building tree 2574 of 3000\n",
      "building tree 2575 of 3000\n",
      "building tree 2576 of 3000\n",
      "building tree 2577 of 3000\n",
      "building tree 2578 of 3000\n",
      "building tree 2579 of 3000\n",
      "building tree 2580 of 3000\n",
      "building tree 2581 of 3000\n",
      "building tree 2582 of 3000\n",
      "building tree 2583 of 3000\n",
      "building tree 2584 of 3000\n",
      "building tree 2585 of 3000\n",
      "building tree 2586 of 3000\n",
      "building tree 2587 of 3000\n",
      "building tree 2588 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2589 of 3000\n",
      "building tree 2590 of 3000\n",
      "building tree 2591 of 3000\n",
      "building tree 2592 of 3000\n",
      "building tree 2593 of 3000\n",
      "building tree 2594 of 3000\n",
      "building tree 2595 of 3000\n",
      "building tree 2596 of 3000\n",
      "building tree 2597 of 3000\n",
      "building tree 2598 of 3000\n",
      "building tree 2599 of 3000\n",
      "building tree 2600 of 3000\n",
      "building tree 2601 of 3000\n",
      "building tree 2602 of 3000\n",
      "building tree 2603 of 3000\n",
      "building tree 2604 of 3000\n",
      "building tree 2605 of 3000\n",
      "building tree 2606 of 3000\n",
      "building tree 2607 of 3000\n",
      "building tree 2608 of 3000\n",
      "building tree 2609 of 3000\n",
      "building tree 2610 of 3000\n",
      "building tree 2611 of 3000\n",
      "building tree 2612 of 3000\n",
      "building tree 2613 of 3000\n",
      "building tree 2614 of 3000\n",
      "building tree 2615 of 3000\n",
      "building tree 2616 of 3000\n",
      "building tree 2617 of 3000\n",
      "building tree 2618 of 3000\n",
      "building tree 2619 of 3000\n",
      "building tree 2620 of 3000\n",
      "building tree 2621 of 3000\n",
      "building tree 2622 of 3000\n",
      "building tree 2623 of 3000\n",
      "building tree 2624 of 3000\n",
      "building tree 2625 of 3000\n",
      "building tree 2626 of 3000\n",
      "building tree 2627 of 3000\n",
      "building tree 2628 of 3000\n",
      "building tree 2629 of 3000\n",
      "building tree 2630 of 3000\n",
      "building tree 2631 of 3000\n",
      "building tree 2632 of 3000\n",
      "building tree 2633 of 3000\n",
      "building tree 2634 of 3000\n",
      "building tree 2635 of 3000\n",
      "building tree 2636 of 3000\n",
      "building tree 2637 of 3000\n",
      "building tree 2638 of 3000\n",
      "building tree 2639 of 3000\n",
      "building tree 2640 of 3000\n",
      "building tree 2641 of 3000\n",
      "building tree 2642 of 3000\n",
      "building tree 2643 of 3000\n",
      "building tree 2644 of 3000\n",
      "building tree 2645 of 3000\n",
      "building tree 2646 of 3000\n",
      "building tree 2647 of 3000\n",
      "building tree 2648 of 3000\n",
      "building tree 2649 of 3000\n",
      "building tree 2650 of 3000\n",
      "building tree 2651 of 3000\n",
      "building tree 2652 of 3000\n",
      "building tree 2653 of 3000\n",
      "building tree 2654 of 3000\n",
      "building tree 2655 of 3000\n",
      "building tree 2656 of 3000\n",
      "building tree 2657 of 3000\n",
      "building tree 2658 of 3000\n",
      "building tree 2659 of 3000\n",
      "building tree 2660 of 3000\n",
      "building tree 2661 of 3000\n",
      "building tree 2662 of 3000\n",
      "building tree 2663 of 3000\n",
      "building tree 2664 of 3000\n",
      "building tree 2665 of 3000\n",
      "building tree 2666 of 3000\n",
      "building tree 2667 of 3000\n",
      "building tree 2668 of 3000\n",
      "building tree 2669 of 3000\n",
      "building tree 2670 of 3000\n",
      "building tree 2671 of 3000\n",
      "building tree 2672 of 3000\n",
      "building tree 2673 of 3000\n",
      "building tree 2674 of 3000\n",
      "building tree 2675 of 3000\n",
      "building tree 2676 of 3000\n",
      "building tree 2677 of 3000\n",
      "building tree 2678 of 3000\n",
      "building tree 2679 of 3000\n",
      "building tree 2680 of 3000\n",
      "building tree 2681 of 3000\n",
      "building tree 2682 of 3000\n",
      "building tree 2683 of 3000\n",
      "building tree 2684 of 3000\n",
      "building tree 2685 of 3000\n",
      "building tree 2686 of 3000\n",
      "building tree 2687 of 3000\n",
      "building tree 2688 of 3000\n",
      "building tree 2689 of 3000\n",
      "building tree 2690 of 3000\n",
      "building tree 2691 of 3000\n",
      "building tree 2692 of 3000\n",
      "building tree 2693 of 3000\n",
      "building tree 2694 of 3000\n",
      "building tree 2695 of 3000\n",
      "building tree 2696 of 3000\n",
      "building tree 2697 of 3000\n",
      "building tree 2698 of 3000\n",
      "building tree 2699 of 3000\n",
      "building tree 2700 of 3000\n",
      "building tree 2701 of 3000\n",
      "building tree 2702 of 3000\n",
      "building tree 2703 of 3000\n",
      "building tree 2704 of 3000\n",
      "building tree 2705 of 3000\n",
      "building tree 2706 of 3000\n",
      "building tree 2707 of 3000\n",
      "building tree 2708 of 3000\n",
      "building tree 2709 of 3000\n",
      "building tree 2710 of 3000\n",
      "building tree 2711 of 3000\n",
      "building tree 2712 of 3000\n",
      "building tree 2713 of 3000\n",
      "building tree 2714 of 3000\n",
      "building tree 2715 of 3000\n",
      "building tree 2716 of 3000\n",
      "building tree 2717 of 3000\n",
      "building tree 2718 of 3000\n",
      "building tree 2719 of 3000\n",
      "building tree 2720 of 3000\n",
      "building tree 2721 of 3000\n",
      "building tree 2722 of 3000\n",
      "building tree 2723 of 3000\n",
      "building tree 2724 of 3000\n",
      "building tree 2725 of 3000\n",
      "building tree 2726 of 3000\n",
      "building tree 2727 of 3000\n",
      "building tree 2728 of 3000\n",
      "building tree 2729 of 3000\n",
      "building tree 2730 of 3000\n",
      "building tree 2731 of 3000\n",
      "building tree 2732 of 3000\n",
      "building tree 2733 of 3000\n",
      "building tree 2734 of 3000\n",
      "building tree 2735 of 3000\n",
      "building tree 2736 of 3000\n",
      "building tree 2737 of 3000\n",
      "building tree 2738 of 3000\n",
      "building tree 2739 of 3000\n",
      "building tree 2740 of 3000\n",
      "building tree 2741 of 3000\n",
      "building tree 2742 of 3000\n",
      "building tree 2743 of 3000\n",
      "building tree 2744 of 3000\n",
      "building tree 2745 of 3000\n",
      "building tree 2746 of 3000\n",
      "building tree 2747 of 3000\n",
      "building tree 2748 of 3000\n",
      "building tree 2749 of 3000\n",
      "building tree 2750 of 3000\n",
      "building tree 2751 of 3000\n",
      "building tree 2752 of 3000\n",
      "building tree 2753 of 3000\n",
      "building tree 2754 of 3000\n",
      "building tree 2755 of 3000\n",
      "building tree 2756 of 3000\n",
      "building tree 2757 of 3000\n",
      "building tree 2758 of 3000\n",
      "building tree 2759 of 3000\n",
      "building tree 2760 of 3000\n",
      "building tree 2761 of 3000\n",
      "building tree 2762 of 3000\n",
      "building tree 2763 of 3000\n",
      "building tree 2764 of 3000\n",
      "building tree 2765 of 3000\n",
      "building tree 2766 of 3000\n",
      "building tree 2767 of 3000\n",
      "building tree 2768 of 3000\n",
      "building tree 2769 of 3000\n",
      "building tree 2770 of 3000\n",
      "building tree 2771 of 3000\n",
      "building tree 2772 of 3000\n",
      "building tree 2773 of 3000\n",
      "building tree 2774 of 3000\n",
      "building tree 2775 of 3000\n",
      "building tree 2776 of 3000\n",
      "building tree 2777 of 3000\n",
      "building tree 2778 of 3000\n",
      "building tree 2779 of 3000\n",
      "building tree 2780 of 3000\n",
      "building tree 2781 of 3000\n",
      "building tree 2782 of 3000\n",
      "building tree 2783 of 3000\n",
      "building tree 2784 of 3000\n",
      "building tree 2785 of 3000\n",
      "building tree 2786 of 3000\n",
      "building tree 2787 of 3000\n",
      "building tree 2788 of 3000\n",
      "building tree 2789 of 3000\n",
      "building tree 2790 of 3000\n",
      "building tree 2791 of 3000\n",
      "building tree 2792 of 3000\n",
      "building tree 2793 of 3000\n",
      "building tree 2794 of 3000\n",
      "building tree 2795 of 3000\n",
      "building tree 2796 of 3000\n",
      "building tree 2797 of 3000\n",
      "building tree 2798 of 3000\n",
      "building tree 2799 of 3000\n",
      "building tree 2800 of 3000\n",
      "building tree 2801 of 3000\n",
      "building tree 2802 of 3000\n",
      "building tree 2803 of 3000\n",
      "building tree 2804 of 3000\n",
      "building tree 2805 of 3000\n",
      "building tree 2806 of 3000\n",
      "building tree 2807 of 3000\n",
      "building tree 2808 of 3000\n",
      "building tree 2809 of 3000\n",
      "building tree 2810 of 3000\n",
      "building tree 2811 of 3000\n",
      "building tree 2812 of 3000\n",
      "building tree 2813 of 3000\n",
      "building tree 2814 of 3000\n",
      "building tree 2815 of 3000\n",
      "building tree 2816 of 3000\n",
      "building tree 2817 of 3000\n",
      "building tree 2818 of 3000\n",
      "building tree 2819 of 3000\n",
      "building tree 2820 of 3000\n",
      "building tree 2821 of 3000\n",
      "building tree 2822 of 3000\n",
      "building tree 2823 of 3000\n",
      "building tree 2824 of 3000\n",
      "building tree 2825 of 3000\n",
      "building tree 2826 of 3000\n",
      "building tree 2827 of 3000\n",
      "building tree 2828 of 3000\n",
      "building tree 2829 of 3000\n",
      "building tree 2830 of 3000\n",
      "building tree 2831 of 3000\n",
      "building tree 2832 of 3000\n",
      "building tree 2833 of 3000\n",
      "building tree 2834 of 3000\n",
      "building tree 2835 of 3000\n",
      "building tree 2836 of 3000\n",
      "building tree 2837 of 3000\n",
      "building tree 2838 of 3000\n",
      "building tree 2839 of 3000\n",
      "building tree 2840 of 3000\n",
      "building tree 2841 of 3000\n",
      "building tree 2842 of 3000\n",
      "building tree 2843 of 3000\n",
      "building tree 2844 of 3000\n",
      "building tree 2845 of 3000\n",
      "building tree 2846 of 3000\n",
      "building tree 2847 of 3000\n",
      "building tree 2848 of 3000\n",
      "building tree 2849 of 3000\n",
      "building tree 2850 of 3000\n",
      "building tree 2851 of 3000\n",
      "building tree 2852 of 3000\n",
      "building tree 2853 of 3000\n",
      "building tree 2854 of 3000\n",
      "building tree 2855 of 3000\n",
      "building tree 2856 of 3000\n",
      "building tree 2857 of 3000\n",
      "building tree 2858 of 3000\n",
      "building tree 2859 of 3000\n",
      "building tree 2860 of 3000\n",
      "building tree 2861 of 3000\n",
      "building tree 2862 of 3000\n",
      "building tree 2863 of 3000\n",
      "building tree 2864 of 3000\n",
      "building tree 2865 of 3000\n",
      "building tree 2866 of 3000\n",
      "building tree 2867 of 3000\n",
      "building tree 2868 of 3000\n",
      "building tree 2869 of 3000\n",
      "building tree 2870 of 3000\n",
      "building tree 2871 of 3000\n",
      "building tree 2872 of 3000\n",
      "building tree 2873 of 3000\n",
      "building tree 2874 of 3000\n",
      "building tree 2875 of 3000\n",
      "building tree 2876 of 3000\n",
      "building tree 2877 of 3000\n",
      "building tree 2878 of 3000\n",
      "building tree 2879 of 3000\n",
      "building tree 2880 of 3000\n",
      "building tree 2881 of 3000\n",
      "building tree 2882 of 3000\n",
      "building tree 2883 of 3000\n",
      "building tree 2884 of 3000\n",
      "building tree 2885 of 3000\n",
      "building tree 2886 of 3000\n",
      "building tree 2887 of 3000\n",
      "building tree 2888 of 3000\n",
      "building tree 2889 of 3000\n",
      "building tree 2890 of 3000\n",
      "building tree 2891 of 3000\n",
      "building tree 2892 of 3000\n",
      "building tree 2893 of 3000\n",
      "building tree 2894 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2895 of 3000\n",
      "building tree 2896 of 3000\n",
      "building tree 2897 of 3000\n",
      "building tree 2898 of 3000\n",
      "building tree 2899 of 3000\n",
      "building tree 2900 of 3000\n",
      "building tree 2901 of 3000\n",
      "building tree 2902 of 3000\n",
      "building tree 2903 of 3000\n",
      "building tree 2904 of 3000\n",
      "building tree 2905 of 3000\n",
      "building tree 2906 of 3000\n",
      "building tree 2907 of 3000\n",
      "building tree 2908 of 3000\n",
      "building tree 2909 of 3000\n",
      "building tree 2910 of 3000\n",
      "building tree 2911 of 3000\n",
      "building tree 2912 of 3000\n",
      "building tree 2913 of 3000\n",
      "building tree 2914 of 3000\n",
      "building tree 2915 of 3000\n",
      "building tree 2916 of 3000\n",
      "building tree 2917 of 3000\n",
      "building tree 2918 of 3000\n",
      "building tree 2919 of 3000\n",
      "building tree 2920 of 3000\n",
      "building tree 2921 of 3000\n",
      "building tree 2922 of 3000\n",
      "building tree 2923 of 3000\n",
      "building tree 2924 of 3000\n",
      "building tree 2925 of 3000\n",
      "building tree 2926 of 3000\n",
      "building tree 2927 of 3000\n",
      "building tree 2928 of 3000\n",
      "building tree 2929 of 3000\n",
      "building tree 2930 of 3000\n",
      "building tree 2931 of 3000\n",
      "building tree 2932 of 3000\n",
      "building tree 2933 of 3000\n",
      "building tree 2934 of 3000\n",
      "building tree 2935 of 3000\n",
      "building tree 2936 of 3000\n",
      "building tree 2937 of 3000\n",
      "building tree 2938 of 3000\n",
      "building tree 2939 of 3000\n",
      "building tree 2940 of 3000\n",
      "building tree 2941 of 3000\n",
      "building tree 2942 of 3000\n",
      "building tree 2943 of 3000\n",
      "building tree 2944 of 3000\n",
      "building tree 2945 of 3000\n",
      "building tree 2946 of 3000\n",
      "building tree 2947 of 3000\n",
      "building tree 2948 of 3000\n",
      "building tree 2949 of 3000\n",
      "building tree 2950 of 3000\n",
      "building tree 2951 of 3000\n",
      "building tree 2952 of 3000\n",
      "building tree 2953 of 3000\n",
      "building tree 2954 of 3000\n",
      "building tree 2955 of 3000\n",
      "building tree 2956 of 3000\n",
      "building tree 2957 of 3000\n",
      "building tree 2958 of 3000\n",
      "building tree 2959 of 3000\n",
      "building tree 2960 of 3000\n",
      "building tree 2961 of 3000\n",
      "building tree 2962 of 3000\n",
      "building tree 2963 of 3000\n",
      "building tree 2964 of 3000\n",
      "building tree 2965 of 3000\n",
      "building tree 2966 of 3000\n",
      "building tree 2967 of 3000\n",
      "building tree 2968 of 3000\n",
      "building tree 2969 of 3000\n",
      "building tree 2970 of 3000\n",
      "building tree 2971 of 3000\n",
      "building tree 2972 of 3000\n",
      "building tree 2973 of 3000\n",
      "building tree 2974 of 3000\n",
      "building tree 2975 of 3000\n",
      "building tree 2976 of 3000\n",
      "building tree 2977 of 3000\n",
      "building tree 2978 of 3000\n",
      "building tree 2979 of 3000\n",
      "building tree 2980 of 3000\n",
      "building tree 2981 of 3000\n",
      "building tree 2982 of 3000\n",
      "building tree 2983 of 3000\n",
      "building tree 2984 of 3000\n",
      "building tree 2985 of 3000\n",
      "building tree 2986 of 3000\n",
      "building tree 2987 of 3000\n",
      "building tree 2988 of 3000\n",
      "building tree 2989 of 3000\n",
      "building tree 2990 of 3000\n",
      "building tree 2991 of 3000\n",
      "building tree 2992 of 3000\n",
      "building tree 2993 of 3000\n",
      "building tree 2994 of 3000\n",
      "building tree 2995 of 3000\n",
      "building tree 2996 of 3000\n",
      "building tree 2997 of 3000\n",
      "building tree 2998 of 3000\n",
      "building tree 2999 of 3000\n",
      "building tree 3000 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=3000, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=3, warm_start=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfR2 = RFR(n_estimators=3000, n_jobs=-1, verbose=3)\n",
    "\n",
    "rfR2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n",
      "[CV] n_estimators=2000, random_state=50 ..............................\n",
      "[CV] n_estimators=2000, random_state=50 ..............................\n",
      "[CV] n_estimators=2000, random_state=50 ..............................\n",
      "[CV] n_estimators=2000, random_state=50 ..............................\n",
      "[CV]  n_estimators=2000, random_state=50, score=0.06991290745877621, total= 6.1min\n",
      "[CV] n_estimators=2500, random_state=50 ..............................\n",
      "[CV]  n_estimators=2000, random_state=50, score=0.052604089443840096, total= 6.2min\n",
      "[CV] n_estimators=2500, random_state=50 ..............................\n",
      "[CV]  n_estimators=2000, random_state=50, score=0.07148754249047451, total= 6.2min\n",
      "[CV] n_estimators=2500, random_state=50 ..............................\n",
      "[CV]  n_estimators=2000, random_state=50, score=0.10584908761376832, total= 6.2min\n",
      "[CV] n_estimators=2500, random_state=50 ..............................\n",
      "[CV]  n_estimators=2500, random_state=50, score=0.07007493185890545, total= 7.7min\n",
      "[CV] n_estimators=3000, random_state=50 ..............................\n",
      "[CV]  n_estimators=2500, random_state=50, score=0.05367481164190746, total= 7.8min\n",
      "[CV]  n_estimators=2500, random_state=50, score=0.07126510977093348, total= 7.8min\n",
      "[CV] n_estimators=3000, random_state=50 ..............................\n",
      "[CV] n_estimators=3000, random_state=50 ..............................\n",
      "[CV]  n_estimators=2500, random_state=50, score=0.10586775796921333, total= 7.8min\n",
      "[CV] n_estimators=3000, random_state=50 ..............................\n",
      "[CV]  n_estimators=3000, random_state=50, score=0.07082674224328811, total= 9.7min\n",
      "[CV]  n_estimators=3000, random_state=50, score=0.05407862058099866, total= 9.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  12 | elapsed: 24.0min remaining:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=3000, random_state=50, score=0.0712807381745596, total= 9.8min\n",
      "[CV]  n_estimators=3000, random_state=50, score=0.10633545376947273, total= 9.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed: 24.0min finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-7cbf9ed1af8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrfR3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgs3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrfR3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgs3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m                 **self.best_params_)\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [2000, 2500, 3000],\n",
    "    'random_state': [50]\n",
    "}\n",
    "\n",
    "rfR3 = RFR()\n",
    "gs3 = GridSearchCV(estimator=rfR3, param_grid=params, cv=4, n_jobs=4, verbose=3)\n",
    "gs3.fit(X_train, y_train)\n",
    "\n",
    "print(gs3.best_params_)\n",
    "print(kfold(10, gs2.best_estimator_, train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfR3 = RFR(n_estimators=3000, n_jobs=-1)\n",
    "rfR3.fit(X_train, y_train)\n",
    "predictions2 = rfR3.predict(test.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.938713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.910050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.916248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.931032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.931161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.938713\n",
       "1  2   0.910050\n",
       "2  3   0.916248\n",
       "3  4   0.931032\n",
       "4  5   0.931161"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission2 = pd.DataFrame(data=predictions2, columns=['Predicted'])\n",
    "submission2.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "submission2['Id'] = submission2['Id'].astype(str)\n",
    "submission2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.939374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.910163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.916366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.930763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.930731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.939374\n",
       "1  2   0.910163\n",
       "2  3   0.916366\n",
       "3  4   0.930763\n",
       "4  5   0.930731"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfR4 = RFR(n_estimators=2500, n_jobs=-1)\n",
    "rfR4.fit(X_train, y_train)\n",
    "predictions3 = rfR4.predict(test.iloc[:, 1:])\n",
    "\n",
    "submission3 = pd.DataFrame(data=predictions3, columns=['Predicted'])\n",
    "submission3.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "submission3['Id'] = submission3['Id'].astype(str)\n",
    "submission3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission3.to_csv(\"rfr_2000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=1500, n_jobs=-1, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BR(n_estimators=1500, n_jobs=-1)\n",
    "br.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.939037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.910711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.916856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.930083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.931816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.939037\n",
       "1  2   0.910711\n",
       "2  3   0.916856\n",
       "3  4   0.930083\n",
       "4  5   0.931816"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brpredict = br.predict(test.iloc[:, 1:])\n",
    "\n",
    "submission4 = pd.DataFrame(data=brpredict, columns=['Predicted'])\n",
    "submission4.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "submission4['Id'] = submission4['Id'].astype(str)\n",
    "submission4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission4.to_csv(\"br1500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.940034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.909831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.915914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.931259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.931471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.940034\n",
       "1  2   0.909831\n",
       "2  3   0.915914\n",
       "3  4   0.931259\n",
       "4  5   0.931471"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr15 = RFR(n_estimators=1500, n_jobs=-1)\n",
    "rfr15.fit(X_train, y_train)\n",
    "\n",
    "rfr15predict = rfr15.predict(test.iloc[:, 1:])\n",
    "\n",
    "submission5 = pd.DataFrame(data=rfr15predict, columns=['Predicted'])\n",
    "submission5.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "submission5['Id'] = submission5['Id'].astype(str)\n",
    "submission5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission5.to_csv(\"rfr1500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0022213548013956252, total=   2.4s\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.002131314202196588, total=   2.6s\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.006831750891926669, total=   2.5s\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0002091274860729353, total=   2.6s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0013164191823589189, total=   7.8s\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.001956461973257273, total=   7.9s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0006346366337162124, total=   7.7s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.033737238665209945, total=   8.0s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.002210160962973884, total=  17.4s\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.020969548578983588, total=  17.4s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0005381593240978422, total=  17.5s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.004478281439738829, total=  19.9s\n",
      "[CV] alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.0031652102246901848, total=  33.8s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) .\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.003164249885831971, total=  33.8s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) .\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-3.9456825851047483e-07, total=  34.0s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) .\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0002090925781501607, total=   2.3s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) .\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0068309170802465236, total=   2.2s\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0021317385777006592, total=   2.2s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) .\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) .\n",
      "[CV]  alpha=5e-05, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.015515527950358708, total=  34.2s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) .\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0022210536794453617, total=   2.6s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) .\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0013169605751661706, total=   9.2s\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0019566298272075677, total=   9.3s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) .\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) .\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.033736936488832914, total=   9.3s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) .\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0006349014898994199, total=   9.5s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.020967980529727814, total=  17.5s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0022096714331294454, total=  18.0s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0005379230199198393, total=  18.4s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0044789764674477706, total=  18.2s\n",
      "[CV] alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.003170416255596331, total=  33.8s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-3.1647900522457917e-07, total=  34.3s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.0031783626556094458, total=  34.4s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.00020998103302694027, total=   2.5s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.0021418921095079924, total=   2.4s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.0001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.015514787068589442, total=  33.8s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.006822300433030781, total=   2.3s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(100, 100, 100, 100, 100, 100), score=-0.002215825642986946, total=   2.5s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0019544708660133026, total=   8.2s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.03375330440600366, total=   8.3s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-7.686854993460557e-05, total=   9.5s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0006383072214783958, total=   8.5s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100) ..\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0008857211110997465, total=  18.3s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0032520780424594165, total=  19.2s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.00693562943631898, total=  19.2s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(500, 500, 500, 500, 100, 100), score=-0.0021537524494845695, total=  19.4s\n",
      "[CV] alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100) \n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.00015591389261304656, total=  38.1s\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.012938189980570503, total=  38.0s\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.021971410646670186, total=  38.3s\n",
      "[CV]  alpha=0.001, hidden_layer_sizes=(1000, 1000, 500, 500, 100, 100), score=-0.010474719033485735, total=  37.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  48 out of  48 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-8c9d97c66a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgsmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params'"
     ]
    }
   ],
   "source": [
    "# Tuning multilayer perceptron\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(100, 100, 100, 100, 100, 100), (500, 500, 100, 100, 100, 100), (500, 500, 500, 500, 100, 100), (1000, 1000, 500, 500, 100, 100)],\n",
    "    'alpha': [0.00005, 0.0001, 0.001]\n",
    "}\n",
    "\n",
    "mlp = MLP(activation='logistic', random_state=50)\n",
    "gsmlp = GridSearchCV(estimator=mlp, param_grid=params, cv=4, n_jobs=4, verbose=3)\n",
    "gsmlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n",
      "[CV] hidden_layer_sizes=(400, 400, 100, 100, 100, 100) ...............\n",
      "[CV] hidden_layer_sizes=(400, 400, 100, 100, 100, 100) ...............\n",
      "[CV] hidden_layer_sizes=(400, 400, 100, 100, 100, 100) ...............\n",
      "[CV] hidden_layer_sizes=(400, 400, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(400, 400, 100, 100, 100, 100), score=-0.002782290238506402, total=  10.2s\n",
      "[CV]  hidden_layer_sizes=(400, 400, 100, 100, 100, 100), score=-3.4159072292982984e-05, total=  10.3s\n",
      "[CV] hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ...............\n",
      "[CV] hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(400, 400, 100, 100, 100, 100), score=-0.005963434902424236, total=  10.7s\n",
      "[CV] hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(400, 400, 100, 100, 100, 100), score=-0.00409296081757593, total=  11.2s\n",
      "[CV] hidden_layer_sizes=(500, 500, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-1.0359407025539724e-06, total=  10.0s\n",
      "[CV] hidden_layer_sizes=(600, 600, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.0016872649003414342, total=  10.2s\n",
      "[CV] hidden_layer_sizes=(600, 600, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.007896872629665319, total=  10.3s\n",
      "[CV] hidden_layer_sizes=(600, 600, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(500, 500, 100, 100, 100, 100), score=-0.003034637730991996, total=  10.5s\n",
      "[CV] hidden_layer_sizes=(600, 600, 100, 100, 100, 100) ...............\n",
      "[CV]  hidden_layer_sizes=(600, 600, 100, 100, 100, 100), score=-0.0027301374496118136, total=  13.3s\n",
      "[CV]  hidden_layer_sizes=(600, 600, 100, 100, 100, 100), score=-3.1961782476663814e-05, total=  13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  12 | elapsed:   35.4s remaining:    7.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  hidden_layer_sizes=(600, 600, 100, 100, 100, 100), score=-0.005812005858984559, total=  13.3s\n",
      "[CV]  hidden_layer_sizes=(600, 600, 100, 100, 100, 100), score=-0.004077449499403363, total=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:   36.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=MLPRegressor(activation='logistic', alpha=0.001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=50, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'hidden_layer_sizes': [(400, 400, 100, 100, 100, 100), (500, 500, 100, 100, 100, 100), (600, 600, 100, 100, 100, 100)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(400, 400, 100, 100, 100, 100), (500, 500, 100, 100, 100, 100), (600, 600, 100, 100, 100, 100)],\n",
    "}\n",
    "\n",
    "mlp = MLP(activation='logistic', solver='sgd', random_state=50, learning_rate_init=0.0001, alpha=0.001)\n",
    "gsmlp = GridSearchCV(estimator=mlp, param_grid=params, cv=4, n_jobs=4, verbose=3)\n",
    "gsmlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027972472343201306"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, gsmlp.best_estimator_, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.919634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.919633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.919634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.919634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.919634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.919634\n",
       "1  2   0.919633\n",
       "2  3   0.919634\n",
       "3  4   0.919634\n",
       "4  5   0.919634"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlppredict = gsmlp.best_estimator_.predict(test.iloc[:, 1:])\n",
    "\n",
    "submission6 = pd.DataFrame(data=mlppredict, columns=['Predicted'])\n",
    "submission6.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "submission6['Id'] = submission6['Id'].astype(str)\n",
    "submission6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission6.to_csv(\"mlp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10 0.12286565634644402\n",
      "1e-09 0.03863578311622712\n",
      "1e-08 0.028299272111959557\n",
      "1e-07 0.028015169546417355\n",
      "1e-06 0.02803103464463681\n",
      "1e-05 0.02824344572416259\n",
      "0.0001 0.02898097032911563\n",
      "0.001 0.02936473830084192\n",
      "0.01 0.0295075746989041\n",
      "0.1 0.029742246754614492\n",
      "1 0.029742836179759823\n",
      "10 0.029627611424617967\n",
      "100 0.032791740778805076\n"
     ]
    }
   ],
   "source": [
    "for alpha in [10 ** i for i in range(-10, 3)]:\n",
    "    print(alpha, kfold(10, KR(alpha=alpha), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10 0.1300089063100798\n",
      "1e-09 0.04326022018991976\n",
      "1e-08 0.028717990035703984\n",
      "1e-07 0.027901701754626317\n",
      "1e-06 0.027888444197910846\n",
      "1e-05 0.02788803615147088\n",
      "0.0001 0.02788759452119577\n",
      "0.001 0.02788360438959959\n",
      "0.01 0.02785337885849116\n",
      "0.1 0.027728402147805052\n",
      "1 0.027452928577104053\n",
      "10 0.027242749342376932\n",
      "100 0.027376528582641346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "for alpha in [10 ** i for i in range(-10, 3)]:\n",
    "    print(alpha, kfold(10, Ridge(alpha=alpha), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.027242749342376932\n",
      "11 0.027240361798796863\n",
      "12 0.027238782187045653\n",
      "13 0.027237842126003536\n",
      "14 0.027237414559526107\n",
      "15 0.02723740188015112\n",
      "16 0.027237727947945684\n",
      "17 0.027238332578745916\n",
      "18 0.027239167648027012\n",
      "19 0.02724019428075406\n",
      "20 0.027241380788482417\n"
     ]
    }
   ],
   "source": [
    "for alpha in [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "    print(alpha, kfold(10, Ridge(alpha=alpha), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestRidge = Ridge(alpha=15)\n",
    "bestRidge.fit(X_train, y_train)\n",
    "ridgepredict = bestRidge.predict(test.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "[CV] alpha=0.01, degree=5, kernel=linear .............................\n",
      "[CV] alpha=0.01, degree=5, kernel=linear .............................\n",
      "[CV] alpha=0.01, degree=5, kernel=linear .............................\n",
      "[CV] alpha=0.01, degree=5, kernel=linear .............................\n",
      "[CV]  alpha=0.01, degree=5, kernel=linear, score=0.027253408556165648, total=   3.8s\n",
      "[CV] alpha=0.01, degree=5, kernel=polynomial .........................\n",
      "[CV]  alpha=0.01, degree=5, kernel=linear, score=-0.10308601877978307, total=   3.9s\n",
      "[CV]  alpha=0.01, degree=5, kernel=linear, score=-0.06035477642701137, total=   4.1s\n",
      "[CV]  alpha=0.01, degree=5, kernel=linear, score=-0.4387546257181172, total=   4.0s\n",
      "[CV] alpha=0.01, degree=5, kernel=polynomial .........................\n",
      "[CV] alpha=0.01, degree=5, kernel=polynomial .........................\n",
      "[CV] alpha=0.01, degree=5, kernel=polynomial .........................\n",
      "[CV]  alpha=0.01, degree=5, kernel=polynomial, score=0.00917259634688894, total=   4.2s\n",
      "[CV] alpha=0.01, degree=5, kernel=sigmoid ............................\n",
      "[CV]  alpha=0.01, degree=5, kernel=polynomial, score=0.004255164060664063, total=   4.0s\n",
      "[CV] alpha=0.01, degree=5, kernel=sigmoid ............................\n",
      "[CV]  alpha=0.01, degree=5, kernel=polynomial, score=0.023849270859453986, total=   4.2s\n",
      "[CV] alpha=0.01, degree=5, kernel=sigmoid ............................\n",
      "[CV]  alpha=0.01, degree=5, kernel=polynomial, score=0.07759456696744982, total=   4.2s\n",
      "[CV] alpha=0.01, degree=5, kernel=sigmoid ............................\n",
      "[CV]  alpha=0.01, degree=5, kernel=sigmoid, score=0.02847805755120836, total=   3.1s\n",
      "[CV] alpha=0.01, degree=5, kernel=laplacian ..........................\n",
      "[CV]  alpha=0.01, degree=5, kernel=sigmoid, score=0.04094021857849128, total=   3.1s\n",
      "[CV] alpha=0.01, degree=5, kernel=laplacian ..........................\n",
      "[CV]  alpha=0.01, degree=5, kernel=sigmoid, score=0.07243438943057345, total=   2.9s\n",
      "[CV]  alpha=0.01, degree=5, kernel=sigmoid, score=0.03611694685004141, total=   3.1s\n",
      "[CV] alpha=0.01, degree=5, kernel=laplacian ..........................\n",
      "[CV] alpha=0.01, degree=5, kernel=laplacian ..........................\n",
      "[CV]  alpha=0.01, degree=5, kernel=laplacian, score=0.04155538890342614, total=  10.2s\n",
      "[CV]  alpha=0.01, degree=5, kernel=laplacian, score=-0.014785329159483494, total=  10.3s\n",
      "[CV] alpha=0.1, degree=5, kernel=linear ..............................\n",
      "[CV] alpha=0.1, degree=5, kernel=linear ..............................\n",
      "[CV]  alpha=0.01, degree=5, kernel=laplacian, score=-0.011452382444389286, total=  10.3s\n",
      "[CV] alpha=0.1, degree=5, kernel=linear ..............................\n",
      "[CV]  alpha=0.1, degree=5, kernel=linear, score=-0.5456556793567251, total=   2.1s\n",
      "[CV]  alpha=0.1, degree=5, kernel=linear, score=-0.05196782068032024, total=   2.2s\n",
      "[CV] alpha=0.1, degree=5, kernel=linear ..............................\n",
      "[CV] alpha=0.1, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=0.1, degree=5, kernel=linear, score=-0.08327824795896888, total=   2.1s\n",
      "[CV] alpha=0.1, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=0.01, degree=5, kernel=laplacian, score=0.0026786316075027017, total=  12.2s\n",
      "[CV] alpha=0.1, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=0.1, degree=5, kernel=linear, score=0.02514753121484281, total=   1.9s\n",
      "[CV] alpha=0.1, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=0.1, degree=5, kernel=polynomial, score=0.0269291121014702, total=   3.4s\n",
      "[CV] alpha=0.1, degree=5, kernel=sigmoid .............................\n",
      "[CV]  alpha=0.1, degree=5, kernel=polynomial, score=0.035384398564457076, total=   3.4s\n",
      "[CV]  alpha=0.1, degree=5, kernel=polynomial, score=0.03269418549496794, total=   3.2s\n",
      "[CV] alpha=0.1, degree=5, kernel=sigmoid .............................\n",
      "[CV] alpha=0.1, degree=5, kernel=sigmoid .............................\n",
      "[CV]  alpha=0.1, degree=5, kernel=polynomial, score=0.07578457056115884, total=   3.0s\n",
      "[CV] alpha=0.1, degree=5, kernel=sigmoid .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   40.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.1, degree=5, kernel=sigmoid, score=0.03671089559635987, total=   2.3s\n",
      "[CV] alpha=0.1, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=0.1, degree=5, kernel=sigmoid, score=0.039474017661788374, total=   2.2s\n",
      "[CV]  alpha=0.1, degree=5, kernel=sigmoid, score=0.03874061762254, total=   2.2s\n",
      "[CV] alpha=0.1, degree=5, kernel=laplacian ...........................\n",
      "[CV] alpha=0.1, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=0.1, degree=5, kernel=sigmoid, score=0.05123023980507879, total=   1.8s\n",
      "[CV] alpha=0.1, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=0.1, degree=5, kernel=laplacian, score=0.05497721723113458, total=  10.0s\n",
      "[CV] alpha=1, degree=5, kernel=linear ................................\n",
      "[CV]  alpha=0.1, degree=5, kernel=laplacian, score=0.041127054691729614, total=  10.2s\n",
      "[CV]  alpha=0.1, degree=5, kernel=laplacian, score=0.04150677943465253, total=  10.1s\n",
      "[CV] alpha=1, degree=5, kernel=linear ................................\n",
      "[CV] alpha=1, degree=5, kernel=linear ................................\n",
      "[CV]  alpha=0.1, degree=5, kernel=laplacian, score=0.06875007315483328, total=   9.8s\n",
      "[CV] alpha=1, degree=5, kernel=linear ................................\n",
      "[CV]  alpha=1, degree=5, kernel=linear, score=-0.03446345729099609, total=   1.9s\n",
      "[CV] alpha=1, degree=5, kernel=polynomial ............................\n",
      "[CV]  alpha=1, degree=5, kernel=linear, score=-0.5483342561846041, total=   2.0s\n",
      "[CV]  alpha=1, degree=5, kernel=linear, score=-0.07419114266317317, total=   2.0s\n",
      "[CV] alpha=1, degree=5, kernel=polynomial ............................\n",
      "[CV] alpha=1, degree=5, kernel=polynomial ............................\n",
      "[CV]  alpha=1, degree=5, kernel=linear, score=0.011770806904951803, total=   1.8s\n",
      "[CV] alpha=1, degree=5, kernel=polynomial ............................\n",
      "[CV]  alpha=1, degree=5, kernel=polynomial, score=0.036369896874843044, total=   3.0s\n",
      "[CV] alpha=1, degree=5, kernel=sigmoid ...............................\n",
      "[CV]  alpha=1, degree=5, kernel=polynomial, score=0.03522883536030319, total=   3.0s\n",
      "[CV]  alpha=1, degree=5, kernel=polynomial, score=0.04174128075663097, total=   3.0s\n",
      "[CV] alpha=1, degree=5, kernel=sigmoid ...............................\n",
      "[CV] alpha=1, degree=5, kernel=sigmoid ...............................\n",
      "[CV]  alpha=1, degree=5, kernel=polynomial, score=0.05674985248513709, total=   3.3s\n",
      "[CV] alpha=1, degree=5, kernel=sigmoid ...............................\n",
      "[CV]  alpha=1, degree=5, kernel=sigmoid, score=0.021085453382029273, total=   2.2s\n",
      "[CV] alpha=1, degree=5, kernel=laplacian .............................\n",
      "[CV]  alpha=1, degree=5, kernel=sigmoid, score=0.016044755503439534, total=   2.2s\n",
      "[CV] alpha=1, degree=5, kernel=laplacian .............................\n",
      "[CV]  alpha=1, degree=5, kernel=sigmoid, score=0.018260968710000602, total=   2.3s\n",
      "[CV] alpha=1, degree=5, kernel=laplacian .............................\n",
      "[CV]  alpha=1, degree=5, kernel=sigmoid, score=0.020230910250915835, total=   2.1s\n",
      "[CV] alpha=1, degree=5, kernel=laplacian .............................\n",
      "[CV]  alpha=1, degree=5, kernel=laplacian, score=0.04164684427385412, total=  10.2s\n",
      "[CV] alpha=10, degree=5, kernel=linear ...............................\n",
      "[CV]  alpha=1, degree=5, kernel=laplacian, score=0.05259067519446947, total=  10.1s\n",
      "[CV]  alpha=1, degree=5, kernel=laplacian, score=0.032271605244722834, total=  10.2s\n",
      "[CV] alpha=10, degree=5, kernel=linear ...............................\n",
      "[CV] alpha=10, degree=5, kernel=linear ...............................\n",
      "[CV]  alpha=1, degree=5, kernel=laplacian, score=0.04489774093469068, total=   9.9s\n",
      "[CV] alpha=10, degree=5, kernel=linear ...............................\n",
      "[CV]  alpha=10, degree=5, kernel=linear, score=-0.03287579380838457, total=   1.9s\n",
      "[CV] alpha=10, degree=5, kernel=polynomial ...........................\n",
      "[CV]  alpha=10, degree=5, kernel=linear, score=-0.049114571137745644, total=   1.9s\n",
      "[CV]  alpha=10, degree=5, kernel=linear, score=-0.48413460833893285, total=   2.0s\n",
      "[CV] alpha=10, degree=5, kernel=polynomial ...........................\n",
      "[CV] alpha=10, degree=5, kernel=polynomial ...........................\n",
      "[CV]  alpha=10, degree=5, kernel=linear, score=-0.016620099665683785, total=   1.9s\n",
      "[CV] alpha=10, degree=5, kernel=polynomial ...........................\n",
      "[CV]  alpha=10, degree=5, kernel=polynomial, score=0.02265028885033893, total=   3.0s\n",
      "[CV] alpha=10, degree=5, kernel=sigmoid ..............................\n",
      "[CV]  alpha=10, degree=5, kernel=polynomial, score=0.01128957927352825, total=   3.0s\n",
      "[CV]  alpha=10, degree=5, kernel=polynomial, score=0.02566680374638053, total=   3.0s\n",
      "[CV] alpha=10, degree=5, kernel=sigmoid ..............................\n",
      "[CV] alpha=10, degree=5, kernel=sigmoid ..............................\n",
      "[CV]  alpha=10, degree=5, kernel=polynomial, score=0.0204266783616881, total=   3.0s\n",
      "[CV] alpha=10, degree=5, kernel=sigmoid ..............................\n",
      "[CV]  alpha=10, degree=5, kernel=sigmoid, score=-0.001566574178418545, total=   1.9s\n",
      "[CV] alpha=10, degree=5, kernel=laplacian ............................\n",
      "[CV]  alpha=10, degree=5, kernel=sigmoid, score=-0.013215034076930587, total=   2.2s\n",
      "[CV] alpha=10, degree=5, kernel=laplacian ............................\n",
      "[CV]  alpha=10, degree=5, kernel=sigmoid, score=0.0076015564213212095, total=   2.2s\n",
      "[CV] alpha=10, degree=5, kernel=laplacian ............................\n",
      "[CV]  alpha=10, degree=5, kernel=sigmoid, score=-0.015409480430874645, total=   2.1s\n",
      "[CV] alpha=10, degree=5, kernel=laplacian ............................\n",
      "[CV]  alpha=10, degree=5, kernel=laplacian, score=0.0021697446646401497, total=  10.1s\n",
      "[CV] alpha=100, degree=5, kernel=linear ..............................\n",
      "[CV]  alpha=10, degree=5, kernel=laplacian, score=0.004156108364262945, total=   9.9s\n",
      "[CV] alpha=100, degree=5, kernel=linear ..............................\n",
      "[CV]  alpha=10, degree=5, kernel=laplacian, score=-0.02644378627441557, total=  10.0s\n",
      "[CV] alpha=100, degree=5, kernel=linear ..............................\n",
      "[CV]  alpha=10, degree=5, kernel=laplacian, score=-0.02219948394455673, total=   9.8s\n",
      "[CV] alpha=100, degree=5, kernel=linear ..............................\n",
      "[CV]  alpha=100, degree=5, kernel=linear, score=-0.3673850454752743, total=   1.6s\n",
      "[CV] alpha=100, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=100, degree=5, kernel=linear, score=-0.6865083902197842, total=   2.1s\n",
      "[CV] alpha=100, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=100, degree=5, kernel=linear, score=-0.4360302643323526, total=   2.2s\n",
      "[CV] alpha=100, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=100, degree=5, kernel=linear, score=-0.3475974236907413, total=   2.1s\n",
      "[CV] alpha=100, degree=5, kernel=polynomial ..........................\n",
      "[CV]  alpha=100, degree=5, kernel=polynomial, score=-0.4914242031351894, total=   2.9s\n",
      "[CV] alpha=100, degree=5, kernel=sigmoid .............................\n",
      "[CV]  alpha=100, degree=5, kernel=polynomial, score=-0.5668185934834884, total=   3.0s\n",
      "[CV] alpha=100, degree=5, kernel=sigmoid .............................\n",
      "[CV]  alpha=100, degree=5, kernel=polynomial, score=-0.473910091657195, total=   3.1s\n",
      "[CV] alpha=100, degree=5, kernel=sigmoid .............................\n",
      "[CV]  alpha=100, degree=5, kernel=polynomial, score=-0.5902674744797312, total=   2.9s\n",
      "[CV] alpha=100, degree=5, kernel=sigmoid .............................\n",
      "[CV]  alpha=100, degree=5, kernel=sigmoid, score=-1.0032563826738783, total=   1.7s\n",
      "[CV] alpha=100, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=100, degree=5, kernel=sigmoid, score=-1.1196823070674178, total=   2.2s\n",
      "[CV] alpha=100, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=100, degree=5, kernel=sigmoid, score=-1.0144073813044479, total=   2.2s\n",
      "[CV] alpha=100, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=100, degree=5, kernel=sigmoid, score=-1.1431816188782467, total=   2.1s\n",
      "[CV] alpha=100, degree=5, kernel=laplacian ...........................\n",
      "[CV]  alpha=100, degree=5, kernel=laplacian, score=-0.6411692141260066, total=  11.0s\n",
      "[CV]  alpha=100, degree=5, kernel=laplacian, score=-0.7695075574168497, total=  10.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=100, degree=5, kernel=laplacian, score=-0.6318438017582717, total=  10.5s\n",
      "[CV]  alpha=100, degree=5, kernel=laplacian, score=-0.7732679649108385, total=  10.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  80 out of  80 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'degree': 5, 'kernel': 'laplacian'}\n"
     ]
    }
   ],
   "source": [
    "# Tuning kernel ridge\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['linear','polynomial', 'sigmoid', 'laplacian'],\n",
    "    'degree': [5]\n",
    "}\n",
    "\n",
    "kr = KR()\n",
    "gskr = GridSearchCV(estimator=kr, param_grid=params, cv=4, n_jobs=4, verbose=3)\n",
    "gskr.fit(X_train, y_train)\n",
    "\n",
    "print(gskr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02716235436637317"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(10, KR(alpha=0.1, kernel='laplacian'), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.928695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.913394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.918961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.931840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.923172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id  Predicted\n",
       "0  1   0.928695\n",
       "1  2   0.913394\n",
       "2  3   0.918961\n",
       "3  4   0.931840\n",
       "4  5   0.923172"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krpredict = gskr.best_estimator_.predict(test.iloc[:, 1:])\n",
    "\n",
    "submission8 = pd.DataFrame(data=krpredict, columns=['Predicted'])\n",
    "submission8.insert(0, \"Id\", range(1, 1 + test.shape[0]))\n",
    "submission8['Id'] = submission8['Id'].astype(str)\n",
    "submission8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission8.to_csv(\"kr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR n = 50 : 0.02705291658433643\n",
      "ABR n = 50 : 0.028649503965292155\n",
      "BR n = 50 : 0.0270610860889704\n",
      "ETR n = 50 : 0.027331120634888885\n",
      "RFR n = 100 : 0.02692968607708654\n",
      "ABR n = 100 : 0.028649503965292155\n",
      "BR n = 100 : 0.026936153403063844\n",
      "ETR n = 100 : 0.027218653780815016\n",
      "RFR n = 150 : 0.02693283560780489\n",
      "ABR n = 150 : 0.028649503965292155\n",
      "BR n = 150 : 0.02691695728076937\n",
      "ETR n = 150 : 0.027178657630401493\n",
      "RFR n = 200 : 0.02692523963007199\n",
      "ABR n = 200 : 0.028649503965292155\n",
      "BR n = 200 : 0.026905604961093205\n",
      "ETR n = 200 : 0.027143235082859186\n",
      "RFR n = 300 : 0.026893909888482864\n",
      "ABR n = 300 : 0.028649503965292155\n",
      "BR n = 300 : 0.026903843640258367\n",
      "ETR n = 300 : 0.0271188293286418\n",
      "RFR n = 400 : 0.026887838108116624\n",
      "ABR n = 400 : 0.028649503965292155\n",
      "BR n = 400 : 0.02689096703049289\n",
      "ETR n = 400 : 0.02712284487139559\n",
      "RFR n = 500 : 0.026885875377998525\n",
      "ABR n = 500 : 0.028649503965292155\n",
      "BR n = 500 : 0.026892693538960916\n",
      "ETR n = 500 : 0.02710615693472219\n",
      "RFR n = 750 : 0.02687203305623213\n",
      "ABR n = 750 : 0.028649503965292155\n",
      "BR n = 750 : 0.026881192163204247\n",
      "ETR n = 750 : 0.027094647991350584\n",
      "RFR n = 1000 : 0.02687836681862289\n",
      "ABR n = 1000 : 0.028649503965292155\n",
      "BR n = 1000 : 0.026883865857519656\n",
      "ETR n = 1000 : 0.02708513410294063\n"
     ]
    }
   ],
   "source": [
    "# More n_est analysis\n",
    "\n",
    "for n_est in [50, 100, 150, 200, 300, 400, 500, 750, 1000]:\n",
    "    print(\"RFR n =\", n_est, \":\", kfold(5, RFR(n_estimators=n_est, random_state=50, n_jobs=4), train))\n",
    "    print(\"ABR n =\", n_est, \":\", kfold(5, ABR(n_estimators=n_est, random_state=50), train))\n",
    "    print(\"BR n =\", n_est, \":\", kfold(5, BR(n_estimators=n_est, random_state=50, n_jobs=4), train))\n",
    "    print(\"ETR n =\", n_est, \":\", kfold(5, ETR(n_estimators=n_est, random_state=50, n_jobs=4), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR n = 50 : 0.027164415494160776\n",
      "ABR n = 50 : 0.029313871153457034\n",
      "BR n = 50 : 0.027110324069706714\n",
      "ETR n = 50 : 0.02726896525618902\n",
      "RFR n = 100 : 0.02701084155903099\n",
      "ABR n = 100 : 0.029313871153457034\n",
      "BR n = 100 : 0.0269860864652349\n",
      "ETR n = 100 : 0.027172267642285853\n",
      "RFR n = 150 : 0.02694101695614845\n",
      "ABR n = 150 : 0.029313871153457034\n",
      "BR n = 150 : 0.026924773012323544\n",
      "ETR n = 150 : 0.027125638464048602\n",
      "RFR n = 200 : 0.026927012158246943\n",
      "ABR n = 200 : 0.029313871153457034\n",
      "BR n = 200 : 0.02691816683734672\n",
      "ETR n = 200 : 0.0271328923968459\n",
      "RFR n = 300 : 0.026896613875746756\n",
      "ABR n = 300 : 0.029313871153457034\n",
      "BR n = 300 : 0.026897840479847314\n",
      "ETR n = 300 : 0.027104249889598375\n",
      "RFR n = 400 : 0.026901825150474273\n",
      "ABR n = 400 : 0.029313871153457034\n",
      "BR n = 400 : 0.02690633581558219\n",
      "ETR n = 400 : 0.027092452995255506\n",
      "RFR n = 500 : 0.02690204077581141\n",
      "ABR n = 500 : 0.029313871153457034\n",
      "BR n = 500 : 0.02690233024776469\n",
      "ETR n = 500 : 0.02708869630193928\n",
      "RFR n = 750 : 0.026884329137684904\n",
      "ABR n = 750 : 0.029313871153457034\n",
      "BR n = 750 : 0.026882777085619314\n",
      "ETR n = 750 : 0.027074551595100277\n",
      "RFR n = 1000 : 0.026875483924165767\n",
      "ABR n = 1000 : 0.029313871153457034\n",
      "BR n = 1000 : 0.026867398098950702\n",
      "ETR n = 1000 : 0.027061545052575346\n"
     ]
    }
   ],
   "source": [
    "for n_est in [50, 100, 150, 200, 300, 400, 500, 750, 1000]:\n",
    "    print(\"RFR n =\", n_est, \":\", kfold(5, RFR(n_estimators=n_est, random_state=40, n_jobs=4), train))\n",
    "    print(\"ABR n =\", n_est, \":\", kfold(5, ABR(n_estimators=n_est, random_state=40), train))\n",
    "    print(\"BR n =\", n_est, \":\", kfold(5, BR(n_estimators=n_est, random_state=40, n_jobs=4), train))\n",
    "    print(\"ETR n =\", n_est, \":\", kfold(5, ETR(n_estimators=n_est, random_state=40, n_jobs=4), train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.0007493241842653842, total=  11.8s\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.000724310089038383, total=  11.7s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.0008344162824369213, total=  11.8s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.000662863687607998, total=  11.9s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0007485322228243138, total=  11.7s\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0008294397495499233, total=  11.7s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0006594759446757816, total=  11.6s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.0007098937998585664, total=  11.9s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0007506392606246386, total=  10.9s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0007213030440407204, total=  11.2s\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0007057357267376652, total=  11.2s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0008273863911962887, total=  11.0s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV] max_depth=5, min_samples_leaf=30 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=30, score=-0.00075156547331667, total=  10.9s\n",
      "[CV] max_depth=5, min_samples_leaf=30 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0006597729789921823, total=  11.2s\n",
      "[CV] max_depth=5, min_samples_leaf=30 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.000722843482945386, total=  11.2s\n",
      "[CV] max_depth=5, min_samples_leaf=30 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0007046824158772954, total=  11.2s\n",
      "[CV] max_depth=5, min_samples_leaf=30 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=30, score=-0.0008262026521533894, total=  11.3s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=30, score=-0.0006599391033876332, total=  11.1s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=30, score=-0.0007241041033463191, total=  11.2s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=30, score=-0.0007051930695885537, total=  11.1s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.0007314338135364755, total=  32.9s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.000658407300908017, total=  32.9s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.0007052192756860904, total=  32.9s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.000816669765637163, total=  33.2s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0007412709966138088, total=  24.5s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0008127469481378861, total=  24.6s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0006537203944198717, total=  24.6s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.0006955415891697044, total=  33.4s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0007450436705725014, total=  20.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0007098141476388169, total=  24.1s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0006964805866357232, total=  24.5s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0008170051594841805, total=  20.0s\n",
      "[CV] max_depth=15, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0006538707105870384, total=  20.1s\n",
      "[CV] max_depth=15, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0007178335668395459, total=  19.8s\n",
      "[CV] max_depth=15, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0006955914944609474, total=  20.1s\n",
      "[CV] max_depth=15, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=30, score=-0.0007470582175219499, total=  17.6s\n",
      "[CV] max_depth=15, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=30, score=-0.0008161400118025765, total=  17.4s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=30, score=-0.0006545263379903267, total=  17.5s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=30, score=-0.0007196447687598972, total=  17.2s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=30, score=-0.0006994030906986937, total=  17.5s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0007290369097502298, total=  49.2s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.000816231506145749, total=  49.5s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0006672728830377998, total=  49.2s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0007050593464696911, total=  49.3s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0007410415587099444, total=  26.1s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0008129921400559102, total=  26.1s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0006542684958915503, total=  26.1s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0006987794738824265, total=  49.5s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0007449982426849325, total=  20.3s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0007103294453868031, total=  25.5s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0006966119464801717, total=  26.1s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0008168788850719746, total=  20.3s\n",
      "[CV] max_depth=25, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0006539956926624992, total=  20.5s\n",
      "[CV] max_depth=25, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0007179574983367239, total=  19.9s\n",
      "[CV] max_depth=25, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0006957324773569059, total=  20.3s\n",
      "[CV] max_depth=25, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=30, score=-0.0007469141863388192, total=  17.4s\n",
      "[CV] max_depth=25, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=30, score=-0.0008161954004790546, total=  17.4s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=30, score=-0.0006545859036046087, total=  17.4s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=30, score=-0.0007195553434525134, total=  17.2s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=30, score=-0.0006994069142386003, total=  17.3s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0007291812171347867, total=  57.0s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0008163229137943558, total=  57.2s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0006705729281881317, total=  57.6s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.000707790022069978, total=  57.0s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0007409082738056272, total=  26.6s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0008130993172766619, total=  26.4s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0006543759530551213, total=  26.9s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0007103522033543093, total=  25.8s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0006965667244837254, total=  26.5s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0006956072967956024, total=  57.9s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0007449953212310106, total=  20.4s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0008168783993444499, total=  20.3s\n",
      "[CV] max_depth=35, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0007179574983367239, total=  19.9s\n",
      "[CV] max_depth=35, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.000653994276975462, total=  20.4s\n",
      "[CV] max_depth=35, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0006957324773569059, total=  20.5s\n",
      "[CV] max_depth=35, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=30, score=-0.0007469141863388192, total=  17.8s\n",
      "[CV] max_depth=35, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=30, score=-0.0008161954004790546, total=  17.6s\n",
      "[CV] max_depth=45, min_samples_leaf=1 ................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=35, min_samples_leaf=30, score=-0.0006545859036046087, total=  17.7s\n",
      "[CV] max_depth=45, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=30, score=-0.0007195553434525134, total=  17.4s\n",
      "[CV] max_depth=45, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=30, score=-0.0006994069142386003, total=  17.4s\n",
      "[CV] max_depth=45, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=45, min_samples_leaf=1, score=-0.00072992703155188, total=  59.0s\n",
      "[CV] max_depth=45, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=45, min_samples_leaf=1, score=-0.0008184011765428323, total=  58.9s\n",
      "[CV] max_depth=45, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=1, score=-0.0006699597589792133, total=  59.9s\n",
      "[CV] max_depth=45, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=1, score=-0.0007059381773517987, total=  58.9s\n",
      "[CV] max_depth=45, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=10, score=-0.0007409082738056272, total=  26.2s\n",
      "[CV] max_depth=45, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=10, score=-0.0008130990414347386, total=  26.0s\n",
      "[CV] max_depth=45, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=10, score=-0.0006543769542885482, total=  26.4s\n",
      "[CV] max_depth=45, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=10, score=-0.0007103522033543093, total=  25.7s\n",
      "[CV] max_depth=45, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=10, score=-0.0006965630637213706, total=  26.4s\n",
      "[CV] max_depth=45, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=1, score=-0.0006975854481621539, total=  59.9s\n",
      "[CV] max_depth=45, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=20, score=-0.0007449953212310106, total=  20.5s\n",
      "[CV] max_depth=45, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=20, score=-0.0008168783993444499, total=  20.2s\n",
      "[CV] max_depth=45, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=20, score=-0.000653994276975462, total=  20.5s\n",
      "[CV] max_depth=45, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=20, score=-0.0007179574983367239, total=  20.0s\n",
      "[CV] max_depth=45, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=20, score=-0.0006957324773569059, total=  20.4s\n",
      "[CV] max_depth=45, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=30, score=-0.0007469141863388192, total=  17.6s\n",
      "[CV] max_depth=45, min_samples_leaf=30 ...............................\n",
      "[CV]  max_depth=45, min_samples_leaf=30, score=-0.0008161954004790546, total=  17.5s\n",
      "[CV]  max_depth=45, min_samples_leaf=30, score=-0.0006545859036046087, total=  17.3s\n",
      "[CV]  max_depth=45, min_samples_leaf=30, score=-0.0007195553434525134, total=  16.3s\n",
      "[CV]  max_depth=45, min_samples_leaf=30, score=-0.0006994069142386003, total=  13.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 10.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 15, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# Additional RFR tuning at n_est=300\n",
    "\n",
    "params = {\n",
    "    'max_depth': [5, 15, 25, 35, 45],\n",
    "    'min_samples_leaf': [1, 10, 20, 30]\n",
    "}\n",
    "\n",
    "rfR = RFR(n_estimators=300, random_state=50)\n",
    "gsrfr = GridSearchCV(estimator=rfR, scoring='neg_mean_squared_error', param_grid=params, n_jobs=-1, cv=5, verbose=3)\n",
    "gsrfr.fit(X_train, y_train)\n",
    "\n",
    "print(gsrfr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.000724310089038383, total=  12.8s\n",
      "[CV] max_depth=5, min_samples_leaf=1 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.0007493241842653842, total=  12.9s\n",
      "[CV] max_depth=5, min_samples_leaf=5 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.0008344162824369213, total=  12.9s\n",
      "[CV] max_depth=5, min_samples_leaf=5 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.000662863687607998, total=  13.0s\n",
      "[CV] max_depth=5, min_samples_leaf=5 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=5, score=-0.0007480494241058517, total=  11.5s\n",
      "[CV] max_depth=5, min_samples_leaf=5 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=5, score=-0.0008310208067319556, total=  11.5s\n",
      "[CV] max_depth=5, min_samples_leaf=5 .................................\n",
      "[CV]  max_depth=5, min_samples_leaf=1, score=-0.0007098937998585664, total=  11.7s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=5, score=-0.0006609351240538169, total=  11.5s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0007485322228243138, total=  11.3s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=5, score=-0.0007075787626553777, total=  11.4s\n",
      "[CV]  max_depth=5, min_samples_leaf=5, score=-0.0007219506665251975, total=  11.4s\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV] max_depth=5, min_samples_leaf=10 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0008294397495499233, total=  11.3s\n",
      "[CV] max_depth=5, min_samples_leaf=15 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0006594759446757816, total=  11.6s\n",
      "[CV] max_depth=5, min_samples_leaf=15 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=15, score=-0.0007491459937316654, total=  11.5s\n",
      "[CV] max_depth=5, min_samples_leaf=15 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0007057357267376652, total=  11.5s\n",
      "[CV] max_depth=5, min_samples_leaf=15 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=10, score=-0.0007213030440407204, total=  11.7s\n",
      "[CV] max_depth=5, min_samples_leaf=15 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=15, score=-0.0008278797632912274, total=  11.1s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=15, score=-0.00065913575087381, total=  11.1s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=15, score=-0.0007229603744994471, total=  11.2s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=15, score=-0.0007054663729910238, total=  11.1s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0007506392606246386, total=  11.7s\n",
      "[CV] max_depth=5, min_samples_leaf=20 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0008273863911962887, total=  11.8s\n",
      "[CV] max_depth=5, min_samples_leaf=25 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0006597729789921823, total=  11.7s\n",
      "[CV] max_depth=5, min_samples_leaf=25 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.000722843482945386, total=  11.9s\n",
      "[CV] max_depth=5, min_samples_leaf=25 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=5, min_samples_leaf=25, score=-0.0007514777692323965, total=  11.4s\n",
      "[CV] max_depth=5, min_samples_leaf=25 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=20, score=-0.0007046824158772954, total=  11.6s\n",
      "[CV] max_depth=5, min_samples_leaf=25 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=25, score=-0.000826787281143119, total=  11.6s\n",
      "[CV] max_depth=10, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=25, score=-0.0006595929821097879, total=  11.5s\n",
      "[CV] max_depth=10, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=25, score=-0.0007232262747547938, total=  12.0s\n",
      "[CV] max_depth=10, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=5, min_samples_leaf=25, score=-0.0007050162197506541, total=  12.1s\n",
      "[CV] max_depth=10, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=1, score=-0.0008207336297914871, total=  25.5s\n",
      "[CV] max_depth=10, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=1, score=-0.000733042198002854, total=  25.6s\n",
      "[CV] max_depth=10, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=1, score=-0.0006541224221194718, total=  26.0s\n",
      "[CV] max_depth=10, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=1, score=-0.0007098967326233763, total=  26.1s\n",
      "[CV] max_depth=10, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=5, score=-0.0007345565525277686, total=  24.8s\n",
      "[CV] max_depth=10, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=1, score=-0.0006998391485800791, total=  26.2s\n",
      "[CV] max_depth=10, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=5, score=-0.0008145463845884557, total=  24.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=5, score=-0.0006543468933632992, total=  24.1s\n",
      "[CV] max_depth=10, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=5, score=-0.0007091123101992233, total=  23.7s\n",
      "[CV] max_depth=10, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=5, score=-0.0006957438993996355, total=  24.3s\n",
      "[CV] max_depth=10, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=10, score=-0.0007410122090393014, total=  22.5s\n",
      "[CV] max_depth=10, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=10, score=-0.0008154140269457912, total=  22.7s\n",
      "[CV] max_depth=10, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=10, score=-0.0006545973386059329, total=  21.1s\n",
      "[CV] max_depth=10, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=10, score=-0.0007110384501063359, total=  20.7s\n",
      "[CV] max_depth=10, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=15, score=-0.0007442388780807889, total=  19.6s\n",
      "[CV] max_depth=10, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=10, score=-0.0006976949817111065, total=  20.9s\n",
      "[CV] max_depth=10, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=15, score=-0.00081643773915129, total=  20.7s\n",
      "[CV] max_depth=10, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=15, score=-0.0006533433531092991, total=  21.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=15, score=-0.0007171019632577218, total=  21.4s\n",
      "[CV] max_depth=10, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=15, score=-0.0006962282235938507, total=  21.5s\n",
      "[CV] max_depth=10, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=20, score=-0.0007447255152878996, total=  20.6s\n",
      "[CV] max_depth=10, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=20, score=-0.0008179724566313614, total=  20.8s\n",
      "[CV] max_depth=10, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=20, score=-0.0006538146917323501, total=  20.9s\n",
      "[CV] max_depth=10, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=20, score=-0.0007181402095393009, total=  20.9s\n",
      "[CV] max_depth=10, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=20, score=-0.0006963358294207377, total=  21.3s\n",
      "[CV] max_depth=10, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=25, score=-0.0007471254723761815, total=  20.2s\n",
      "[CV] max_depth=10, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=10, min_samples_leaf=25, score=-0.0008166301187603931, total=  20.9s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=25, score=-0.0006547205047747895, total=  21.0s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=25, score=-0.0007179843663509199, total=  20.4s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=10, min_samples_leaf=25, score=-0.0006990999816131787, total=  20.8s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.0007314338135364755, total=  40.2s\n",
      "[CV] max_depth=15, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.000816669765637163, total=  40.1s\n",
      "[CV] max_depth=15, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.000658407300908017, total=  39.7s\n",
      "[CV] max_depth=15, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.0007052192756860904, total=  39.2s\n",
      "[CV] max_depth=15, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=5, score=-0.0007324975508173113, total=  30.3s\n",
      "[CV] max_depth=15, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=1, score=-0.0006955415891697044, total=  36.6s\n",
      "[CV] max_depth=15, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=5, score=-0.000809938819698233, total=  30.9s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=5, score=-0.0006565934669490469, total=  31.0s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=5, score=-0.0007077639749148113, total=  30.7s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0007412709966138088, total=  25.3s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=5, score=-0.0006936935017718255, total=  30.1s\n",
      "[CV] max_depth=15, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0008127469481378861, total=  25.5s\n",
      "[CV] max_depth=15, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0006537203944198717, total=  24.8s\n",
      "[CV] max_depth=15, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=15, score=-0.0007442538205613503, total=  22.1s\n",
      "[CV] max_depth=15, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0007098141476388169, total=  24.4s\n",
      "[CV] max_depth=15, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=10, score=-0.0006964805866357232, total=  24.8s\n",
      "[CV] max_depth=15, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=15, score=-0.0008147631447798002, total=  24.4s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=15, min_samples_leaf=15, score=-0.0006532159593633333, total=  24.6s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=15, score=-0.0007168977874244083, total=  24.3s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=15, score=-0.0006952673133177487, total=  24.8s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0007450436705725014, total=  23.9s\n",
      "[CV] max_depth=15, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0008170051594841805, total=  24.1s\n",
      "[CV] max_depth=15, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0006538707105870384, total=  25.1s\n",
      "[CV] max_depth=15, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0007178335668395459, total=  24.6s\n",
      "[CV] max_depth=15, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=20, score=-0.0006955914944609474, total=  23.2s\n",
      "[CV] max_depth=15, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=25, score=-0.0007468912112256411, total=  21.3s\n",
      "[CV] max_depth=15, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=15, min_samples_leaf=25, score=-0.0008161605196517402, total=  20.3s\n",
      "[CV] max_depth=20, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=25, score=-0.0006550570177509906, total=  20.2s\n",
      "[CV] max_depth=20, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=25, score=-0.0007178576646503624, total=  19.8s\n",
      "[CV] max_depth=20, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=15, min_samples_leaf=25, score=-0.0006987401055158776, total=  20.1s\n",
      "[CV] max_depth=20, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=1, score=-0.0007291604443109632, total=  43.8s\n",
      "[CV] max_depth=20, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=1, score=-0.0008156679105920487, total=  43.9s\n",
      "[CV] max_depth=20, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=1, score=-0.0006629207200932988, total=  42.3s\n",
      "[CV] max_depth=20, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=1, score=-0.0007034216112567425, total=  42.3s\n",
      "[CV] max_depth=20, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=5, score=-0.0007332494818127397, total=  32.6s\n",
      "[CV] max_depth=20, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=1, score=-0.0006974144945169336, total=  45.4s\n",
      "[CV] max_depth=20, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=5, score=-0.0008094419198377194, total=  35.5s\n",
      "[CV] max_depth=20, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=5, score=-0.0006578861181074731, total=  35.8s\n",
      "[CV] max_depth=20, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=5, score=-0.0007074982994628288, total=  34.7s\n",
      "[CV] max_depth=20, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=10, score=-0.0007410593732379192, total=  28.1s\n",
      "[CV] max_depth=20, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=5, score=-0.0006945761553210809, total=  34.8s\n",
      "[CV] max_depth=20, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=10, score=-0.0008130440952546987, total=  28.2s\n",
      "[CV] max_depth=20, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=10, score=-0.0006543846333953812, total=  29.6s\n",
      "[CV] max_depth=20, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=15, score=-0.0007439632339723505, total=  24.6s\n",
      "[CV] max_depth=20, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=10, score=-0.0007100918730778087, total=  27.6s\n",
      "[CV] max_depth=20, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=10, score=-0.0006969725267890025, total=  28.6s\n",
      "[CV] max_depth=20, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=15, score=-0.0008146726029378532, total=  26.6s\n",
      "[CV] max_depth=20, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=15, score=-0.0006535118864639326, total=  29.0s\n",
      "[CV] max_depth=20, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=15, score=-0.0007170752497123762, total=  28.2s\n",
      "[CV] max_depth=20, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=15, score=-0.0006950658700340182, total=  28.8s\n",
      "[CV] max_depth=20, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=20, score=-0.0007450139025362353, total=  25.1s\n",
      "[CV] max_depth=20, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=20, score=-0.0008168595791600843, total=  23.0s\n",
      "[CV] max_depth=20, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=20, score=-0.000654000128935191, total=  23.0s\n",
      "[CV] max_depth=20, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=20, score=-0.0007180031518153086, total=  22.4s\n",
      "[CV] max_depth=20, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=20, score=-0.0006957266869381842, total=  22.3s\n",
      "[CV] max_depth=20, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=25, score=-0.0007467448627750005, total=  21.9s\n",
      "[CV] max_depth=20, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=20, min_samples_leaf=25, score=-0.0008161122844685088, total=  22.0s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=25, score=-0.0006551162642835631, total=  22.2s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=25, score=-0.0007178798048198951, total=  21.3s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=20, min_samples_leaf=25, score=-0.0006986189675703591, total=  20.9s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 11.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0007290369097502298, total=  54.8s\n",
      "[CV] max_depth=25, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.000816231506145749, total=  55.0s\n",
      "[CV] max_depth=25, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0006672728830377998, total=  53.7s\n",
      "[CV] max_depth=25, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0007050593464696911, total=  53.5s\n",
      "[CV] max_depth=25, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=5, score=-0.0007332983060982229, total=  36.3s\n",
      "[CV] max_depth=25, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=5, score=-0.000810122674558885, total=  39.0s\n",
      "[CV] max_depth=25, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=1, score=-0.0006987794738824265, total=  57.8s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=5, score=-0.0006588498584227162, total=  40.7s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=5, score=-0.0007063558888271401, total=  42.0s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0007410415587099444, total=  32.3s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=5, score=-0.0006953992379168863, total=  41.3s\n",
      "[CV] max_depth=25, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0008129921400559102, total=  32.4s\n",
      "[CV] max_depth=25, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0006542684958915503, total=  30.6s\n",
      "[CV] max_depth=25, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=15, score=-0.0007440234796823454, total=  26.6s\n",
      "[CV] max_depth=25, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0007103294453868031, total=  29.6s\n",
      "[CV] max_depth=25, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=10, score=-0.0006966119464801717, total=  30.3s\n",
      "[CV] max_depth=25, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=15, score=-0.0008146713679370912, total=  24.0s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=15, score=-0.0006536159746259766, total=  23.1s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=15, score=-0.0007170283955941944, total=  22.6s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=15, score=-0.0006950640550991202, total=  23.0s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0007449982426849325, total=  21.0s\n",
      "[CV] max_depth=25, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0008168788850719746, total=  21.5s\n",
      "[CV] max_depth=25, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0006539956926624992, total=  21.7s\n",
      "[CV] max_depth=25, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0007179574983367239, total=  21.3s\n",
      "[CV] max_depth=25, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=20, score=-0.0006957324773569059, total=  21.4s\n",
      "[CV] max_depth=25, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=25, score=-0.0007467400107342905, total=  19.5s\n",
      "[CV] max_depth=25, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=25, min_samples_leaf=25, score=-0.0008161102511921191, total=  19.4s\n",
      "[CV] max_depth=30, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=25, score=-0.000655117555357487, total=  19.3s\n",
      "[CV] max_depth=30, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=25, score=-0.0007178798048198951, total=  21.4s\n",
      "[CV] max_depth=30, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=25, min_samples_leaf=25, score=-0.0006986153664028944, total=  22.3s\n",
      "[CV] max_depth=30, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=1, score=-0.0007282030540489317, total= 1.1min\n",
      "[CV] max_depth=30, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=1, score=-0.0008194970978434007, total= 1.1min\n",
      "[CV] max_depth=30, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=1, score=-0.0006669552414889477, total= 1.0min\n",
      "[CV] max_depth=30, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=1, score=-0.0007042237615450135, total= 1.0min\n",
      "[CV] max_depth=30, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=5, score=-0.0007334748807758462, total=  38.0s\n",
      "[CV] max_depth=30, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=5, score=-0.0008105085393676478, total=  37.2s\n",
      "[CV] max_depth=30, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=5, score=-0.0006592607644005769, total=  37.5s\n",
      "[CV] max_depth=30, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=1, score=-0.0006970859813092398, total= 1.0min\n",
      "[CV] max_depth=30, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=5, score=-0.0007069112862356799, total=  38.6s\n",
      "[CV] max_depth=30, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=10, score=-0.0007409214758517839, total=  32.0s\n",
      "[CV] max_depth=30, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=5, score=-0.0006954053263195764, total=  40.7s\n",
      "[CV] max_depth=30, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=10, score=-0.0008130936737580516, total=  31.5s\n",
      "[CV] max_depth=30, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=10, score=-0.0006543794784755877, total=  30.5s\n",
      "[CV] max_depth=30, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=15, score=-0.0007440065221648704, total=  26.3s\n",
      "[CV] max_depth=30, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=10, score=-0.0007103467467088152, total=  29.3s\n",
      "[CV] max_depth=30, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=10, score=-0.0006965543048796047, total=  30.4s\n",
      "[CV] max_depth=30, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=15, score=-0.0008146759498664653, total=  27.3s\n",
      "[CV] max_depth=30, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=15, score=-0.0007170492440029859, total=  27.3s\n",
      "[CV] max_depth=30, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=15, score=-0.0006536172509151169, total=  28.2s\n",
      "[CV] max_depth=30, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=15, score=-0.0006950697668534891, total=  28.0s\n",
      "[CV] max_depth=30, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=20, score=-0.0007449953212310106, total=  23.3s\n",
      "[CV] max_depth=30, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=20, score=-0.0008168783993444499, total=  21.6s\n",
      "[CV] max_depth=30, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=20, score=-0.000653994276975462, total=  21.5s\n",
      "[CV] max_depth=30, min_samples_leaf=25 ...............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=30, min_samples_leaf=20, score=-0.0007179574983367239, total=  20.9s\n",
      "[CV] max_depth=30, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=20, score=-0.0006957324773569059, total=  21.7s\n",
      "[CV] max_depth=30, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=25, score=-0.0007467400107342905, total=  21.4s\n",
      "[CV] max_depth=30, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=30, min_samples_leaf=25, score=-0.0008161102511921191, total=  21.6s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=25, score=-0.000655117555357487, total=  22.1s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=25, score=-0.0007178798048198951, total=  22.7s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=30, min_samples_leaf=25, score=-0.0006986153664028944, total=  22.3s\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0007291812171347867, total= 1.1min\n",
      "[CV] max_depth=35, min_samples_leaf=1 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0008163229137943558, total= 1.1min\n",
      "[CV] max_depth=35, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0006705729281881317, total= 1.1min\n",
      "[CV] max_depth=35, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.000707790022069978, total= 1.1min\n",
      "[CV] max_depth=35, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=5, score=-0.0007333750122972962, total=  37.0s\n",
      "[CV] max_depth=35, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=5, score=-0.0008104120974745999, total=  35.8s\n",
      "[CV] max_depth=35, min_samples_leaf=5 ................................\n",
      "[CV]  max_depth=35, min_samples_leaf=5, score=-0.0006593931460858513, total=  35.6s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=1, score=-0.0006956072967956024, total= 1.0min\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=5, score=-0.0007067774884775769, total=  35.8s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0007409082738056272, total=  28.4s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=5, score=-0.0006956070508852949, total=  36.6s\n",
      "[CV] max_depth=35, min_samples_leaf=10 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0008130993172766619, total=  28.9s\n",
      "[CV] max_depth=35, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0006543759530551213, total=  29.0s\n",
      "[CV] max_depth=35, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0007103522033543093, total=  27.7s\n",
      "[CV] max_depth=35, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=10, score=-0.0006965667244837254, total=  28.5s\n",
      "[CV] max_depth=35, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=15, score=-0.0007440065221648704, total=  23.8s\n",
      "[CV] max_depth=35, min_samples_leaf=15 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=15, score=-0.0008146759498664653, total=  22.6s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=15, score=-0.0006536172509151169, total=  22.9s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=15, score=-0.0007170492440029859, total=  22.2s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=15, score=-0.0006950708339449748, total=  22.7s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0007449953212310106, total=  20.4s\n",
      "[CV] max_depth=35, min_samples_leaf=20 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0008168783993444499, total=  20.3s\n",
      "[CV] max_depth=35, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.000653994276975462, total=  20.4s\n",
      "[CV] max_depth=35, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0007179574983367239, total=  20.0s\n",
      "[CV] max_depth=35, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=20, score=-0.0006957324773569059, total=  22.0s\n",
      "[CV] max_depth=35, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=25, score=-0.0007467400107342905, total=  21.5s\n",
      "[CV] max_depth=35, min_samples_leaf=25 ...............................\n",
      "[CV]  max_depth=35, min_samples_leaf=25, score=-0.0008161102511921191, total=  21.4s\n",
      "[CV]  max_depth=35, min_samples_leaf=25, score=-0.000655117555357487, total=  21.4s\n",
      "[CV]  max_depth=35, min_samples_leaf=25, score=-0.0007178798048198951, total=  16.4s\n",
      "[CV]  max_depth=35, min_samples_leaf=25, score=-0.0006986153664028944, total=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 24.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 15, 'min_samples_leaf': 5}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30, 35],\n",
    "    'min_samples_leaf': [1, 5, 10, 15, 20, 25]\n",
    "}\n",
    "\n",
    "rfR = RFR(n_estimators=300, random_state=50)\n",
    "gsrfr = GridSearchCV(estimator=rfR, scoring='neg_mean_squared_error', param_grid=params, n_jobs=-1, cv=5, verbose=3)\n",
    "gsrfr.fit(X_train, y_train)\n",
    "\n",
    "print(gsrfr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "def cv_rmse(GS, i):\n",
    "    running = sqrt(-(GS.cv_results_['split0_test_score'][i]))\n",
    "    running += sqrt(-(GS.cv_results_['split1_test_score'][i]))\n",
    "    running += sqrt(-(GS.cv_results_['split2_test_score'][i]))\n",
    "    running += sqrt(-(GS.cv_results_['split3_test_score'][i]))\n",
    "    running += sqrt(-(GS.cv_results_['split4_test_score'][i]))\n",
    "    return running/5*10000 - 268\n",
    "\n",
    "resultsd = np.array([gsrfr.cv_results_['param_max_depth'][i] for i in range(len(gsrfr.cv_results_['params']))])\n",
    "resultsl = np.array([gsrfr.cv_results_['param_min_samples_leaf'][i] for i in range(len(gsrfr.cv_results_['params']))])\n",
    "resultscv = np.array([cv_rmse(gsrfr, i) for i in range(len(gsrfr.cv_results_['split0_test_score']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.scatter(resultsd[i::6], resultscv[i::6], label=(\"min_samples_leaf = \" + str(resultsl[i])))\n",
    "plt.legend()\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Scaled 5-fold RMSE')\n",
    "plt.savefig('rfrgrid2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "for i in range(6):\n",
    "    ax.scatter(resultsd[i::6], resultsl[i::6], resultscv[i::6])\n",
    "    ax.plot(resultsd[i::6], resultsl[i::6], resultscv[i::6])\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('min_samples_leaf')\n",
    "plt.savefig('rfrgrid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] .. n_estimators=2000, score=-0.0007649287251163518, total= 8.4min\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] .. n_estimators=2000, score=-0.0007738380726630949, total= 8.4min\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] .. n_estimators=2000, score=-0.0006880693821534702, total= 8.5min\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] .. n_estimators=2000, score=-0.0008673338605855696, total= 8.5min\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] ... n_estimators=2000, score=-0.000677096122168506, total= 8.1min\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] .... n_estimators=2000, score=-0.00073011328060626, total= 8.1min\n",
      "[CV] n_estimators=2000 ...............................................\n",
      "[CV] .. n_estimators=2000, score=-0.0006369363181692178, total= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed: 16.8min remaining:  7.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_estimators=2000, score=-0.0006789765277761487, total= 8.2min\n",
      "[CV] ... n_estimators=2000, score=-0.000740627714347862, total= 5.2min\n",
      "[CV] .. n_estimators=2000, score=-0.0006411565517031741, total= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 22.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=50, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [2000]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10-fold RFR nest=2000\n",
    "new_params = {'n_estimators': [2000]}\n",
    "\n",
    "rfR2 = RFR(random_state=50)\n",
    "gsrfr22 = GridSearchCV(estimator=rfR2, scoring='neg_mean_squared_error', param_grid=new_params, n_jobs=-1, cv=10, verbose=3)\n",
    "gsrfr22.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026802889915708384"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gsrfr22.cv_results_\n",
    "\n",
    "def correct(num):\n",
    "    return sqrt(-num)\n",
    "\n",
    "(correct(g['split0_test_score'][0])+correct(g['split1_test_score'][0])+correct(g['split2_test_score'][0])+correct(g['split3_test_score'][0])+correct(g['split4_test_score'][0])+correct(g['split5_test_score'][0])+correct(g['split6_test_score'][0])+correct(g['split7_test_score'][0])+correct(g['split8_test_score'][0])+correct(g['split9_test_score'][0]))/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
